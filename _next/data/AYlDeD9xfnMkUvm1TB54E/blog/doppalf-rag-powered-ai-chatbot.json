{"pageProps":{"postMetadata":{"title":"Doppalf: RAG powered full-stack AI chatbot like ChatGPT","description":"Build a full-stack RAG-powered AI chatbot like ChatGPT to give LLM your personality with Python, FastAPI, Llamaindex, Cohere, Qdrant, Next.js (Typescript), and Tailwind CSS.","imgName":"doppalf-rag-powered-ai-chatbot/doppalf-rag-powered-ai-chatbot.jpg","date":"May 24, 2024","tags":["python","ai","next-js","react"],"keywords":["python","llm","next-js","llamaindex","tailwind-css","typescript","fastapi","rag","cohere","qdrant","cohere-ai","ai-chatbot","llm","chatbot","chatgpt"],"id":"doppalf-rag-powered-ai-chatbot"},"postContent":{"compiledSource":"var m=Object.defineProperty,c=Object.defineProperties;var h=Object.getOwnPropertyDescriptors;var n=Object.getOwnPropertySymbols;var i=Object.prototype.hasOwnProperty,s=Object.prototype.propertyIsEnumerable;var p=(a,t,r)=>t in a?m(a,t,{enumerable:!0,configurable:!0,writable:!0,value:r}):a[t]=r,e=(a,t)=>{for(var r in t||(t={}))i.call(t,r)&&p(a,r,t[r]);if(n)for(var r of n(t))s.call(t,r)&&p(a,r,t[r]);return a},l=(a,t)=>c(a,h(t));var d=(a,t)=>{var r={};for(var o in a)i.call(a,o)&&t.indexOf(o)<0&&(r[o]=a[o]);if(a!=null&&n)for(var o of n(a))t.indexOf(o)<0&&s.call(a,o)&&(r[o]=a[o]);return r};const layoutProps={},MDXLayout=\"wrapper\";function MDXContent(r){var o=r,{components:a}=o,t=d(o,[\"components\"]);return mdx(MDXLayout,l(e(e({},layoutProps),t),{components:a,mdxType:\"MDXLayout\"}),mdx(\"p\",null,mdx(\"img\",e({parentName:\"p\"},{src:\"doppalf-rag-powered-ai-chatbot/doppalf-rag-powered-ai-chatbot.jpg\",alt:\"Doppalf: RAG powered fullstack AI chatbot like ChatGPT\"}))),mdx(\"h6\",null,\"Published on: \",mdx(\"strong\",{parentName:\"h6\"},\"May 24, 2024\")),mdx(\"h1\",null,\"Doppalf: RAG-powered fullstack AI chatbot like ChatGPT\"),mdx(\"p\",null,\"I have built an end-to-end full-stack AI chatbot web application like ChatGPT. It's powered with RAG (Retrieval Augmentation Generation) to give LLM my personality. Meaning, that LLM will behave like me and assume the character of \",mdx(\"strong\",{parentName:\"p\"},\"Lakshmi Narayana\"),\". I have built this application with LLamaindex, Cohere AI, and the Qdrant database. The full tech stack is:\"),mdx(\"ul\",null,mdx(\"li\",{parentName:\"ul\"},mdx(\"strong\",{parentName:\"li\"},\"Docker\")),mdx(\"li\",{parentName:\"ul\"},mdx(\"strong\",{parentName:\"li\"},\"Nginx\")),mdx(\"li\",{parentName:\"ul\"},mdx(\"strong\",{parentName:\"li\"},\"UI\"),\":\",mdx(\"ul\",{parentName:\"li\"},mdx(\"li\",{parentName:\"ul\"},\"Next.js (v14)\"),mdx(\"li\",{parentName:\"ul\"},\"Typescript\"),mdx(\"li\",{parentName:\"ul\"},\"Tailwind CSS\"))),mdx(\"li\",{parentName:\"ul\"},mdx(\"strong\",{parentName:\"li\"},\"Backend\"),\":\",mdx(\"ul\",{parentName:\"li\"},mdx(\"li\",{parentName:\"ul\"},\"Python (v3.12)\"),mdx(\"li\",{parentName:\"ul\"},\"FastAPI\"),mdx(\"li\",{parentName:\"ul\"},\"Llamaindex\"),mdx(\"li\",{parentName:\"ul\"},\"Cohere AI\"),mdx(\"li\",{parentName:\"ul\"},\"Qdrant Cloud\")))),mdx(\"p\",null,mdx(\"img\",e({parentName:\"p\"},{src:\"doppalf-rag-powered-ai-chatbot/doppalf-response.gif\",alt:\"Doppalf AI:=:100:=:Doppalf AI streaming response\"}))),mdx(\"h2\",null,\"Architecture\"),mdx(\"p\",null,mdx(\"img\",e({parentName:\"p\"},{src:\"doppalf-rag-powered-ai-chatbot/doppalf-arch.png\",alt:\"Doppalf Architecture:=:100:=:Doppalf High level Architecture\"}))),mdx(\"blockquote\",null,mdx(\"p\",{parentName:\"blockquote\"},\"The whole project code can be found on GitHub for \",mdx(\"a\",e({parentName:\"p\"},{href:\"https://github.com/santhalakshminarayana/doppalf\"}),\"Doppalf\"),\".\")),mdx(\"p\",null,\"For Doppalf, Docker has been used for container orchestration and Nginx as a reverse proxy. This application has a total three services (repos):\"),mdx(\"ul\",null,mdx(\"li\",{parentName:\"ul\"},\"doppalf-rp (Nginx)\"),mdx(\"li\",{parentName:\"ul\"},\"doppalf-ui (Next.js)\"),mdx(\"li\",{parentName:\"ul\"},\"doppalf-ai (Python)\")),mdx(\"p\",null,\"Each of the above repos has an individual \",mdx(\"strong\",{parentName:\"p\"},\"Dockerfile\"),\" with optimized configuration. The following are the Dockerfiles for individual services:\"),mdx(\"h3\",null,\"doppalf-ui (Next.js)\"),mdx(\"pre\",null,mdx(\"code\",e({parentName:\"pre\"},{className:\"language-yaml:Dockerfile\"}),`FROM node:21-bullseye-slim AS deps\nWORKDIR /app\nCOPY package*.json ./\nEXPOSE 3001\nENV PORT 3001\nENV HOSTNAME \"0.0.0.0\"\nRUN npm ci\n\n# Development\nFrom deps as dev\nENV NODE_ENV=development\nCOPY . .\nCMD [\"npm\", \"run\", \"dev\"]\n\nFROM node:21-bullseye-slim AS builder\nWORKDIR /app\nCOPY --from=deps /app/node_modules ./node_modules\nCOPY . .\nRUN npm run build\nRUN npm prune --production\n\n# Production\nFROM node:21-bullseye-slim AS prod\nWORKDIR /app\nENV NODE_ENV production\n# Add nextjs user\nRUN addgroup --system --gid 1001 nodejs\nRUN adduser --system --uid 1001 nextjs\n# Set the permission for prerender cache\nRUN mkdir .next\nRUN chown nextjs:nodejs .next\nUSER nextjs\n# Automatically leverage output traces to reduce image size\nCOPY --from=builder --chown=nextjs:nodejs /app/.next/static ./.next/static\nCOPY --from=builder --chown=nextjs:nodejs /app/public ./public\n\nEXPOSE 3001\nENV PORT 3001\nENV HOSTNAME \"0.0.0.0\"\n\nCMD [\"npm\", \"start\"]\n`)),mdx(\"p\",null,\"The above \",mdx(\"strong\",{parentName:\"p\"},\"Dockerfile\"),\" has a multi-stage build config for both \",mdx(\"em\",{parentName:\"p\"},\"dev\"),\" and \",mdx(\"em\",{parentName:\"p\"},\"prod\"),\" environments. The exact build stage to be used will be determined by configuration from \",mdx(\"em\",{parentName:\"p\"},\"docker-compose.yaml\"),\". By default, the \",mdx(\"em\",{parentName:\"p\"},\"dev\"),\" stage is used. The above configuration for \",mdx(\"em\",{parentName:\"p\"},\"prod\"),\" is very optimal and reduces the final running docker image size due to Next.js optimizations.\"),mdx(\"h3\",null,\"doppalf-ai (Python)\"),mdx(\"pre\",null,mdx(\"code\",e({parentName:\"pre\"},{className:\"language-yaml:Dockerfile\"}),`# Why bookworm? https://pythonspeed.com/articles/base-image-python-docker-images/\nFROM python:3.12-slim-bookworm as base\nWORKDIR /app\nRUN apt-get update && \\\\\n    apt-get install -y --no-install-recommends gcc\nCOPY requirements.txt .\nRUN pip wheel --no-cache-dir --no-deps --wheel-dir /app/wheels -r requirements.txt\n\nFROM python:3.12-slim-bookworm as build\nENV PYTHONDONTWRITEBYTECODE=1\nENV PYTHONUNBUFFERED=1\nWORKDIR /app\nCOPY --from=base /app/wheels /app/wheels\nCOPY --from=base /app/requirements.txt .\nRUN pip install --no-cache /app/wheels/*\nCOPY . /app\nEXPOSE 4001\nCMD [\"python\", \"main.py\"]\n`)),mdx(\"p\",null,\"The above \",mdx(\"strong\",{parentName:\"p\"},\"Dockerfile\"),\" pulls Python 3.12 \",mdx(\"em\",{parentName:\"p\"},\"slim-bookworm\"),\" docker image as opposed to the common \",mdx(\"em\",{parentName:\"p\"},\"alpine\"),\" image. You can refer to the linked article for why. Here instead of normally installing requirements through \",mdx(\"em\",{parentName:\"p\"},\"pip\"),\", we install dependencies through \",mdx(\"em\",{parentName:\"p\"},\"wheels\"),\" which optimizes the docker image build speed.\"),mdx(\"p\",null,\"The Next.js service will be running on PORT \",mdx(\"em\",{parentName:\"p\"},\"3001\"),\" and the Python service will run on \",mdx(\"em\",{parentName:\"p\"},\"4001\"),\".\"),mdx(\"h3\",null,\"doppalf-rp (Nginx)\"),mdx(\"p\",null,\"And finally, Nginx configuration for forwarding the traffic to individual services are done as \"),mdx(\"pre\",null,mdx(\"code\",e({parentName:\"pre\"},{className:\"language-:includes/proxy.conf\"}),`proxy_set_header Host $host;\nproxy_set_header X-Real-IP $remote_addr;\nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\nproxy_set_header X-Forwarded-Proto $scheme;\nproxy_set_header X-Forwarded-Host $server_name;\nproxy_buffering off;\nproxy_request_buffering off;\nproxy_http_version 1.1;\nproxy_intercept_errors on;\n`)),mdx(\"pre\",null,mdx(\"code\",e({parentName:\"pre\"},{className:\"language-yaml:Dockerfile\"}),`From nginx:stable-alpine\nRUN mkdir -p /run/nginx\nWORKDIR /run/nginx\nCOPY ./nginx.conf /etc/nginx/conf.d/default.conf\nCOPY ./includes /etc/nginx/includes/\nEXPOSE 80\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\n`)),mdx(\"p\",null,\"Nginx configruation for forwarding rules for different services\"),mdx(\"pre\",null,mdx(\"code\",e({parentName:\"pre\"},{className:\"language-:nginx.conf\"}),`server {\n    listen 80 default_server;\n    listen [::]:80 default_server;\n    server_name _;\n\n    index index.html;\n\n    location / {\n        proxy_pass http://doppalf-ui-service:3001;\n    }\n\n    location /doppalf-ai/v1 {\n        proxy_pass http://doppalf-ai-service:4001;\n\n        proxy_redirect off;\n        # SSE connection config\n        proxy_set_header Connection '';\n        proxy_cache off;\n    }\n}\n`)),mdx(\"p\",null,\"So, the root \",mdx(\"em\",{parentName:\"p\"},\"/\"),\" request (127.0.0.1) will be forwarded to the Next.js UI service for the UI page, and any request with the prefix \",mdx(\"em\",{parentName:\"p\"},\"/doppalf-ai/v1\"),\" will be forwarded to the Python AI service.\"),mdx(\"h2\",null,\"Doppalf UI\"),mdx(\"p\",null,\"The UI has been built with Next.js (Typescript) and Tailwind CSS. It's a single-page application that mainly contains an input box for providing Query and the response will be generated like ChatGPT where the UI will show the user and system messages. The AI-generated answer will be streamed like ChatGPT and rendered as Markdown. This has been done by Streaming the text from the Backend as \",mdx(\"strong\",{parentName:\"p\"},\"SSE (Server Sent Events)\"),\" and reading those messages in Next.js using using Microsoft's \",mdx(\"a\",e({parentName:\"p\"},{href:\"https://www.npmjs.com/package/@microsoft/fetch-event-source\"}),\"Fetch Event Source package\"),\" because normal browser SSE doesn't support \",mdx(\"em\",{parentName:\"p\"},\"POST\"),\" request.\"),mdx(\"p\",null,mdx(\"img\",e({parentName:\"p\"},{src:\"doppalf-rag-powered-ai-chatbot/doppalf-ui-query-response.png\",alt:\"Doppalf UI Landing Page:=:100:=:Showing User and System Message\"}))),mdx(\"h4\",null,\"UI Features include:\"),mdx(\"ul\",null,mdx(\"li\",{parentName:\"ul\"},\"Dark mode\"),mdx(\"li\",{parentName:\"ul\"},\"Streaming responses like ChatGPT\"),mdx(\"li\",{parentName:\"ul\"},\"Auto-scroll to the bottom while generating the answer\"),mdx(\"li\",{parentName:\"ul\"},\"New chat session\")),mdx(\"h2\",null,\"Doppalf AI\"),mdx(\"p\",null,\"The main part of this application is building the RAG pipeline and giving LLM a personal character that answers like me. For this, I have used Llamaindex for building the RAG pipeline, Cohere AI as LLM, and Qdrant Cloud for storing vector embeddings.\"),mdx(\"p\",null,\"This costs me nothing as they both offer free APIs, you can get free \",mdx(\"a\",e({parentName:\"p\"},{href:\"https://dashboard.cohere.com/api-keys\"}),\"Cohere API trail Key\"),\" and 1 GB cluster for storing vectors in \",mdx(\"a\",e({parentName:\"p\"},{href:\"https://cloud.qdrant.io\"}),\"Qdrant Cloud API Key and URL\"),\".\"),mdx(\"p\",null,\"The web framework for providing APIs used was FastAPI and its support for streaming the response like SSE without using any extra configuration was a huge thumbs up for this kind of AI Chatbot application.\"),mdx(\"p\",null,\"For building the RAG pipeline, I have followed the following pipeline process:\"),mdx(\"ul\",null,mdx(\"li\",{parentName:\"ul\"},\"Load Documents\"),mdx(\"li\",{parentName:\"ul\"},\"Parse text into Sentences (as nodes) with Window size as 1\"),mdx(\"li\",{parentName:\"ul\"},\"Get vector embeddings for each node (sentences) with Cohere Embeddings\"),mdx(\"li\",{parentName:\"ul\"},\"Index the nodes and store the vector embeddings in the Qdrant cloud\"),mdx(\"li\",{parentName:\"ul\"},\"Persist the index for re-use further runtimes\"),mdx(\"li\",{parentName:\"ul\"},'Build a Chat engine from the index with a retrieval strategy as \"Small-to-Big\" and with some buffered chat memory history'),mdx(\"li\",{parentName:\"ul\"},\"Provide the retrieved context and use Cohere Rerank for reranking the retrieved nodes\"),mdx(\"li\",{parentName:\"ul\"},\"Synthesis the response using Cohere\")),mdx(\"p\",null,mdx(\"img\",e({parentName:\"p\"},{src:\"doppalf-rag-powered-ai-chatbot/rag-pipeline.jpg\",alt:\"RAG Pipeline:=:100:=:RAG pipeline\"}))),mdx(\"p\",null,\"The complete code for the RAG pipeline using Llamaindex is \"),mdx(\"pre\",null,mdx(\"code\",e({parentName:\"pre\"},{className:\"language-python:rag.py\"}),`from llama_index.core import load_index_from_storage\nfrom llama_index.core.memory import ChatMemoryBuffer\nfrom llama_index.core.node_parser import SentenceWindowNodeParser\nfrom llama_index.core.postprocessor import MetadataReplacementPostProcessor\nfrom llama_index.embeddings.cohere import CohereEmbedding\nfrom llama_index.llms.cohere import Cohere\nfrom llama_index.postprocessor.cohere_rerank import CohereRerank\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\nfrom qdrant_client import QdrantClient\n\nfrom src.config.env import ENV, env_keys\nfrom src.config.logger import get_logger\n\nfrom .constants import CHAT_PROMPT\n\nenvk = ENV()\nlogger = get_logger()\n\nindex = None\nchat_engine = None\n\ndef load_rag() -> None:\n    global index\n    global chat_engine\n\n    cdir = os.getcwd()\n    docs_dir = envk.get(env_keys.get(\"DOCS_DIR\"))\n    docs_path = os.path.join(cdir, docs_dir)\n\n    # check if any documents are provided for index\n    if not os.path.exists(docs_path):\n        raise FileNotFoundError(f\"Documents dir at path: {docs_path} not exists.\")\n    if not os.listdir(docs_dir):\n        raise FileNotFoundError(f\"Provide documents inside directory: {docs_path} for indexing.\")\n    \n    storage_dir = envk.get(env_keys.get(\"INDEX_STORAGE_DIR\"))\n    storage_path = os.path.join(cdir, storage_dir)\n    \n    cohere_api_key = envk.get(env_keys.get(\"COHERE_API_KEY\"))\n    qdrant_api_key = envk.get(env_keys.get(\"QDRANT_API_KEY\"))\n\n    Settings.llm = Cohere(\n        api_key=cohere_api_key,\n        model=\"command-r-plus\", \n    )\n    Settings.embed_model = CohereEmbedding(\n        cohere_api_key=cohere_api_key,\n        model_name=\"embed-english-v3.0\",\n        input_type=\"search_document\",\n    )\n    \n    qd_client = QdrantClient(\n        envk.get(env_keys.get(\"QDRANT_CLOUD_URL\")),\n        api_key=qdrant_api_key,\n    )\n\n    sentence_node_parser = SentenceWindowNodeParser.from_defaults(\n        window_size=1,\n        window_metadata_key=\"window\",\n        original_text_metadata_key=\"original_text\", \n    )\n\n    vector_store = QdrantVectorStore(\n        client=qd_client, \n        collection_name=envk.get(env_keys.get(\"COLLECTION_NAME\")),\n    )\n\n    # index was previously persisted\n    if os.path.exists(storage_path) and os.listdir(storage_path):\n        logger.debug(\"Using existing index.\")\n        storage_context = StorageContext.from_defaults(\n            vector_store=vector_store, persist_dir=storage_path\n        )\n        \n        index = load_index_from_storage(storage_context)\n\n    else:\n        logger.debug(\"Creating new index for documents.\")\n        reader = SimpleDirectoryReader(input_dir=docs_path)\n        \n        all_docs = []\n        for docs in reader.iter_data():\n            all_docs.extend(docs)\n        \n        for doc in all_docs:\n            logger.debug(f\"id: {doc.doc_id}\\\\nmetada: {doc.metadata}\")\n\n        nodes = sentence_node_parser.get_nodes_from_documents(all_docs)\n        \n        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n        \n        index = VectorStoreIndex(nodes, storage_context=storage_context)\n\n        index.storage_context.persist(persist_dir=storage_path)\n\n\n    chat_engine = index.as_chat_engine(\n        chat_mode=\"condense_plus_context\",\n        memory=ChatMemoryBuffer.from_defaults(token_limit=int(envk.get(env_keys.get(\"MAX_BUFFER_MEMORY_TOKENS\")))),\n        context_prompt=CHAT_PROMPT,\n        similarity_top_k=3, \n        node_postprocessors=[\n            MetadataReplacementPostProcessor(target_metadata_key=\"window\"),\n            CohereRerank(api_key=cohere_api_key, top_n=3),\n        ],\n        verbose=False,\n    )\n\n\ndef chat(query: str):\n    global chat_engine\n    \n    response = chat_engine.stream_chat(query)\n    for res in response.response_gen:\n        yield f\"data: {json.dumps({\"message\":res})}\\\\n\\\\n\"\n`)),mdx(\"p\",null,\"The above \",mdx(\"em\",{parentName:\"p\"},\"load_rag()\"),\" function will first check if any previously stored index exists for re-using else it will build one. If no index is stored in the given \",mdx(\"em\",{parentName:\"p\"},\"storage_path\"),\" directory, it will build the index by loading the documents from the \",mdx(\"em\",{parentName:\"p\"},\"docs_dir\"),\" directory. \",mdx(\"em\",{parentName:\"p\"},\"SentenceWindowNodeParser\"),\" is used for parsing the sentences into nodes with a window size of 1. This window size makes the post-retrieval context have surrounding sentences also part of it for better answer synthesis. \"),mdx(\"p\",null,\"For created nodes, get the vector embeddings from the Cohere embeddings and store them in the Qdrant Cloud. And finally store the index for skipping this whole process again when the application restarts.\"),mdx(\"p\",null,\"From the index, build the chat engine with some memory (4096 tokens here) for memorizing past conversations with the user and provide Cohere Re-ranker as post node retrieval processer for reranking the nodes based on the query relevancy. This chat engine retrieves the surrounding context for each retrieved node for more information and finally sends the whole retrieved-context and the query to the LLM for answer generation as a prompt. I have used a custom prompt for making LLM answer as me.\"),mdx(\"pre\",null,mdx(\"code\",e({parentName:\"pre\"},{className:\"language-python:/src/generate/constants.py\"}),`CHAT_PROMPT=(\n    \"You are impersonating the human 'Lakshmi Narayana' and so your name.\"\n    \"So you are Lakshmi Narayana and answers in first person.When asked any question about you, you will answer as if Lakshmi Narayana is answering.\"\n    \"You will answer politely and take the help of the following context for more relevant answers.\"\n    \"If you don't have any sufficient information from the context, use your knowledge to answer.\"\n    \"Or don't hallucinate if you are sure you cannot answer.\"\n    \"Here are the relevant documents for the context:\\\\n{context_str}\\\\n\"\n    \"Instruction: Use the previous chat history, or the context above, to interact and help the user and answer as if you are Lakshmi Narayana.\"\n    \"Don't add any additional data if the answer can be derived from context.\"\n    \"Generate the response in markdown format.\"\n)\n`)),mdx(\"p\",null,\"LLamaindex uses this prompt for context ingestion and sends this to LLM for answer generation.\"),mdx(\"p\",null,\"Finally, the chat generation API is exposed for streaming the response using FastAPI as follows\"),mdx(\"pre\",null,mdx(\"code\",e({parentName:\"pre\"},{className:\"language-python:api.py\"}),`from fastapi import APIRouter, HTTPException\nfrom pydantic import BaseModel\nfrom starlette.responses import StreamingResponse\n\nfrom .rag import chat\n\n\nclass GenerateModel(BaseModel):\n    message: str\n    message_id: str\n    role: str\n    timestamp: str\n\n\ngrouter = APIRouter(tags=[\"generate\"])\n\n\n@grouter.post(\"\")\nasync def generate(data: GenerateModel):\n    try:\n        return StreamingResponse(\n            chat(data.message), \n            media_type='text/event-stream',\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=e)\n`)),mdx(\"p\",null,\"And final interactivity with LLM that answers about me like I am talking as \"),mdx(\"p\",null,mdx(\"img\",e({parentName:\"p\"},{src:\"doppalf-rag-powered-ai-chatbot/query-response-history.png\",alt:\"LLM answering as me:=:100:=:LLM answering like me with history\"}))),mdx(\"p\",null,mdx(\"strong\",{parentName:\"p\"},\"New Chat session\")),mdx(\"p\",null,mdx(\"img\",e({parentName:\"p\"},{src:\"doppalf-rag-powered-ai-chatbot/new-chat.gif\",alt:\"New Chat Session:=:100:=:New Chat Session\"}))),mdx(\"hr\",null),mdx(\"p\",null,\"I will some more features in the future like:\"),mdx(\"ul\",null,mdx(\"li\",{parentName:\"ul\"},\"Adding or removing the documents dynamically from the UI\"),mdx(\"li\",{parentName:\"ul\"},\"Voice cloning for speaking out the answer as a character\"),mdx(\"li\",{parentName:\"ul\"},\"Enhance the LLM answering with more RAG strategies\")),mdx(\"p\",null,\"Please check out the complete project code in my Github repository for \",mdx(\"a\",e({parentName:\"p\"},{href:\"https://github.com/santhalakshminarayana/doppalf\"}),\"Doppalf\"),\".\"))}MDXContent.isMDXComponent=!0;\n","scope":{}},"id":"doppalf-rag-powered-ai-chatbot"},"__N_SSG":true}