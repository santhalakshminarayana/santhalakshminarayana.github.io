<!DOCTYPE html><html lang="en"><head><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin /><link rel="stylesheet" data-href="https://fonts.googleapis.com/css2?family=Maven+Pro&amp;family=Share+Tech+Mono&amp;family=Source+Sans+Pro&amp;family=Ubuntu&amp;display=swap" data-optimized-fonts="true"/><meta charSet="utf-8" class="jsx-436519370"/><meta name="viewport" content="width=device-width, initial-scale=1" class="jsx-436519370"/><meta name="description" content="I&#x27;m Santha Lakshmi Narayana, a voyager on mission exploring digital universe to understand how it works." class="jsx-436519370"/><meta name="author" content="Santha Lakshmi Narayana" class="jsx-436519370"/><meta name="keywords" content="Blog,Tutorial,Python,Javascript" class="jsx-436519370"/><meta property="og:title" content="Santha Lakshmi Narayana" class="jsx-436519370"/><meta property="og:description" content="I&#x27;m Santha Lakshmi Narayana, a voyager on mission exploring digital universe to understand how it works." class="jsx-436519370"/><meta property="og:url" content="https://santhalakshminarayana.github.io/" class="jsx-436519370"/><meta property="og:image" content="https://santhalakshminarayana.github.io/images/santha lakshmi narayana logo.png" class="jsx-436519370"/><meta property="og:type" content="article" class="jsx-436519370"/><meta property="og:article:publisher" content="https://santhalakshminarayana.github.io/" class="jsx-436519370"/><meta property="og:site_name" content="Santha Lakshmi Narayana" class="jsx-436519370"/><meta name="twitter:card" content="summary" class="jsx-436519370"/><meta name="twitter:title" content="Santha Lakshmi Narayana" class="jsx-436519370"/><meta name="twitter:description" content="I&#x27;m Santha Lakshmi Narayana, a voyager on mission exploring digital universe to understand how it works." class="jsx-436519370"/><meta name="twitter:url" content="https://santhalakshminarayana.github.io/" class="jsx-436519370"/><meta name="twitter:site" content="@santhalakshminarayana" class="jsx-436519370"/><meta name="twitter:image" content="https://santhalakshminarayana.github.io/images/santha lakshmi narayana logo.png" class="jsx-436519370"/><meta name="twitter:creator" content="@santhalakshminarayana" class="jsx-436519370"/><link rel="icon" href="/images/santha-lakshmi-narayana-logo.png?" class="jsx-436519370"/><link rel="canonical" href="https://santhalakshminarayana.github.io/" class="jsx-436519370"/><meta name="google-site-verification" content="3p5W6wHr-TDhnkyuewv0nYJd2S9OuTQlj5__OUyLLcU" class="jsx-436519370"/><title class="jsx-436519370">Santha Lakshmi Narayana</title><meta name="next-head-count" content="24"/><link rel="preload" href="/_next/static/css/b1be9be44572118d40d2.css" as="style"/><link rel="stylesheet" href="/_next/static/css/b1be9be44572118d40d2.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-a40ef1678bae11e696dba45124eadd70.js"></script><script src="/_next/static/chunks/webpack-2b6f4fb4c650415a78b4.js" defer=""></script><script src="/_next/static/chunks/framework.1b3b121feae4c8ce410b.js" defer=""></script><script src="/_next/static/chunks/commons.5b2d72a8e34e4f7cda46.js" defer=""></script><script src="/_next/static/chunks/main-d372fe187139d1f38e70.js" defer=""></script><script src="/_next/static/chunks/pages/_app-8dd04848a34b8c116228.js" defer=""></script><script src="/_next/static/chunks/d0447323.b45849b482775ab7aa09.js" defer=""></script><script src="/_next/static/chunks/773f402080c38a523cb0b4f52dac49b5097e45fc.4e3d67b40aa7baa9fe81.js" defer=""></script><script src="/_next/static/chunks/6188d020c9511bd1e7ae4c1c2034aa4e99a1704e.9b7f5c8773315bea6b89.js" defer=""></script><script src="/_next/static/chunks/pages/index-4803bfb51ac967ab28ef.js" defer=""></script><script src="/_next/static/WcgTTsGPX2O1xgu7UXzKY/_buildManifest.js" defer=""></script><script src="/_next/static/WcgTTsGPX2O1xgu7UXzKY/_ssgManifest.js" defer=""></script><style id="__jsx-3242260268">.tags.jsx-3242260268{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;padding:1vw 5vw 1vw 5vw;background-color:#fffecb;border-bottom:5px solid #071013;}.tags.jsx-3242260268 a.jsx-3242260268{font-family:"Share Tech Mono",monospace;color:#1d2b35;margin:0 1em 0 1em;-webkit-text-decoration:none;text-decoration:none;border-bottom:1px dashed #fb232e;}.tags.jsx-3242260268 a.jsx-3242260268:hover{background:#20a4f3;color:#fffecb;border-bottom:1px dashed #fffecb;}.tags.jsx-3242260268 a.jsx-3242260268:active{background:#fb232e;}</style><style id="__jsx-90057834">.header-container.jsx-90057834{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1;-ms-flex:1;flex:1;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;padding:10px 5vw 10px 5vw;background:#1d2b35;}.header-left.jsx-90057834{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1;-ms-flex:1;flex:1;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}.header-left.jsx-90057834 a.jsx-90057834{font-family:"Source Sans Pro",sans-serif;-webkit-text-decoration:none;text-decoration:none;color:#ffaa33;}img.jsx-90057834{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1;-ms-flex:1;flex:1;width:calc(1.2rem + 1vw);max-width:calc(1.2rem + 1vw);height:auto;margin:0 5px 0 5px;}img.jsx-90057834:hover{cursor:pointer;}.header-left.jsx-90057834 a.jsx-90057834:hover{color:#20a4f3;}.header-left.jsx-90057834 a.jsx-90057834:active{color:#fb232e;}.header-right.jsx-90057834{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:3;-ms-flex:3;flex:3;-webkit-flex-direction:row-reverse;-ms-flex-direction:row-reverse;flex-direction:row-reverse;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}.header-right.jsx-90057834 a.jsx-90057834{padding:0 1vw 0 1vw;font-family:"Source Sans Pro",sans-serif;-webkit-text-decoration:none;text-decoration:none;color:#ffaa33;}.header-right.jsx-90057834 a.jsx-90057834:hover{color:#20a4f3;}.header-right.jsx-90057834 a.jsx-90057834:active{color:#fb232e;}@media screen and (max-width:920px){.header-container.jsx-90057834{padding:10px 2vw 10px 2vw;}#name.jsx-90057834{display:none;}img.jsx-90057834{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}@media screen and (max-width:480px){.header-container.jsx-90057834{padding:10px 2vw 10px 2vw;overflow-x:scroll;}}@media screen and (max-width:300px){img.jsx-90057834{display:none;}}</style><style id="__jsx-3665177261">.card-layout.jsx-3665177261{display:grid;grid-template-columns:repeat(auto-fit,minmax(450px,1fr));margin:1vh 11vw 1vh 11vw;}.card-container.jsx-3665177261{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:0 1 auto;-ms-flex:0 1 auto;flex:0 1 auto;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;box-sizing:border-box;margin:1vh 0.87vw 1vh 0.87vw;border-bottom:3px solid transparent;}.card-container.jsx-3665177261:hover{border-bottom:3px solid #fb232e;-webkit-transform:scale(0.99);-ms-transform:scale(0.99);transform:scale(0.99);}.img-container.jsx-3665177261{width:100%;max-width:100%;}img.jsx-3665177261{width:100%;max-width:100%;aspect-ratio:2/1;object-fit:cover;border-radius:5px;}.info-container.jsx-3665177261{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:100%;-ms-flex:100%;flex:100%;max-width:100%;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;overflow-wrap:break-word;}.info-container-link.jsx-3665177261{font-family:"Maven Pro",sans-serif;font-weight:bold;font-size:calc(20px + (26 - 20) * ((100vw - 300px) / (1600 - 300)));-webkit-text-decoration:none;text-decoration:none;overflow-wrap:break-word;color:#1d2b35;}.info-container-link.jsx-3665177261:hover{color:#20a4f3;}.info-container-link.jsx-3665177261:active{color:#fb232e;}.date.jsx-3665177261{font-family:"Source Sans Pro",sans-serif;font-size:calc(15px + (18 - 15) * ((100vw - 300px) / (1600 - 300)));font-weight:600;margin:0.5vh 0 0.5vh 0;color:#20a4f3;max-width:100%;}.description-container.jsx-3665177261{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;max-width:100%;}.description.jsx-3665177261{font-family:"Source Sans Pro",sans-serif;font-size:calc(15px + (18 - 15) * ((100vw - 300px) / (1600 - 300)));color:#071013cc;max-width:100%;}.tags-container.jsx-3665177261{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:100%;-ms-flex:100%;flex:100%;max-width:100%;-webkit-flex-direction:column-reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse;margin:2vh 0 2vh 0;}.tags-container-tag.jsx-3665177261{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;}.tag-link.jsx-3665177261{font-family:"Share Tech Mono",monospace;font-size:calc(14px + (16 - 14) * ((100vw - 300px) / (1600 - 300)));color:#1d2b35;-webkit-text-decoration:none;text-decoration:none;margin-right:1em;border-bottom:1px dashed #fb232e;}.tag-link.jsx-3665177261:hover{background:#20a4f3;color:#fffecb;border-bottom:1px dashed #fffecb;}.tag-link.jsx-3665177261:active{background:#fb232e;}@media screen and (max-width:1800px){.card-layout.jsx-3665177261{grid-template-columns:repeat(3,1fr);margin:1vh 7vw 1vh 7vw;}}@media screen and (max-width:1280px){.card-layout.jsx-3665177261{grid-template-columns:repeat(2,1fr);margin:1vh 5vw 1vh 5vw;}.card-container.jsx-3665177261{margin:1vh 1vw 1vh 1vw;}}@media screen and (max-width:560px){.card-layout.jsx-3665177261{grid-template-columns:repeat(1,1fr);margin:1vh 5vw 1vh 5vw;}.card-container.jsx-3665177261{margin:1vh 0 1vh 0;border-bottom:3px solid #fb232e;}}</style><style id="__jsx-931837452">.footer-container.jsx-931837452{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;background:#1d2b35f2;padding:1vw 5vw 1vw 5vw;}.footer-container-links.jsx-931837452{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;}.footer-links.jsx-931837452{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}.link-item.jsx-931837452{margin:0 1vw 0 1vw;}.link.jsx-931837452{-webkit-text-decoration:none;text-decoration:none;cursor:pointer: background-color:yellow;}.link.jsx-931837452:hover{color:black;}.copy-right-container.jsx-931837452{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;margin-top:10px;}.copy-right-text.jsx-931837452{font-family:'Maven Pro',sans-serif;font-size:calc(0.75rem + 0.1vw);color:#fffecb;}</style><style id="__jsx-436519370">.header-info.jsx-436519370{background:#1d2b35;}.greetings.jsx-436519370{padding:2vw 10vw 2vw 10vw;}.greetings-heading.jsx-436519370{font-family:"Ubuntu",sans-serif;font-size:2em;text-align:center;color:#ffaa33;}.greetings-statement.jsx-436519370{font-family:"Ubuntu",sans-serif;font-size:1.5em;padding-top:1vh;text-align:center;color:#ffaa33;}.greetings-tags.jsx-436519370{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;padding:1vw 5vw 1vw 5vw;}.posts-display-container.jsx-436519370{padding:1vh 10vw 1vh 10vw;}.quote.jsx-436519370{color:#fb232e;font-family:"Ubuntu",sans-serif;font-size:calc(2rem + 0.5vw);}@media screen and (max-width:920px){.greetings-heading.jsx-436519370{font-size:20px;}.greetings-statement.jsx-436519370{font-size:18px;}.greetings-tags.jsx-436519370{display:none;}.posts-display-container.jsx-436519370{padding:1vh 5vw 1vh 5vh;}}@media screen and (max-width:480px){.greetings-heading.jsx-436519370{font-size:18px;}.greetings-statement.jsx-436519370{font-size:16px;}.greetings-tags.jsx-436519370{display:none;}.posts-display-container.jsx-436519370{padding:1vh 5vw 1vh 5vh;}}</style><style data-href="https://fonts.googleapis.com/css2?family=Maven+Pro&family=Share+Tech+Mono&family=Source+Sans+Pro&family=Ubuntu&display=swap">@font-face{font-family:'Maven Pro';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/mavenpro/v36/7Auup_AqnyWWAxW2Wk3swUz56MS91Eww8SX25nM.woff) format('woff')}@font-face{font-family:'Share Tech Mono';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/sharetechmono/v15/J7aHnp1uDWRBEqV98dVQztYldFc7pw.woff) format('woff')}@font-face{font-family:'Source Sans Pro';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/sourcesanspro/v22/6xK3dSBYKcSV-LCoeQqfX1RYOo3aPA.woff) format('woff')}@font-face{font-family:'Ubuntu';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ubuntu/v20/4iCs6KVjbNBYlgo6ew.woff) format('woff')}@font-face{font-family:'Maven Pro';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/mavenpro/v36/7Auup_AqnyWWAxW2Wk3swUz56MS91Eww8SX21nijpBh8CvRBOB1s.woff) format('woff');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Maven Pro';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/mavenpro/v36/7Auup_AqnyWWAxW2Wk3swUz56MS91Eww8SX21nmjpBh8CvRBOB1s.woff) format('woff');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Maven Pro';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/mavenpro/v36/7Auup_AqnyWWAxW2Wk3swUz56MS91Eww8SX21nejpBh8CvRBOA.woff) format('woff');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Share Tech Mono';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/sharetechmono/v15/J7aHnp1uDWRBEqV98dVQztYldFcLowEFA87Heg.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Source Sans Pro';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/sourcesanspro/v22/6xK3dSBYKcSV-LCoeQqfX1RYOo3qNa7lujVj9_mf.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C8A,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Source Sans Pro';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/sourcesanspro/v22/6xK3dSBYKcSV-LCoeQqfX1RYOo3qPK7lujVj9_mf.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Source Sans Pro';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/sourcesanspro/v22/6xK3dSBYKcSV-LCoeQqfX1RYOo3qNK7lujVj9_mf.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Source Sans Pro';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/sourcesanspro/v22/6xK3dSBYKcSV-LCoeQqfX1RYOo3qO67lujVj9_mf.woff2) format('woff2');unicode-range:U+0370-0377,U+037A-037F,U+0384-038A,U+038C,U+038E-03A1,U+03A3-03FF}@font-face{font-family:'Source Sans Pro';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/sourcesanspro/v22/6xK3dSBYKcSV-LCoeQqfX1RYOo3qN67lujVj9_mf.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Source Sans Pro';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/sourcesanspro/v22/6xK3dSBYKcSV-LCoeQqfX1RYOo3qNq7lujVj9_mf.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Source Sans Pro';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/sourcesanspro/v22/6xK3dSBYKcSV-LCoeQqfX1RYOo3qOK7lujVj9w.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Ubuntu';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ubuntu/v20/4iCs6KVjbNBYlgoKcg72nU6AF7xm.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C8A,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Ubuntu';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ubuntu/v20/4iCs6KVjbNBYlgoKew72nU6AF7xm.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Ubuntu';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ubuntu/v20/4iCs6KVjbNBYlgoKcw72nU6AF7xm.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Ubuntu';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ubuntu/v20/4iCs6KVjbNBYlgoKfA72nU6AF7xm.woff2) format('woff2');unicode-range:U+0370-0377,U+037A-037F,U+0384-038A,U+038C,U+038E-03A1,U+03A3-03FF}@font-face{font-family:'Ubuntu';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ubuntu/v20/4iCs6KVjbNBYlgoKcQ72nU6AF7xm.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Ubuntu';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/ubuntu/v20/4iCs6KVjbNBYlgoKfw72nU6AFw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style></head><body><div id="__next"><div class="jsx-436519370"><div class="jsx-436519370"><div class="jsx-90057834 header"><div class="jsx-90057834 header-container"><div class="jsx-90057834 header-left"><img src="/images/santha-lakshmi-narayana-logo.png" alt="Logo" class="jsx-90057834"/><a id="name" class="jsx-90057834" href="/"><b class="jsx-90057834">Santha Lakshmi Narayana</b></a></div><div class="jsx-90057834 header-right"><a class="jsx-90057834" href="/about">About</a><a href="#" class="jsx-90057834">Tags</a><a class="jsx-90057834" href="/">Home</a></div></div><div style="display:none" class="jsx-90057834"><div class="jsx-3242260268 tags"><a class="jsx-3242260268" href="/tags/ai">#ai</a><a class="jsx-3242260268" href="/tags/system-design">#system-design</a><a class="jsx-3242260268" href="/tags/python">#python</a><a class="jsx-3242260268" href="/tags/python-performance">#python-performance</a><a class="jsx-3242260268" href="/tags/go">#go</a><a class="jsx-3242260268" href="/tags/mysql">#mysql</a><a class="jsx-3242260268" href="/tags/image-processing">#image-processing</a><a class="jsx-3242260268" href="/tags/opencv">#opencv</a><a class="jsx-3242260268" href="/tags/concurrency">#concurrency</a><a class="jsx-3242260268" href="/tags/color-science">#color-science</a><a class="jsx-3242260268" href="/tags/react">#react</a><a class="jsx-3242260268" href="/tags/next-js">#next-js</a><a class="jsx-3242260268" href="/tags/flutter">#flutter</a></div></div></div><div class="jsx-436519370 header-info"><div class="jsx-436519370 greetings"><p class="jsx-436519370 greetings-heading">Greetings, Programs! in the Matrix called Earth.</p><p class="jsx-436519370 greetings-statement">I&#x27;m Santha Lakshmi Narayana, a voyager on a mission to explore the digital universe.</p></div></div><div class="jsx-436519370 posts-display-container"><p class="jsx-436519370 quote">All Posts</p></div><div class="jsx-3665177261 card-layout"><div class="jsx-3665177261 card-container"><div class="jsx-3665177261"><div class="jsx-3665177261 img-container"><img src="/images/mysql-performance-optimization-techniques/mysql-performance-optimization-techniques.jpg" alt="MySQL performance optimization techniques" class="jsx-3665177261"/></div><div class="jsx-3665177261 info-container"><div class="jsx-3665177261 description-container"><p class="jsx-3665177261 date">Nov 10, 2024</p><a class="jsx-3665177261 info-container-link" href="/blog/mysql-performance-optimization-techniques">MySQL performance optimization techniques</a><p class="jsx-3665177261 description">Improve MySQL performance in production by configuring various optimization techniques and tuning InnoDB settings.</p></div><div class="jsx-3665177261 tags-container"><div class="jsx-3665177261 tags-container-tag"><a class="jsx-3665177261 tag-link" href="/tags/mysql">#mysql</a><a class="jsx-3665177261 tag-link" href="/tags/system-design">#system-design</a></div></div></div></div></div><div class="jsx-3665177261 card-container"><div class="jsx-3665177261"><div class="jsx-3665177261 img-container"><img src="/images/software-development-comprehensive-guide/software-development-comprehensive-guide.jpg" alt="Software Development Comprehensive Guide" class="jsx-3665177261"/></div><div class="jsx-3665177261 info-container"><div class="jsx-3665177261 description-container"><p class="jsx-3665177261 date">Oct 31, 2024</p><a class="jsx-3665177261 info-container-link" href="/blog/software-development-comprehensive-guide">Software Development Comprehensive Guide</a><p class="jsx-3665177261 description">SDLC, System Design &amp; Architecture, Distributed Systems, Microservices, Database Replication &amp; Partition.</p></div><div class="jsx-3665177261 tags-container"><div class="jsx-3665177261 tags-container-tag"><a class="jsx-3665177261 tag-link" href="/tags/system-design">#system-design</a></div></div></div></div></div><div class="jsx-3665177261 card-container"><div class="jsx-3665177261"><div class="jsx-3665177261 img-container"><img src="/images/doppalf-rag-powered-ai-chatbot/doppalf-rag-powered-ai-chatbot.jpg" alt="Doppalf: RAG powered full-stack AI chatbot like ChatGPT" class="jsx-3665177261"/></div><div class="jsx-3665177261 info-container"><div class="jsx-3665177261 description-container"><p class="jsx-3665177261 date">May 24, 2024</p><a class="jsx-3665177261 info-container-link" href="/blog/doppalf-rag-powered-ai-chatbot">Doppalf: RAG powered full-stack AI chatbot like ChatGPT</a><p class="jsx-3665177261 description">Build a full-stack RAG-powered AI chatbot like ChatGPT to give LLM your personality with Python, FastAPI, Llamaindex, Cohere, Qdrant, Next.js (Typescript), and Tailwind CSS.</p></div><div class="jsx-3665177261 tags-container"><div class="jsx-3665177261 tags-container-tag"><a class="jsx-3665177261 tag-link" href="/tags/python">#python</a><a class="jsx-3665177261 tag-link" href="/tags/ai">#ai</a><a class="jsx-3665177261 tag-link" href="/tags/next-js">#next-js</a><a class="jsx-3665177261 tag-link" href="/tags/react">#react</a></div></div></div></div></div><div class="jsx-3665177261 card-container"><div class="jsx-3665177261"><div class="jsx-3665177261 img-container"><img src="/images/go-gotchas-and-good-practices/go-gotchas-and-good-practices.jpg" alt="Go Gotchas, Tips, and Good Practices for Efficient Go" class="jsx-3665177261"/></div><div class="jsx-3665177261 info-container"><div class="jsx-3665177261 description-container"><p class="jsx-3665177261 date">Mar 25, 2024</p><a class="jsx-3665177261 info-container-link" href="/blog/go-gotchas-and-good-practices">Go Gotchas, Tips, and Good Practices for Efficient Go</a><p class="jsx-3665177261 description">Review some of the common mistakes we make while writing Go code and learn how to avoid them along with good practices for efficient Go.</p></div><div class="jsx-3665177261 tags-container"><div class="jsx-3665177261 tags-container-tag"><a class="jsx-3665177261 tag-link" href="/tags/go">#go</a></div></div></div></div></div><div class="jsx-3665177261 card-container"><div class="jsx-3665177261"><div class="jsx-3665177261 img-container"><img src="/images/advanced-golang/advanced-golang.jpg" alt="Advanced Go: Internals, Memory Model, Garbage Collection and Concurrency" class="jsx-3665177261"/></div><div class="jsx-3665177261 info-container"><div class="jsx-3665177261 description-container"><p class="jsx-3665177261 date">Mar 23, 2024</p><a class="jsx-3665177261 info-container-link" href="/blog/advanced-golang-memory-model-concurrency">Advanced Go: Internals, Memory Model, Garbage Collection and Concurrency</a><p class="jsx-3665177261 description">Deep dive into the Go&#x27;s Memory model, the internals of Go&#x27;s data structures, Garbage collection and Concurrency model.</p></div><div class="jsx-3665177261 tags-container"><div class="jsx-3665177261 tags-container-tag"><a class="jsx-3665177261 tag-link" href="/tags/go">#go</a><a class="jsx-3665177261 tag-link" href="/tags/concurrency">#concurrency</a></div></div></div></div></div><div class="jsx-3665177261 card-container"><div class="jsx-3665177261"><div class="jsx-3665177261 img-container"><img src="/images/concurrency-patterns-python/concurrency-patterns-python.jpg" alt="Asynchronous or Concurrency patterns in Python with Asyncio" class="jsx-3665177261"/></div><div class="jsx-3665177261 info-container"><div class="jsx-3665177261 description-container"><p class="jsx-3665177261 date">Mar 6, 2024</p><a class="jsx-3665177261 info-container-link" href="/blog/concurrency-patterns-python">Asynchronous or Concurrency patterns in Python with Asyncio</a><p class="jsx-3665177261 description">Effective implementation of asynchronous or concurrency patterns like Background task and Worker Pool in Python using Asyncio.</p></div><div class="jsx-3665177261 tags-container"><div class="jsx-3665177261 tags-container-tag"><a class="jsx-3665177261 tag-link" href="/tags/concurrency">#concurrency</a><a class="jsx-3665177261 tag-link" href="/tags/python-performance">#python-performance</a><a class="jsx-3665177261 tag-link" href="/tags/python">#python</a></div></div></div></div></div><div class="jsx-3665177261 card-container"><div class="jsx-3665177261"><div class="jsx-3665177261 img-container"><img src="/images/super-fast-python-numba/super-fast-python-numba.jpg" alt="Super fast Python (Part-5): Numba" class="jsx-3665177261"/></div><div class="jsx-3665177261 info-container"><div class="jsx-3665177261 description-container"><p class="jsx-3665177261 date">Dec 25, 2022</p><a class="jsx-3665177261 info-container-link" href="/blog/super-fast-python-numba">Super fast Python (Part-5): Numba</a><p class="jsx-3665177261 description">Speed up Numerical computations and functions in Python with Numba and Numpy.</p></div><div class="jsx-3665177261 tags-container"><div class="jsx-3665177261 tags-container-tag"><a class="jsx-3665177261 tag-link" href="/tags/python-performance">#python-performance</a><a class="jsx-3665177261 tag-link" href="/tags/python">#python</a></div></div></div></div></div><div class="jsx-3665177261 card-container"><div class="jsx-3665177261"><div class="jsx-3665177261 img-container"><img src="/images/super-fast-python-cython/super-fast-python-cython.jpg" alt="Super fast Python (Part-4): Cython" class="jsx-3665177261"/></div><div class="jsx-3665177261 info-container"><div class="jsx-3665177261 description-container"><p class="jsx-3665177261 date">Nov 18, 2022</p><a class="jsx-3665177261 info-container-link" href="/blog/super-fast-python-cython">Super fast Python (Part-4): Cython</a><p class="jsx-3665177261 description">Convert slow Python code to run as fast as C/C++ using Cython.</p></div><div class="jsx-3665177261 tags-container"><div class="jsx-3665177261 tags-container-tag"><a class="jsx-3665177261 tag-link" href="/tags/python-performance">#python-performance</a><a class="jsx-3665177261 tag-link" href="/tags/python">#python</a></div></div></div></div></div><div class="jsx-3665177261 card-container"><div class="jsx-3665177261"><div class="jsx-3665177261 img-container"><img src="/images/super-fast-python-multi-processing/super-fast-python-multi-processing.jpg" alt="Super fast Python (Part-3): Multi-processing" class="jsx-3665177261"/></div><div class="jsx-3665177261 info-container"><div class="jsx-3665177261 description-container"><p class="jsx-3665177261 date">Nov 12, 2022</p><a class="jsx-3665177261 info-container-link" href="/blog/super-fast-python-multi-processing">Super fast Python (Part-3): Multi-processing</a><p class="jsx-3665177261 description">Make computations in Python faster with Multi-processing .</p></div><div class="jsx-3665177261 tags-container"><div class="jsx-3665177261 tags-container-tag"><a class="jsx-3665177261 tag-link" href="/tags/python-performance">#python-performance</a><a class="jsx-3665177261 tag-link" href="/tags/python">#python</a></div></div></div></div></div><div class="jsx-3665177261 card-container"><div class="jsx-3665177261"><div class="jsx-3665177261 img-container"><img src="/images/super-fast-python-good-practices/super-fast-python-good-practices.jpg" alt="Super fast Python (Part-2): Good Practices" class="jsx-3665177261"/></div><div class="jsx-3665177261 info-container"><div class="jsx-3665177261 description-container"><p class="jsx-3665177261 date">Nov 9, 2022</p><a class="jsx-3665177261 info-container-link" href="/blog/super-fast-python-good-practices">Super fast Python (Part-2): Good Practices</a><p class="jsx-3665177261 description">Write Python programs by following good practices to run code incredibly faster.</p></div><div class="jsx-3665177261 tags-container"><div class="jsx-3665177261 tags-container-tag"><a class="jsx-3665177261 tag-link" href="/tags/python-performance">#python-performance</a><a class="jsx-3665177261 tag-link" href="/tags/python">#python</a></div></div></div></div></div><div class="jsx-3665177261 card-container"><div class="jsx-3665177261"><div class="jsx-3665177261 img-container"><img src="/images/super-fast-python-why-python-slow/super-fast-python-why-python-slow.jpg" alt="Super fast Python (Part-1): Why Python is Slow?" class="jsx-3665177261"/></div><div class="jsx-3665177261 info-container"><div class="jsx-3665177261 description-container"><p class="jsx-3665177261 date">Nov 7, 2022</p><a class="jsx-3665177261 info-container-link" href="/blog/super-fast-python-why-python-slow">Super fast Python (Part-1): Why Python is Slow?</a><p class="jsx-3665177261 description">Why Python is slow compared to C/C++ or Java? And where can we improve our code to run fast?</p></div><div class="jsx-3665177261 tags-container"><div class="jsx-3665177261 tags-container-tag"><a class="jsx-3665177261 tag-link" href="/tags/python-performance">#python-performance</a><a class="jsx-3665177261 tag-link" href="/tags/python">#python</a></div></div></div></div></div><div class="jsx-3665177261 card-container"><div class="jsx-3665177261"><div class="jsx-3665177261 img-container"><img src="/images/retinex-image-enhancement/retinex-image-enhancement.jpg" alt="Image Enhancement using Retinex Algorithms" class="jsx-3665177261"/></div><div class="jsx-3665177261 info-container"><div class="jsx-3665177261 description-container"><p class="jsx-3665177261 date">Mar 23, 2022</p><a class="jsx-3665177261 info-container-link" href="/blog/retinex-image-enhancement">Image Enhancement using Retinex Algorithms</a><p class="jsx-3665177261 description">Enhance low-light images using Retinex algorithms with Fast Fourier Transform in Python.</p></div><div class="jsx-3665177261 tags-container"><div class="jsx-3665177261 tags-container-tag"><a class="jsx-3665177261 tag-link" href="/tags/ai">#ai</a><a class="jsx-3665177261 tag-link" href="/tags/image-processing">#image-processing</a><a class="jsx-3665177261 tag-link" href="/tags/color-science">#color-science</a><a class="jsx-3665177261 tag-link" href="/tags/opencv">#opencv</a><a class="jsx-3665177261 tag-link" href="/tags/python">#python</a></div></div></div></div></div><div class="jsx-3665177261 card-container"><div class="jsx-3665177261"><div class="jsx-3665177261 img-container"><img src="/images/retinex-theory/retinex-theory.jpg" alt="Retinex Theory of Color Vision" class="jsx-3665177261"/></div><div class="jsx-3665177261 info-container"><div class="jsx-3665177261 description-container"><p class="jsx-3665177261 date">Feb 28, 2022</p><a class="jsx-3665177261 info-container-link" href="/blog/retinex-theory-of-color-vision">Retinex Theory of Color Vision</a><p class="jsx-3665177261 description">Retinex theory explains the color constancy of Human Visual System that used in many Image processing applications.</p></div><div class="jsx-3665177261 tags-container"><div class="jsx-3665177261 tags-container-tag"><a class="jsx-3665177261 tag-link" href="/tags/color-science">#color-science</a></div></div></div></div></div><div class="jsx-3665177261 card-container"><div class="jsx-3665177261"><div class="jsx-3665177261 img-container"><img src="/images/chromatic-adaptation/chromatic-adaptation.jpg" alt="Chromatic adaptation (Color constancy)" class="jsx-3665177261"/></div><div class="jsx-3665177261 info-container"><div class="jsx-3665177261 description-container"><p class="jsx-3665177261 date">Nov 26, 2021</p><a class="jsx-3665177261 info-container-link" href="/blog/chromatic-adaptation">Chromatic adaptation (Color constancy)</a><p class="jsx-3665177261 description">Color constancy of an image using Chromatic adaptation technique in Python.</p></div><div class="jsx-3665177261 tags-container"><div class="jsx-3665177261 tags-container-tag"><a class="jsx-3665177261 tag-link" href="/tags/image-processing">#image-processing</a><a class="jsx-3665177261 tag-link" href="/tags/color-science">#color-science</a><a class="jsx-3665177261 tag-link" href="/tags/opencv">#opencv</a><a class="jsx-3665177261 tag-link" href="/tags/python">#python</a></div></div></div></div></div><div class="jsx-3665177261 card-container"><div class="jsx-3665177261"><div class="jsx-3665177261 img-container"><img src="/images/whiteboard-enhance/whiteboard-image-enhancement.jpg" alt="Whiteboard Image Enhancement using OpenCV" class="jsx-3665177261"/></div><div class="jsx-3665177261 info-container"><div class="jsx-3665177261 description-container"><p class="jsx-3665177261 date">Oct 19, 2021</p><a class="jsx-3665177261 info-container-link" href="/blog/whiteboard-image-enhancement-opencv-python">Whiteboard Image Enhancement using OpenCV</a><p class="jsx-3665177261 description">Enhance whiteboard images taken from mobile using OpenCV.</p></div><div class="jsx-3665177261 tags-container"><div class="jsx-3665177261 tags-container-tag"><a class="jsx-3665177261 tag-link" href="/tags/image-processing">#image-processing</a><a class="jsx-3665177261 tag-link" href="/tags/opencv">#opencv</a><a class="jsx-3665177261 tag-link" href="/tags/python">#python</a></div></div></div></div></div><div class="jsx-3665177261 card-container"><div class="jsx-3665177261"><div class="jsx-3665177261 img-container"><img src="/images/color-theory/color-theory.jpg" alt="Color Theory" class="jsx-3665177261"/></div><div class="jsx-3665177261 info-container"><div class="jsx-3665177261 description-container"><p class="jsx-3665177261 date">Sep 14, 2021</p><a class="jsx-3665177261 info-container-link" href="/blog/color-theory">Color Theory</a><p class="jsx-3665177261 description">Different color properties, color models, and color space that are useful in image processing, graphic design, and game design.</p></div><div class="jsx-3665177261 tags-container"><div class="jsx-3665177261 tags-container-tag"><a class="jsx-3665177261 tag-link" href="/tags/color-science">#color-science</a></div></div></div></div></div><div class="jsx-3665177261 card-container"><div class="jsx-3665177261"><div class="jsx-3665177261 img-container"><img src="/images/color-science/color-science.jpg" alt="Color Science" class="jsx-3665177261"/></div><div class="jsx-3665177261 info-container"><div class="jsx-3665177261 description-container"><p class="jsx-3665177261 date">Aug 28, 2021</p><a class="jsx-3665177261 info-container-link" href="/blog/color-science">Color Science</a><p class="jsx-3665177261 description">Understanding the concept of Color from human eye&#x27;s perception to digital world representation.</p></div><div class="jsx-3665177261 tags-container"><div class="jsx-3665177261 tags-container-tag"><a class="jsx-3665177261 tag-link" href="/tags/color-science">#color-science</a></div></div></div></div></div><div class="jsx-3665177261 card-container"><div class="jsx-3665177261"><div class="jsx-3665177261 img-container"><img src="/images/note-app-flutter/note-app-in-flutter.jpg" alt="Create a Notes App with Flutter" class="jsx-3665177261"/></div><div class="jsx-3665177261 info-container"><div class="jsx-3665177261 description-container"><p class="jsx-3665177261 date">Jun 15, 2021</p><a class="jsx-3665177261 info-container-link" href="/blog/create-a-notes-app-with-flutter">Create a Notes App with Flutter</a><p class="jsx-3665177261 description">Create a color-rich Note-taking app with Flutter.</p></div><div class="jsx-3665177261 tags-container"><div class="jsx-3665177261 tags-container-tag"><a class="jsx-3665177261 tag-link" href="/tags/flutter">#flutter</a></div></div></div></div></div><div class="jsx-3665177261 card-container"><div class="jsx-3665177261"><div class="jsx-3665177261 img-container"><img src="/images/blog-nextjs-mdx/nextjs.jpeg" alt="Build Blog with Next.js and MDX &amp; Deploy to Github Pages" class="jsx-3665177261"/></div><div class="jsx-3665177261 info-container"><div class="jsx-3665177261 description-container"><p class="jsx-3665177261 date">Dec 31, 2020</p><a class="jsx-3665177261 info-container-link" href="/blog/build-blog-with-nextjs-mdx-and-deploy-to-github-pages">Build Blog with Next.js and MDX &amp; Deploy to Github Pages</a><p class="jsx-3665177261 description">Create a blog with Next.js as Static Site Generator, MDX for writing content, Github Pages for deploying the static website. Also add SEO and Image optimization.</p></div><div class="jsx-3665177261 tags-container"><div class="jsx-3665177261 tags-container-tag"><a class="jsx-3665177261 tag-link" href="/tags/react">#react</a><a class="jsx-3665177261 tag-link" href="/tags/next-js">#next-js</a></div></div></div></div></div></div><div class="jsx-931837452 footer-container"><div class="jsx-931837452 footer-container-links"><div class="jsx-931837452 footer-links"><div class="jsx-931837452 link-item"><a href="https://github.com/santhalakshminarayana" rel="noreferrer" target=" _blank" class="jsx-931837452 link"><div><svg stroke="currentColor" fill="currentColor" stroke-width="0" role="img" viewBox="0 0 24 24" style="color:#fffecb;font-size:calc(1rem + 1vw)" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><title></title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg></div></a></div><div class="jsx-931837452 link-item"><a href="https://www.linkedin.com/in/santhalakshminarayana/" rel="noreferrer" target="_blank" class="jsx-931837452 link"><div><svg stroke="currentColor" fill="currentColor" stroke-width="0" role="img" viewBox="0 0 24 24" style="color:#fffecb;font-size:calc(1rem + 1vw)" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><title></title><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"></path></svg></div></a></div><div class="jsx-931837452 link-item"><a href="https://medium.com/@santhalakshminarayana/" rel="noreferrer" target="_blank" class="jsx-931837452 link"><div><svg stroke="currentColor" fill="currentColor" stroke-width="0" role="img" viewBox="0 0 24 24" style="color:#fffecb;font-size:calc(1rem + 1vw)" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><title></title><path d="M13.54 12a6.8 6.8 0 01-6.77 6.82A6.8 6.8 0 010 12a6.8 6.8 0 016.77-6.82A6.8 6.8 0 0113.54 12zM20.96 12c0 3.54-1.51 6.42-3.38 6.42-1.87 0-3.39-2.88-3.39-6.42s1.52-6.42 3.39-6.42 3.38 2.88 3.38 6.42M24 12c0 3.17-.53 5.75-1.19 5.75-.66 0-1.19-2.58-1.19-5.75s.53-5.75 1.19-5.75C23.47 6.25 24 8.83 24 12z"></path></svg></div></a></div></div></div><div class="jsx-931837452 copy-right-container"><p class="jsx-931837452 copy-right-text">Santha Lakshmi Narayana © 2024</p></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postsMetaData":[{"metadata":{"title":"MySQL performance optimization techniques","description":"Improve MySQL performance in production by configuring various optimization techniques and tuning InnoDB settings.","imgName":"mysql-performance-optimization-techniques/mysql-performance-optimization-techniques.jpg","date":"Nov 10, 2024","tags":["mysql","system-design"],"keywords":["mysql","mysql-performance-optimization","speed-up-mysql","innodb-performance-configuration","mysql-tuning"],"id":"mysql-performance-optimization-techniques"},"content":"\n![MySQl performance optimization techniques](mysql-performance-optimization-techniques/mysql-performance-optimization-techniques.jpg)\n\n###### Published on: **Nov 10, 2024**\n\n# MySQL performance optimization techniques\n\nMySQL is the top 2 most used RDBMS for production. It scales well vertically for small-medium-size databases. The default configuration that MySQL comes with works well for only the initial stages of deployment. For deploying in production, the default settings must be tuned for better performance. Other than the parameters/settings, we also need to take care of how we handle/interact with the MySQL like query optimization, indexes, connection pool, etc.\n\nThis blog goes through some of the most important configurations that need to be looked at, which improve MySQL performance by multiplefold with fewer settings.\n\n\n---\n\n### DB size\n \nFirst, check the total DB data size to better understand the type of improvements and tuning to apply to various parameters.\n\n- total DB size (both data and index size):\n```text\nSELECT ROUND(SUM(data_length + index_length) / 1024 / 1024, 2) AS \"Size (MB)\" \nFROM information_schema.TABLES;\n```\n- by each database:\n```text\nSELECT table_schema AS \"Database\", ROUND(SUM(data_length + index_length) / 1024 / 1024, 2) AS \"Size (MB)\" \nFROM information_schema.TABLES \nGROUP BY table_schema;\n```\n- by table in **database_name**: \n```text\nSELECT table_name AS \"Table\", ROUND(((data_length + index_length) / 1024/ 1024), 2) AS \"Size (MB)\" \nFROM information_schema.TABLES \nWHERE table_schema = \"database_name\" \nORDER BY (data_length + index_length) DESC;\n```\n\n\n---\n\n## InnoDB Architecture\n\nIt is crucial to understand the InnoDB architecture and how it saves data into memory because configuring InnoDB related parameters like *innodb_buffer_pool_size* results in boosted performance. The high-level architectural components of InnoDB we are interested in are:\n- Tablespace \u0026 Data Pages\n- InnoDB buffer pool\n- Dublewrite buffer\n- Undo Log\n- Redo Log\n\n![InnoDB Architecture:=:60:=:InnoDB Architecture$$$https://dev.mysql.com/doc/refman/8.4/en/innodb-architecture.html](mysql-performance-optimization-techniques/innodb-architecture.png)\n\nData is stored in tables as data pages under tablespace generally with a file-per-table mode in harddisk. Reading the pages from the disk for every operation adds latency and MySQL can't serve multiple requests concurrently. So, InnoDB reads the data pages and loads them into a buffer in memory (RAM) for fast access called **Buffer Pool**. The larger the buffer pool size, the more read hits the InnoDB can get from this buffer cache. So, it is crucial to allocate more buffer pool memory. Generally, **50-75%** of memory is recommended to allocate for the buffer pool. InnoDB first checks if the data page is available in the buffer pool, if not it loads them to memory. To clean up the space for newer pages, InnoDB removes the pages based on the LRU method.\n\nThe buffer pool is all about optimizing reads faster. If any write happens to those pages in the memory, instead of writing them directly to the disk, changes happen to the pages in the buffer pool itself. This is to save the time for I/O operations. \n\nRegarding writes, any write operation is first recorded in the **Redo Log** buffer as **Write-ahead Log** for recovery and backup purposes. After writing to the log buffer has become a success then only the transactions happen to the data pages in the buffer pool. So, when the crash happens, InnoDB can redo the un-committed transactions by reading through these Redo Logs. Logs are stored in the buffer and later flushed to disk.\n\nAny transaction writes are first stored in **Undo Log** where it stores the before-data of the affected rows of the transactions. This is also used for transaction rollback or crash recovery when the system crashes while performing any transactions or simply to undo the transaction changes. When multiple connections read the same data that another connection is locked in, they all see the before data stored in the undo logs.\n\nData pages in the buffer that are changing due to transactions are called **Dirty Pages** and are flushed to actual disk storage later when the checkpoint happens. InnoDB uses fuzzy checkpoints rather than static/fixed checkpoint intervals to flush the pages. Fuzzy checkpoint means instead of stopping the new transactions record for flushing the data synchronously, analyze and find the dirty pages and flush them asynchronously in the background without stopping new transactions.\n\nFor recovery purposes, data is first synced to **Doublewrite buffer** in the disk and then directly in tablespace files. This is because, after the crash, InnoDB checks the data in this double-write buffer and the actual data, and syncs them by each page. The operations needed to perform for actual data sync are more than simply writing the pages to the double-write buffer. So, it is faster and the pages are flushed to buffer in batches rather than file-by-file.\n\n**Ref**:\n- https://www.alibabacloud.com/blog/mysql-memory-allocation-and-management-part-ii_600992\n- https://hidetatz.medium.com/how-innodb-writes-data-on-the-disk-1b109a8a8d14\n- https://www.percona.com/blog/understanding-the-differences-between-innodb-undo-log-and-redo-log/\n- https://www.alibabacloud.com/blog/what-are-the-differences-and-functions-of-the-redo-log-undo-log-and-binlog-in-mysql_598035\n\n---\n\n### Innodb buffer pool hit ratio check\n\nShow InnoDB buffer present config\n```text\nSHOW engine innodb status;\n```\n\nGet buffer pool size(bytes)\n\n```text\nSELECT ROUND(@@innodb_buffer_pool_size / 1024 / 1024 / 1024, 2) AS \"Buffer pool size (GB)\";\n```\n\nGet pages read and read requests from the buffer pool\n```text\nSELECT variable_name, variable_value from sys.metrics \nWHERE variable_name IN ('innodb_pages_read', 'innodb_buffer_pool_read_requests');\n```\n\n$$\nHit\\text{\\textendash}Ratio=100 \\times (1 - (\\frac{innodb\\_pages\\_read}{innodb\\_buffer\\_pool\\_read\\_requests}))\n$$\n\nIf the hit ratio is not close to **100%** then increase the buffer pool size to allow the maximum DB data to fit but within memory requirements.\n\n\n---\n\n### Query optimization\n\nUse **explain**, **analyze**, and **format**={json, tree} to analyze the query breakdown steps, indexes used, and operations performed to get a hold on where we can improve the query processing by adding necessary indexes, dropping unnecessary conditions, re-ordering query steps, and other bottleneck steps.\n- Check the query optimization plan\n- Add indexes like composite index and covering index to completely cover all columns in the query for super fast lookup\n- Re-write query as a **SARGABLE**(Search ARGument ABLE) query which utilizes the index for quick searching instead of working on each row\n- Avoid functions and calculations on index columns in the query as these will apply for each row\n- For function-based where clause, create a function-based index on the column\n- Limit the amount of data rows that need to be sorted by better query execution planning\n- Rewrite JOINs into a subqueries sometimes gives better results\n- Order of execution: FROM, JOIN, WHERE, GROUP BY, HAVING, SELECT, ORDER BY, LIMIT\n\n\n### Indexes in MySQL\n\nEvery table should have a primary key. If a primary key is not defined, then InnoDB uses the first unique non-null column as the primary key. If no such column exists, InnoDB creates a hidden row incremental key for the primary key. \n\nSome of the trade-offs to consider are\n- Sequential vs Non-sequential primary key\n- Clustered vs Non-clustered index\n- Single-level index vs Composite indexes (and Covering index) \n- Single-level index vs Column order index\n\n\nIf there are redundant indexes, then write operations takes time, check and remove the redundant indexes\n```text\nselect * from sys.schema_redundant_indexes;\n```\n\nAlso, check how much memory an index takes, and if it is not in acceptable levels, drop that index and change the schema for better performance.\n\n---\n\n## Connection Pool\n- A DB connection comprises opening the tcp socket, acknowledgement, authentication, authorization, network session creation, etc. So, it takes time to open a new connection every time. So maintain a pool of connections to re-use.\n- By default, the *max_open_connections* a MySQL server can handle is 150, but can be set up to 2^32 (but up to 100,000 should be the limit generally).\n- Types of connection pooling:\n  - Session: Maintain connection until the session completes. The client can make any number of transactions until connection timeout is reached.\n  - Transaction: Connection is returned to the pool when the transaction completes.\n  - Statement: A connection is used only for a single SQL statement.\n- Generally, *max_pool_size* (*max_active_connections*) is set to (2 or 4 * no.of cores), but it varies depending on the type of application and the traffic.\n- For normal setup, *max_idle_connections* (ex: 80) will be less than *max_pool_size* (ex: 100, and 20 connections will be closed after use as the max idle connections are 80).\n- For high concurrent systems, set the *max_idle_connections* the same as the *max_pool_size*, idle connections take some memory but it's a trade-off compared to the overhead of opening connections for highly frequent requests. \n\n### Scaling challenges\n- Max open connections, idle connections, and pool size should be limited considering the system resources limit.\n- System resources like memory, CPU cache, and data storage are required if there are more connections opened at a time. If the system can't handle more connections, all operations will be rejected and that leads to data inconsistency.\n- In Linux servers, the *ulimit* restricts the max open file descriptors.\n- MySQL is multi-threaded and allocates one thread per connection which requires thread management overhead along with system resources.\n- Configuration variables like *thread_cache_size* ({8 + (max_connections/100)} defaults to 8-100, and multiplexed to cores) which defines how many threads can be cached for re-use when the client disconnects. This also requires additional memory but improves performance.\n- MySQL creates a THD (Thread Handle Descriptor) for each connection with a minimum memory of ~10KB and can grow to ~10MB for average connection when executing queries. So, handling huge no.of parallel connections requires huge memory requirements and also high thrashing.\n- As more no.of connections increases but the max_connections are set in limit which are nothing but user threads, there has to be a balance between the user thread-per-core ratio (max ratio recommended is ~4) and the latency. So, based on this, the transactions-per-second (TPS) the server can handle can be determined, and has to scale the DB for the expected load.\n- Another important challenge is the underlying disk storage. If there is huge data stored, the user threads spend most of the time for data to arrive from disk. So, better disk storage mechanism has to be considered like SSDs, cache, read/write heavy disks, etc.\n- In MySQL thread pool (enterprise edition), correctly tuning the *thread_pool_size* and *max_transaction_limit* for high concurrency is very difficult but it's better than the default thread handling mechanism.\n\n**Ref**:\n- https://dev.mysql.com/blog-archive/mysql-connection-handling-and-scaling/\n- https://dev.mysql.com/blog-archive/the-new-mysql-thread-pool/\n- https://dev.to/dbvismarketing/a-guide-to-multithreading-in-sql-3hh1\n\n\n---\n\n## Transactions Isolation\n\nIn high concurrent transaction systems, it is important to isolate the transactions for data consistency. so, when multiple connections try to change the same data, how can transaction isolation be maintained so that the following problems can be avoided?\n- **Dirty Reads**: A query in a transaction may return inconsistent data due to uncommitted changes in other transactions.\n- **Non-repeatable Reads**: The same query in a transaction reads different data rows if executed multiple times. This may happen due to other transactions committed to the changes.\n- **Phantom Reads**: The data rows returned by the select statements differ within the same transaction as another transaction might have inserted new rows.\n\nThe following isolation levels can be set in MySQL for transaction isolation:\n- **Read Uncommitted**: Transactions see other transactions' un-committed changes that may cause data inconsistencies. Suitable for highly frequent updates where accuracy/consistency is not critical. Ex: Dashboards, Analytics. Solves: None\n- **Read Committed**: Transactions see other transactions changes only if they are committed. Consistent data but anything can happen between transactions. Suitable for both highly frequent and data-consistent scenarios. Ex: Banking, reservations. Solves: Dirty Reads.\n- **Repeatable Read**: A transaction sees the same snapshot of data throughout the life cycle and is unaffected by other transactions. Default in MySQL with InnoDB's MVCC(Multi-Version Concurrency Control). Suitable for data-consistent systems where performance can be compromised. Ex: General Web services. Solves Dirty Reads, Non-Repeatable Reads\n- **Serialization**: Transactions lock the rows restricting other transactions to wait until completion. Suitable for high data consistent systems. Ex: Financial services. Solves: Dirty Reads, Non-repeatable Reads, Phantom Reads.\n\nSo, based on the type of requirements, set the isolation level for high concurrent database transactions.\n\n**Ref**:\n- https://planetscale.com/blog/mysql-isolation-levels-and-how-they-work\n- https://dev.to/eyo000000/a-straightforward-guide-for-mysql-locks-56i1\n- https://simon-ninon.medium.com/dont-break-production-learn-about-mysql-locks-297671ec8e73\n\n\n---\n\n## MySQL Monitoring\n\nMySQL can handle high concurrent traffic with a single instance if it is deployed with high CPU cores, fast access storage (SSD), and high RAM. Some of the variables to look out for monitoring are \n- max_connections\n- table_open_cache\n- threads_connected\n- threads_running\n- thread_cache_size\n- innodb_buffer_pool_size\n\nAlso, monitor the status variables like\n- Com_select, Com_insert, Com_update, Com_delete\n- status_queries\n- max_used_connections\n\nCheck the reads and writes for a table with index used or not. Read count increases even without any operations also due to various reasons like background processes, cache eviction, etc., So these are not very accurate but give the proportion of operations that MySQL performs.\n```text\nselect object_schema, object_name, count_read, count_write, index_name \nfrom performance_schema.table_io_waits_summary_by_index_usage \norder by count_read+count_write desc limit 5;\n```\n\nCheck the max percentage of concurrent connections at a time relative to max connections. If the value touches \u003e95%, then increase the max_connections value.\n```text\n100 * threads_connected / max_connections\n```\n\nTune the following for better performance\n- Increase the *max_connections* and *thread_cache_size* for handling high concurrent connections with reuse.\n- Increase the *innodb_buffer_pool_size* for more cached data and less time for the server to read from the disk.\n- Monitor the **p99** which measures the latency of 99% of all transactions and check if that latency is within the defined limits.\n\n\n---\n\n### MySQL configuration parameters\n\nHaving discussed the various topics and internals of MySQL, the following are some of the important parameters that affect the MySQL performance. As the list is very large, I have just mentioned the names and they require separate discussion deeply.\n\n- default-character-set = utf8mb4\n- max_connections\n- max_user_connections, max_allowed_packet, for limiting DDoS attacks\n- innodb_change_buffering\n- innodb_buffer_pool_size\n- innodb_buffer_pool_instances, if innodb_buffer_pool_size is more than 1GB set multiple instances\n- innodb_lru_scan_depth\n- innodb_io_capacity, innodb_io_capacity_max\n- innodb_max_dirty_pages_pct\n- innodb_flush_neighbors\n- innodb_read_io_threads, innodb_write_io_threads\n- innodb_flush_log_at_trx_commit, 1 is the default for ACID compliance. 0 or 2 for fast writes. For master keep it as 1.\n- innodb_log_buffer_size\n- innodb_log_file_size, innodb_log_files_in_group, increase these for write-heavy, for read-heavy these are not important\n- innodb_flush_method, for SSD set value as O_DIRECT\n- innodb_adaptive_flushing\n- innodb_stats_on_metadata\n- innodb_doublewrite\n- innodb_purge_threads\n- innodb_thread_concurrency\n- innodb_parallel_read_threads\n- thread_cache_size\n- have_query_cache, disable for read-heavy production\n- wait_timeout, interactive_timeout\n- sort_buffer_size, read_rnd_buffer_size, join_buffer_size\n- performance_schema, disable if not needed\n- max_prepared_stmt_count, restrict to limit memory leaks\n- tmp_table_size, max_heap_table_size\n- table_open_cache, should be higher than opened_tables\n- table_open_cache_instances, table_definition_cache\n- slow_query_log, log_query_time\n\n\n---\n\nFor better performance, monitor the following things continuously\n- QPS (queries per second) under load\n- Max concurrent connections at peak\n- Read/write ratio to enhance the performance by replicas/partitions\n- Buffer pool hit ratio\n- No.of I/O operations under load\n- CPU load with I/O wait time\n- p99 or p95 latency\n\n\n---\n\n**Ref**:\n- https://github.com/major/MySQLTuner-perl/blob/master/INTERNALS.md\n- https://www.percona.com/blog/mysql-101-parameters-to-tune-for-mysql-performance/\n- https://www.woktron.com/secure/knowledgebase/272/How-to-optimize-MySQL-performance.html\n- https://www.red-gate.com/simple-talk/databases/mysql/optimizing-my-cnf-for-mysql-performance/\n- https://releem.com/docs/mysql-performance-parameters\n- https://blog.searce.com/fewer-cpu-cores-will-blockyour-database-iops-in-gcp-b817b8cb8af2\n- https://blog.searce.com/how-max-prepared-stmt-count-bring-down-the-production-mysql-system-6ca28e577663\n- https://tanishiking24.hatenablog.com/entry/innodb-durability\n"},{"metadata":{"title":"Software Development Comprehensive Guide","description":"SDLC, System Design \u0026 Architecture, Distributed Systems, Microservices, Database Replication \u0026 Partition.","imgName":"software-development-comprehensive-guide/software-development-comprehensive-guide.jpg","date":"Oct 31, 2024","tags":["system-design"],"keywords":["sdlc","system-design","design-patterns","system-architecture","system-design-and-architecture","software-design-and-architecture","distributed-systems","microservices","data-replication","data-partition"],"id":"software-development-comprehensive-guide"},"content":"\n![Software Development Comprehensive Guide](software-development-comprehensive-guide/software-development-comprehensive-guide.jpg)\n\n###### Published on: **Oct 31, 2024**\n\n# Software Development Comprehensive Guide\n\n## Software Development Life Cycle (SDLC)\n\n### Phases\n- Planning\n- Requirements\n  - Software Requirement Specification (SRS)\n- Design\n  - Design Document Specification (DDS)\n- Development\n  - Greenfield development\n  - Brownfield development\n- Testing/Integration\n- Deployment/Release\n- Maintenance\n\n### SDLC Models\n- Waterfall\n- Spiral\n- Iterative/Incremental\n- V-shaped\n- Agile (Scrum)\n- Big Bang\n\n### Software Requirements\n- Functional: What the software should provide\n  - ex: User registration, video download\n- Non-functional: How the software should perform, its behavior, quality, and other constraints\n  - ex: Performance (\u003c100ms latency), security, 99.9999% availability\n- Domain: How the software should fit for the domain (rules, terminology, expectations)\n\n**Types of requirements**\n- User\n- Business\n- Domain\n- System\n- Design\n\n**Ref:**\n- https://www.geeksforgeeks.org/software-development-life-cycle-sdlc/\n\n\n---\n\n## Software Testing\n\n### Functional testing\n- Unit testing\n  - Gorilla testing (random input and regressive testing)\n- Interface testing\n- Integration testing\n- Smoke testing (un-stable build)\n- Regression testing\n  - Sanity testing (stable build)\n- System testing\n  - End-to-end testing\n  - Monkey testing (random input and order testing)\n- Acceptance testing\n  - Alpha testing\n  - Beta testing\n  - User acceptance testing\n\n### Non-functional testing\n- Performance\n  - Load testing\n  - Stress testing\n- Security\n- Reliability\n- Usability\n- Compatibility\n\n**Testign Techniques**\n- Black box\n- White box\n- Gray box\n\n**Ref:**\n- https://www.browserstack.com/guide/types-of-testing\n- https://testsigma.com/blog/the-different-software-testing-types-explained/\n- https://www.softwaretestinghelp.com/\n\n\n---\n\n## Software Quality\n\n### Characteristics\n- Functionality\n- Usability\n  - Portability\n  - Interoperability\n  - Flexibility\n  - 5 9s availability\n- Efficiency\n  - Modularity\n  - Scalability\n  - Performance\n  - Security\n- Reliability\n  - Recoverability\n  - Fault Tolerance\n- Maintainability\n  - Testability\n  - Stability\n  - Reusability\n  - Adaptability\n\n### Code smells\n- **Rigidity**: The more modules get affected by a single change the more rigidity is\n- **Fragility**: Introduction of changes cause code to break in multiple places\n- **Immobility**: It is hard to reuse the same type of software for other projects as the software in question is too dependent on many things so it is hard to separate\n- **Viscosity**: Low viscosity when the new changes preserve the design. If it is not possible to preserve the design without employing the hacks then the code is highly viscous\n- **Complexity**: Unncessary code at that time making the code complex like adding unnecessary methods to a module anticipating future \n- **Repetition**: Writing the same type of code in multiple places instead of writing it as a separate function of the module\n- **Opacity**: Over the age, code becomes tough to understand\n- **Coupling**: High coupling means two or more modules are tightly dependent on each other resulting in rigidness and immobility\n- **Cohesion**: Cohesion is the degree to which components in a module work together for a single functionality. Low cohesion means it is difficult to understand and maintain the module as it serves multiple purposes leading to complexity and opacity\n\n**Ref:**\n- https://www.linkedin.com/pulse/symptoms-rotten-software-design-why-engineer-should-know-milan-ashara\n- https://refactoring.guru/refactoring/smells\n- https://medium.com/globant/characteristics-of-a-poor-software-design-de71e7b7a73c\n\n\n---\n\n## System Design vs Software Design\n\n**System design**: Focuses on the overall architecture of the system involving multiple components (hardware and software) and their interaction and interface. And also emphasizing efficiency, scalability, performance, integration, etc.\n  - Ex: High-Level Architecture, Microservices, Message Communication between services, Integration (Payment, Authentication), CDN, HA, Deployment, Scalability, Security, Performance, Maintainability\n\n**Software design**: Focuses on the individual components within software like classes, modules, structures, algorithm implementation, and their interactions. Also creates detailed coding implementation designs to meet functional and non-functional requirements. It encompasses both HLD and LLD.\n  - Ex: Class and Sequence diagrams, UX, APIs, Algorithms and Structures, Failure handling\n\n\n---\n\n## System Design vs System Architecture Design\n\nSystem design is a broad aspect of preparing a blueprint of the system that translates the requirements and the scope of the software into implementation. It involves high-level architecture designs, interfaces, and interaction with both internal and external components/systems, maintainability, performance, scalability, etc. It also includes both HLD and LLD.\n\nWhereas System Architecture is a subset of system design that mainly focuses on high-level system structure, organization, relationship, coordination, principles, and guidelines for various components and how they relate to each other.\n\nSystem design is more detailed and specific about all components and behavior of the system. Architecture defines the boundaries and overall structure of the system. If system architecture is about defining the major components of an app, then system design states how to display the data, how many users to server and load, how to communicate with external systems for getting data, and what will happen when any component fails.\n\n\n---\n\n### High-level Design (HLD) vs Low-level Design (LLD)\n\nHigh-level design (HLD) provides an overall architecture and organization of the system involving major components and their interactions. Low-level design (LLD) focuses on the detailed implementation of each component or sub-system like how to define modules and classes, what are data structures and algorithms needed, and how all of these components interact with each other and other external systems. LLD is the realization of HLD.\n\nHLD includes defining components like services, databases, types of interfaces, and architectural patterns. LLD includes design patterns including classes and modules, API design, and error handling.\n\n\n---\n\n## Design principles\n\n### SOLID\n- **Single Responsibility Principle (SRP)**: A component must serve one functionality to reduce coupling and increase cohesion.\n- **Open-Closed Principle**: Closed for modification but open for extension. Modifying the code makes other parts change. Code should not be altered for new changes and design should be preserved.\n- **Liskov Substitution Principle**: Componenets interacts through contract.\n- **Interface Segregation Principle**: A set of small pieces contribute to a much larger component. Break down the large interfaces into small ones if they are used by multiple classes. This means clients do not need to use unnecessary methods that they don't have. \n- **Dependency Inversion Principle**: Dependency between low and high-level modules should be based on abstraction. If Class A depends on Class B, then create an interface I that Class B should satisfy and make Class A depend on the interface rather than the concrete class.\n\n### Guidelines\n- Don't Repeat Yourself (DRY): Don't repeat code and avoid duplication.\n- Separation of Concerns: Separate common code into modules/layers or a unit code piece.\n- Persistence Ignorance: Persistent code logic shouldn't be affected by the choice of persistence hardware/technology.\n- Do One Thing: Any component should mean one thing in any given context and must do only one thing/\n- Keep It Stupid Simple (KISS): Keep the design and code simple which results in fewer bugs, easier to modify, and less complexity.\n- Don't make think: Code should be easier to understand.\n- You Aren't Gonna Need It (YAGNI): Design/implement things when you absolutely need them, don't anticipate the future, and over-design.\n- Don't optimize pre-maturely: First concentrate on the software completion and delivery instead of being struck with optimization guilt.\n- Refactoring Hell: Don't struck in the refactoring loop as the code never becomes optimized.\n- Don't Re-invent the Wheel: Don't waste time designing that already exists.\n\n**Ref**:\n- https://dave.cheney.net/2016/08/20/solid-go-design\n\n\n---\n\n## Design Patterns\n\nDesign patterns are common solutions for typical problems in software development that can be reusable in situations like flexible components creation (**Creational**), composition of different components (**Structural**), and interaction and communaction between multiple components (**Behavioral**).\n\n### Creational \n- Singleton\n- Factory\n- Factory Method\n- Abstract Factory\n- Builder\n- Prototype\n- Object Pool\n- Lazy Initialization\n- Dependency Injection\n\n### Structural Patterns\n- Adapter\n- Bridge\n- Composite\n- Decorator\n- Facade\n- Flyweight\n- Proxy\n\n### Behavioral\n- Chain of Responsibility\n- Command\n- Interpreter\n- Iterator\n- Mediator\n- Memento\n- Observer\n- State\n- Strategy\n- Template Method\n- Visitor\n- Null Object\n- Blackboard\n- Fluent Interface\n\n**Ref**:\n- https://en.wikipedia.org/wiki/Software_design_pattern\n- https://www.oodesign.com\n- https://sourcemaking.com/design_patterns\n- https://refactoring.guru/design-patterns\n\n\n---\n\n## Clean Architecture\n\n**Principles**:\n- Separation of Concerns\n- Modularity\n- Dependency Rule\n- Testability\n\n**Benefits**\n- Independence of frameworks\n- Independent Testing\n- Platform Independence\n- Swap dependencies easily\n- Decouple\n\n### Architecture Styles Evolution\n- N-layer\n  - Presentation (UI), Business Logic, Data Access\n- Domain-driven design\n  - Presentation, Application (Controller), Domain (Service), Infrastructure (Data)\n- Hexagon\n  - Core connected by Ports (Interface) and Adapters (implementation)\n  - Core implements business rules and functionalities independently of external services\n  - External services are kinda plugins attached through ports, and without which the core still functions as usual.\n- Onion\n  - Domain, Application Service, Infrastructure Service, Presentation\n- Clean\n  - Entities, Application Use cases, Interface Adapters, Frameworks \u0026 Drivers\n\n**Ref**\n- https://sd.blackball.lv/en/articles/read/19191-exploring-the-evolution-of-backend-software-architecture-n-layered\n- https://herbertograca.com/2017/11/16/explicit-architecture-01-ddd-hexagonal-onion-clean-cqrs-how-i-put-it-all-together/\n- https://netflixtechblog.com/ready-for-changes-with-hexagonal-architecture-b315ec967749\n- https://betterprogramming.pub/the-clean-architecture-beginners-guide-e4b7058c1165\n- https://programmingpulse.vercel.app/blog/hexagonal-vs-clean-vs-onion-architectures\n\n\n---\n\n## Software Architecture Patterns\nArchitectural patterns are high-level reusable solutions for common software design cases like structure and behavior. Architectural styles are the principles and guidelines for system development activities like implementation, organization, technologies, architecture, communication, structure, infrastructure, etc. Both constitute for better system design and implementation that solves the current business problem.\n\n- N-layered architecture\n- Client-server\n- Master-slave\n- Peer-to-Peer (P2P)\n- MVC, MVP, MVVM, MVVM-C, VIPER\n- Domain-driven Design (DDD)\n- Test-driven Design (TDD)\n- Component-based Architecture (CBA)\n- Service-oriented Architecture (SOA)\n- Event-driven Architecture\n- Stream-based Architecture\n- Event-bus Architecture\n- Monolith\n- Microservices\n- Reactive\n- Serverless Architecture\n- Space-based Architecture (SBA)\n- Pipes and filters\n- Blackboard\n- Hybrid\n\n**Ref**:\n- https://www.freecodecamp.org/news/an-introduction-to-software-architecture-patterns\n- https://medium.com/@amitvsolutions/the-software-architect-deep-dive-into-17-essential-design-patterns-a0fb5a4726ab\n- https://medium.com/@mahmoudIbrahimAbbas/software-architecture-patterns-558486f4c3aa\n- https://romanglushach.medium.com/building-high-performing-software-systems-a-guide-to-architectural-styles-and-techniques-00dc89e08147\n\n\n---\n\n## Distributed systems\nDistributed systems are multiple systems running independently that aim to solve a common business problem.\n\n#### Characteristics and Advantages\n- Easy Scalability\n- Separation of functionality\n- Fault-tolerant by minimizing the failures\n- Cross-platform, infrastructure, polyglot\n- Different communication protocols\n\n### Principles and Metrics\n- Availability:\n  - 5 9's availability\n- Scalability\n  - Static and Dynamic\n- Latency\n  - Response Time\n- Throughput\n- Modularity\n- Decoupling\n- Consistency\n- Fault-tolerant\n- Partition tolerant\n- Caching\n- Extensibility\n- Maintainability\n- Testability\n- Reliability: \n  - MTBF (Mean Time Between Failures)\n  - MTTR (Mean Time To Repair)\n  - FMEA (Failure Modes and Effects Analysis)\n  - Fault Tolerance\n- Security: \n  - Authentication and Authorization\n  - Secure Communication\n\n### Challenges\n- **Network Management**\n  - Stateful vs Stateless\n  - TCP, UDP, QUICK\n  - Sync, Async, Stream communication\n  - DNS scalability\n- **Data Management**\n  - Database scalability\n  - Consistent data\n  - Data segregation and isolation\n  - Data compatibility\n  - Platform dependency (SQL, NoSQL)\n  - Partitions, Sharding, Replication\n- **Caching**\n  - Distributed Caching\n  - CDN Scalability and management\n  - Content service and distribution\n  - Replication and Invalidation\n  - Content consistency\n  - Read/write strategies\n- **Service Communication**\n  - REST, gRPC, Async Queues, GraphQL\n  - Pull/Push, Stream, Pub/Sub\n  - Latency\n  - Data Interoperability\n- **Event processing**\n  - Async/Sync processing\n  - Event decentralized processing\n  - Cross-service transactions commit/rollback\n- **Scalability**\n  - Static vs dynamic\n  - Load-balancing strategies\n  - High availability\n  - Fault-tolerant\n- **Testing**\n  - Service isolation (Unit) testing \n  - Integration testing\n  - Load testing\n  - Scalability and Elasticity testing\n  - A/B testing\n- **Managing builds and integration**\n  - Latest version adoption across services\n  - Backward compatibility \n  - Service upgrades with no downtime\n- **Deployment**\n  - A/B, Blue-green deployment\n  - Dynamic Scalability and Elasticity\n  - Cloud (Hybrid)\n  - Serverless\n  - Containerization and orchestration\n- **Monitoring**\n  - Distributed IDs\n  - Debugging\n  - Metrics\n  - Log Aggregation\n- **Security**\n  - Authentication and Authorization\n  - Cross-service Authentication\n  - Data encryption\n  - Throttle, DDoS, and Requests limit\n- **Maintainance**\n  - High Availability\n  - Fault-tolerant\n  - Reliability\n  - Scalability\n  - Error Handling\n  - Backups and Recovery\n  - Single point-of-failure\n  - Cascading System failures\n\n#### Trade-offs\n- Stateless vs Stateful\n- Server vs Serverless\n- Scalability vs Elasticity\n- API-gateway vs Load-balancer\n- Forward-proxy vs Reverse-proxy\n- Strong vs Eventual Consistency\n- Sync vs Async Communication\n- Latency vs Throughput\n- Read vs Write Through Cache\n- Read vs Write Heavy\n- SQL vs NoSQL\n- Replication vs Partition\n\n### Theorems\n- ACID\n- BASE\n- CAP\n- PACELC\n\n### Protocols and Algorithms\n- **Master election**: Protocol to select a leader/primary node to coordinate syncing, failure handling, monitoring, etc.\n- **Split Brain**: A situation where there will be multiple nodes acting as leaders or independent nodes leading to inconsistencies.\n- **Raft, Paxos**: Consensus algorithms to coordinate agreement among distributed nodes. \n- **Consistent Hashing**: Technique to distribute data with minimal effect on data re-organization when nodes are added or removed.\n- **Vector Clocks**: Resolve shared data conflicts with versioning among distributed nodes.\n- **Gossip**: Protocol to effectively spread the information across distributed nodes, especially liveness and availability.\n- **Quorum**: Minimum number of votes required to perform an operation.\n- **Sloppy Quorum**: Under temporary node failures, other available nodes participate in quorum with eventual consistency.\n- **Hinted Handoff**: Temporary storage of data by available nodes later to be synced by original nodes.\n- **Read Repair**: Fix data inconsistency during reads in a distributed storage. \n- **Anti-entropy Repair**: Background job to ensure data consistency across distributed nodes and storage.\n- **Merkle trees**: Fast data structure to check the file/data integrity using tree-level hashes.\n- **Write-ahead log**: Record the change before applying to replay them after node failures.\n- **Segmented log**: Segment memory logically for better log storage and rotation.  \n- **Log-rotation**: Store and remove temporal logs for effective log management.\n- **High-water Mark**: Mark the highest data entity syncing to ensure data integrity in partition failures.\n- **Leases \u0026 Fencing**: Automatic lock expiry based on time for resources across distributed systems.\n- **Heartbeat**: Periodic signal received among nodes to indicate availability.\n- **Phi Accrual Failure Detection**: Probabilistic failure detection algorithm to check node availability based on heartbeats.\n- **Bloom Filter**: Probabilistic data structure to check the existence of a key that definitely gives true for the non-existent key.\n- **Cuckoo Filter**: Probabilistic data structure same as bloom filter but with key deletion support and also better performance.\n- **Count-Min Sketch**: Probabilistic data structure to get the counts of an entity in a stream.\n- **Hashed and Hierarchical Timing Wheel**: Efficient data structure for scheduling events based on time.\n\n**Ref**:\n- https://levelup.gitconnected.com/system-design-master-template-how-to-answer-any-system-design-interview-question-ee5dc332acd5\n- https://medium.com/@abhirup.acharya009/chit-chat-tech-unveiling-the-gossip-protocol-magic-f5148e9d125d\n- https://sookocheff.com/post/time/vector-clocks/\n- https://medium.com/@adityashete009/sloppy-quorum-hinted-handoff-amazon-dynamodb-part-3-fe97bd058fcb\n- https://pawan-bhadauria.medium.com/distributed-systems-part-3-managing-anti-entropy-using-merkle-trees-443ea3fc6213\n- https://2minutestreaming.beehiiv.com/p/kafka-log-segment-files-explained\n- https://www.linkedin.com/pulse/high-watermark-distributed-design-patterns-pratik-pandey/\n- https://medium.com/codex/kafka-what-is-2-generals-problem-and-can-it-affect-my-kafka-producer-2a621d4e1fbd\n- https://medium.com/@arpitbhayani/phi-%CF%86-accrual-failure-detection-79c21ce53a7a\n- https://www.synnada.ai/blog/probabilistic-data-structures-in-streaming-count-min-sketch\n- https://www.confluent.io/blog/apache-kafka-purgatory-hierarchical-timing-wheels/\n\n\n---\n\n## Microservices\nMicroservices are full cohesive small autonomous components that provide independent development, scaling, testing, optimization, deployment, fault tolerance, etc. These small components are modularized means they provide single functionality within a bounded context and these are integrated through various communication methods. The big challenge in micro-services is not building the services themselves but communication between the services.\n\n\n### Styles\n- Bounded Context\n- Back-end For Front-end (BFF)\n- Service Registry and Discovery\n- Event-sourcing\n- Data Replication and Partitioning\n- Service-mesh\n- Micro-frontend\n- Shadow Deployment\n- Polyglot Persistence\n\n### Patterns\n- API Gateway\n- Saga\n  - 2 Phase Commit (2PC), 3 Phase Commit (3PC)\n  - Choreography \u0026 Orchestration\n- Command Query Responsibility Segregation (CQRS)\n- Aggregator, Chained, and Branch\n- Circuit Breaker (Threshold)\n- Bulkhead\n- Database-per-service\n- Anti-corruption layer\n- Configuration Externalization\n- Strangler\n- Microkernel\n- Broker\n- Sidecar\n- Smart endpoints, dumb pipes\n- Blackboard\n\n### Methods \u0026 Techniques\n- API Composition\n- Backpressure\n- Throttle \u0026 Debounce\n- Outbox\n- Retry\n- Timeout\n- Fallback\n- Fail-silent, Fail-fast, Fail-safe\n- Health Check\n- Object pool\n\n### Best practices\n- A micro-service should be a bounded context single-purpose service with a scope to share as little as possible and be less dependent on other services. Separate services with well-defined functionality and scope for less coupling and more cohesion. \n- Data should be split based on the functionality rather than the services themselves. 2 or more services can share the same DB when their collective functionality is the same.\n- Service contracts through interfaces make the services loosely coupled and can be tested independently.\n- If possible, data changes should be monitored with a change-data-capture or event-push model where the services that rely on this data get notified asynchronously and will have the latest data.\n- Avoid distributed monoliths, multiple monoliths, and fragile systems.\n- Build for failure as all system failures can't be anticipated and prevented and it's better to build the micro-services for handling the situations when any service fails. \n- Prevent cascading failures and whole system downtime when any service fails.\n- For inter-service communication, use asynchronous communication if possible to reduce the service waiting time or send the data through Message Queues if the data needs to be received by multiple components.\n- Use service registry for service communication instead of hard-coded service addresses. This allows the services to scale and is still possible for communication without any changes.\n- API gateways provide service coordination, security, and implementation of any customer service rule.\n- As each service should own its data to itself, the other services that require this data should get it through interfaces only and not directly as this will make the system more coupled. Implement gPRC for fast access or CQRS for read-only data.\n- Service connectivity should be monitored, and re-connect should be done when any other service/component goes up/down. Instead of the connectivity managed at the service level, something like service discovery can be helpful. \n- The service communication mediator component can check and maintain the active services either through periodic heartbeat notifications or retry for connection when any other service fails with exponential backoff. This way the services can make decisions based on the other services activities.\n- Higher latency and service blockage for communication are some of the most common problems in microservices. Utilize async communication and reduce the system bottlenecks like DB calls, heavy sync compute operations, etc, to reduce the latency.\n- Implement service logic with idempotency in the view as in microservice with async messages it may be possible that the same type of operations need to be performed multiple times. Due to some issues, the same message can be processed by multiple instances, and all these service handling of the request should produce the same result and maintain the same state.\n- Use rate limiter at the API gateway layer itself.\n- Handle transient (short time) errors like service unavailable due to load, system restarts, service down, etc, gracefully with circuit breaker or by adopting rules like fail-fast/safe.\n- Monitor service heartbeats periodically to check the liveness. If any service is not responding, block the requests to that service with a Circuit breaker or bring up a new service.\n- Centralized monitoring and logging are essential for checking system failures, debugging, etc. \n- Monitor the services and implement an alerting mechanism to prevent system overload and failures.\n- Chaos/stress testing will give the big picture about system resilience, and performance under load, and may produce several transient errors to look for. Periodically doing these tests will result in better development of the product.\n- A single change in one of the services should be contained to that service only. The ability to deploy services independently makes the system more cohesive and leads to quick development, build, and deployment.\n- Support backward compatibility and maintain versioning to serve different customers at different times and configurations.\n\n**Ref**:\n- https://learn.microsoft.com/en-us/azure/architecture/microservices/design/patterns\n- https://medium.com/microservices-in-practice/service-mesh-for-microservices-2953109a3c9a\n- https://medium.com/@joudwawad/microservices-pattern-distributed-transactions-saga-92b5e933cea1\n- https://diptendud.medium.com/sidecar-pattern-and-its-benefits-in-microservices-architecture-8a4c7ba78525\n- https://blog.bitsrc.io/understanding-throttling-and-debouncing-973131c1ba07\n- https://medium.com/inspiredbrilliance/patterns-for-microservices-e57a2d71ff9e\n- https://talktotheduck.dev/fail-fast-reliable-software-strategy-debug-failures-effectively\n- https://www.oreilly.com/content/microservices-antipatterns-and-pitfalls/\n- https://www.simform.com/blog/microservice-best-practices/\n- https://github.com/katopz/best-practices/blob/master/best-practices-for-building-a-microservice-architecture.md\n- https://medium.com/@kmdkhadeer/microservices-best-practices-5fe3e28394b3\n- https://blog.risingstack.com/designing-microservices-architecture-for-failure/\n\n\n---\n\n## Message Communication\n\n### Communication Protocols\n- HTTP, QUIC, DASH\n- gRPC\n- WebSocket\n- SSE (Server Sent Events)\n- WebRTC\n- WebTransport\n- MQTT (MQ Telemetry Transport)\n- AMQP (Advanced Message Queuing Protocol)\n- CoAP (Constrained Application Protocol)\n\n### Data Exchange Protocols\n- REST\n- SOAP\n- XML\n- XMPP\n\n### Messaging Patterns\n- Sync vs Async\n- Pub/Sub\n- Push vs Pull\n- Polling (Short, Long)\n- WebHook\n- WebSub\n- Message Queue\n- Message Stream\n\n### Message-oriented Middleware\n- Event Bus\n- Message Queue\n- Dead Letter Queue\n- Event Broker\n- Message Broker\n\n### Pub/sub Patterns\n- Fan-in\n- Fan-out\n- Multicast\n- Broadcast\n- Message Filtering\n  - Content-based routing\n  - Topic-based routing\n\n**Ref**:\n- https://getstream.io/blog/communication-protocols/\n- https://www.svix.com/resources/faq/message-broker-vs-message-queue/\n- https://medium.com/riskified-technology/message-broker-vs-event-broker-when-to-use-each-one-of-them-15597320a8ba\n\n\n---\n\n## Database Optimizations\n\n### Operations-Based Scaling\n- Read-heavy: Horizontal scaling with multiple read-replicas.\n- Write-heavy: Horizontal scaling with data partition/sharded replicas that can be vertically scaled.\n\n### High Availability\n- Replication: Reduces the latency and improves service availability with data replication across DBs.\n- Partition: Improves availability by partitioning the data across multiple regions, or based on use cases like celebrity data, frequent update data, etc.\n\n\n---\n\n## Data Replication\n\n**Advantages**\n- High Availability\n- Data Redundancy\n- Data Consistency \u0026 Integration\n- Improved Performance\n- Load balance \u0026 Scalability\n- Disaster Recovery\n\n**Challenges**\n- Data inconsistency\n- Latency\n- Conflict Resolution\n- Network bandwidth\n- Dynamic Scalability\n- Performance Monitoring\n\n### Syncing Types:\n- **Synchronous**: Primary DB waits and acknowledges write only if all other replicas are synced.\n  - Strong consistency\n  - Low write throughput\n  - High latency\n  - Use cases: Financial systems, environments where data integrity is critical\n- **Asynchronous**: Primary DB returns as soon as its own write is committed and makes async data change push to other replicas.\n  - Eventual consistency\n  - High write throughput\n  - Low latency\n  - Use cases: Gaming, Social media app features like post likes\n- **Semi-synchronous**: Write synchronously to a subset of replicas and return to the client. Asynchronously update the others.\n  - Not-so-strong consistency\n  - Write throughput trades-off with consistency\n  - Latency trades-off with consistency\n  - Use cases: Streaming applications, Gaming\n\n### Replication strategies:\n- **Single-leader multi replicas**: \n  - Only single leader and other replicas act as data backups. When the leader fails, one of the replicas is promoted as a leader. \n  - For huge write traffic, there will be latency as the single leader has to take all the loads. \n  - Also, the leader can fail before updating the replicas leading to data loss and the newly elected leader points to old data.\n  - Write throughput is reduced as a single node has to handle all writes.\n  - Durable data as all write operations are serialized in read replicas and there are no write conflicts.\n  - As a leader is the bottleneck, fault tolerance is difficult.\n- **Multi-leader multi replicas**: \n  - Multiple leaders distribute writes and update other leaders who in turn update their own set of replicas. This provides smooth failover as other leaders already have up-to-date data. \n  - If any leader fails before updating others then there will be data loss or data inconsistency. \n  - Conflict resolution is hard as multiple leaders have to merge the write operation incoming from multiple other leaders.\n  - Improved write throughput as write operations are distributed.\n- **Leaderless/peer-to-peer replicas**: \n  - No leader and all nodes are responsible for write/read operations and updating others. \n  - Same write operations are passed to a subset of nodes for handling failover. \n  - Only asynchronous syncing is possible as there is no hierarchy present and that leads to eventual consistency. \n  - Failover handling is easy as there is no leader and data consistent at every db. \n  - Increased read/write throughput and durable data.\n  - Conflict resolution is hard as each db is a leader and may have different snapshots of data. Fully scalable and fault tolerant.\n\n### Replication methods:\n- **Full-table**:\n  - Copies the whole table irrespective of changes and replicates everything providing full data sync but needs huge network bandwidth leading to slow replication.\n- **Snapshot**:\n  - Replicate data in snapshots at pre-defined or specific intervals.\n  - All data is consistent and simple to implement.\n  - Will have old data and not suitable for real-time reads.\n- **Incremental**:\n  - Replicate the data that has only changed since the last cycle with reduced network bandwidth and latency. Data may not be the latest compared to the source.\n- **Key-based**:\n  - Incremently replicate data based on the changes that happened to unique keys that may be any columns, or data change type (like timestamps).\n  - Replication takes less bandwidth and is efficient compared to the above methods but requires the definition of keys.\n  - Not effective when the replication keys don't uniquely produce change.\n- **Transaction**:\n  Data replication happens whenever a transaction occurs, and the replicas are updated in the same order as the transactions.\n  - Real-time updates and data are synchronized in the same order which is suitable for real-time analytics.\n  - Requires huge network traffic for high writes.\n  - Also it is complex to coordinate the replication process as the co-ordinator has to track the state and replication status.\n- **Log-based**:\n  - Replicate data by processing the transactional logs committed to the DB.\n  - Real-time data consistency with minimal delay and requires less network bandwidth and resources.\n  - Entirely depends on the source database type (MySQL, PgSQL) for generating logs and format which is not suitable for polyglot databases (SQL to NoSQL syncing) or requires special processing.\n  - Compared to *Transactional* syncing, the syncing steps and complexity are less with low latency.\n- **Multi Merge**:\n  - Merge data updates from multiple sources to each replica by effectively resolving merge conflicts.\n  - Multi-leader environments need this type of replication as multiple nodes handle parallel updates.\n  - Good for high scalable replication as the load is distributed and each node has the latest data that ensures resilience.\n  - Requires complex conflict resolution setup and logic. Also demands efficient communication for passing updates to each other node.\n- **Application trigger**:\n  - Data replication logic is transformed to the application where the system tracks the changes and triggers the syncing based on certain rules.\n\n### Replication techniques:\n- **Storage Array-based**: Track and replicate the changes to data blocks or volumes. Ex: Full-table, Snapshot, log-based replication.\n- **Host-based**: Replicate the changes that happened at the file level or application level where the software agent captures the changes. Ex: Transaction, key-based, log-based replication.\n- **Hypervisor-based**: Hypervisor manages, monitors, and replicates the VMs where data changes may be array-based or host-based. Ex: Snapshot, Application-trigger replication. \n- **Network-based**: Capture the network layer updates and replicate those at each target source. Ex: Mult-merge replication.\n\n### Replication steps:\n- Identify primary data source and destination replication sources (SQL, NoSQL, Warehouse, Data Lakes).\n- Set up the distributed replication system by choosing a single-leader or multi-leader replication system that aligns with business requirements.\n- Set data replication scope like full data replication every time or incremental.\n- Configure data frequency like real-time or eventual consistency and data syncing type like synchronous or asynchronous.\n- Choose replication methods like log-based, transactional, or custom-triggered.\n- Adopt replication tools and libraries for syncing, conflict resolution, monitoring, and salinity. \n\n**Ref**:\n- https://medium.com/@anil.goyal0057/strategies-for-scaling-databases-a-comprehensive-guide-b69cda7df1d3\n- https://estuary.dev/data-replication-strategies/\n- https://rivery.io/data-learning-center/data-replication/\n- https://stonefly.com/blog/array-vs-host-vs-hypervisor-vs-network-based-replication/\n\n\n---\n\n## Data Partitioning\n\n**Advantages**\n- High Availability\n- Use case-based scaling\n- Reduced load and latency\n- Write/read efficiency\n- Increased concurrency\n- Improved query processing time\n- Parallel processing operations\n\n**Challenges**\n- Coordination complexity\n- Node failure and re-hashing\n- Schema changes overhead\n- Joins across partitions\n- Data reconciliation\n- Dynamic partition\n- Uneven data distribution\n\n### Partitioning Types:\n- **Vertical Partitioning**: Partition one or more columns in separate nodes based on high reads/writes that improve high throughput and reduced latency. Store frequently updated/read columns in a high-performing system and less required information in slow or low system resources.\n- **Horizontal Partitioning (Sharding)**: Split the table rows into small ranges called shards and scale the database for increased concurrency. This is very advantageous in cases like celebrity data problems. Also helps in localizing the data geographically.\n- **Functionality Partitioning (Federation)**: Functionality-based partition like storing all users, billing, inventory, etc, information in separate databases. This makes load distribution based on functionality and db type resulting in fewer reads/writes for a particular db. For specific cases where data accumulation is required, the operations are costly and complex making.\n \n### Partitioning Criteria:\n- **Range-based Partition**: Divide the data based on the range values like event dates. Challenges arise when the data accumulation is more for specific ranges of values like festivals, holidays, etc. and if these are not considered while partitioning then the distribution is not event. \n- **Hash-based Partition**: Based on one or more unique identifiers like user-ids, take the hash that maps to the partition node where this data will be stored. A poorly selected hash can lead to improper data distribution that leads to scaling challenges. Re-hashing is complex when a node fails or data migration the data across nodes.\n- **List-based Partition**: Data is split based on the list of values that a particular data entity falls like the country name. If a particular list value maps to a huge chunk of data than others then this requires a further partition of data to ease the load. Data integration becomes non-manageable across partitions due to isolated data.\n- **Composite Partition**: A composition of the above partition criteria like list-hash, hash-range, and etc. This is too complex to coordinate and requires special rules for every type of partition. Improves the availability, scalability, and load balancing when carefully implemented.\n\n### Effective Partitioning:\n- **Partition Key**: A Partition key is a partition selection key that can be based on criteria, use-case, query operations, etc., that divides the data and distributes it across partitioning nodes. The selection of a good partition key is important for proper data distribution, load balancing, and effective query processing.\n- **Data Re-balancing**: Re-balancing distributes the data across partitions properly when any partition has accumulated huge data. Dynamic re-balancing should be incorporated to reduce the load on nodes. Data re-balancing also improves query processing by performing operations only on a subset of data every time.\n- **Data distribution**: Data distribution can be based on different scenarios like frequency, location, read/write operations, etc. A correctly distributed data partition improves concurrency, proper scaling, and low latency. \n\n**Ref**:\n- https://thinhdanggroup.github.io/database-partition/#choosing-the-right-partitioning-technique\n- https://learn.microsoft.com/en-us/azure/architecture/best-practices/data-partitioning\n- https://airbyte.com/data-engineering-resources/what-is-data-partitioning\n\n\n---\n\n## Leader in Distributed System and Leader Election\n- In a distributed system, the role of a leader is determined based on the coordination it does like\n  - All nodes are the same and any node can become a leader to handle particular coordination like writes-only db, A/B testing, etc.\n  - One node processes the complex task and distributes the sub-jobs to other needs and later aggregates the results like distributed processing of huge data.\n- All the follower nodes keep track of the leader's liveness and kicks-off the election process when they find the leader has not responded for a fixed amount of time\n- Leader election can happen in any of the following situation\n  - The Distributed System is starting up and needs to elect a single node as the leader\n  - When the elected leader is failed and detected by follower nodes, the election process triggers\n  - As part of scaling up/down the services, a new leader is needed based on selective configuration\n- Leader election algorithm should satisfy both the following conditions\n  - Only the leader should be elected after the election process\n  - All worthy candidates should participate in the process\n\n### Leader election algorithms\n- Bully\n- LCR\n- Floodmax\n- Ring\n- Next-in line Failover\n- Consensus algorithms\n  - Paxos\n  - RAFT\n  - Zab (ZooKeeper Atomic Broadcast)\n\n**Ref**:\n- https://aws.amazon.com/builders-library/leader-election-in-distributed-systems/\n- https://igotanoffer.com/blogs/tech/leader-election-system-design-interview\n- https://medium.com/@mani.saksham12/raft-and-paxos-consensus-algorithms-for-distributed-systems-138cd7c2d35a\n\n\n---\n\n## System Monitoring and Optimizations\n- Monitor HTTP requests that are going and coming to the server.\n- Monitor the CPU load and Memory when in peak load and make decisions to scale or not.\n- Check whether the average request latency is in the expected threshold or not.\n- Periodically check heartbeats or health checks of a system to detect liveliness.\n- If the latency is higher, it could be for any of the following reasons\n  - CPU is loaded high and can't take new requests\n  - More threads are created and the system can't handle them with increased memory\n  - OS spends huge time in thrashing of storage\n  - System frequently switches between threads and wastes resources\n- Increase the concurrent requests handling capacity by\n  - Setting more number of file descriptor count for more requests handling at a time\n  - Increase the CPU cores for handling parallel requests with threads\n  - Choosing faster reads/writes hardware for disk storage\n\n\n---\n\n## Deployment\n\n### Deployment types\n- **Bigbang**: Deploy software with all business requirements developed at once.\n- **Continous/incremental/phased**: Deploy changes as a version with each version including new features, improvements, or fixes.\n\n### Deployment strategies\n- **Recreate deployment**:\n  - Bring down the old version and spin up the new one.\n  - Pros: Easy, only one version running at a time\n  - Cons: Takes time to upgrade and rollback, so much downtime window\n- **Ramped/Rolling update**:\n  - As the new version is spinning up, keep the old one running without downtime. Load balancers often do this like Kubernetes pods.\n  - Pros: Zero/minimal downtime\n  - Cons: Rollback has to be handled carefully\n- **Blue-green deployment**:\n  - At a time two instances will run blue and green where only one of them handles the load and the other one is idle. Switch deploy between them.\n  - Pros: No downtime, easy rollback\n  - Cons: One instance is always idle\n- **canary deployment**:\n  - A new version will brought up but only a fraction of the load (like 5%-15%) at starting will be handled. If no problems arise, switch the whole load to the new one slowly and the current running service is brought down.\n  - Pros: Servers as test setup, easy rollback\n  - Cons: Load switching complexity, downtimes when there are any issues\n- **A/B Testing deployment**:\n  - Deploy a new version with the new version and divert the traffic with a selected subset of users to this new version to test the system with new features, performance, and under load.\n  - Pros: Can easily test new versions of system\n  - Cons: Complex identification of sub-set of users\n- **Shadow deployment**:\n  - Deploy the new version and mirror the traffic but don't serve customers. Test the new version and later switch it for user load serving.\n  - Pros: Testable setup under load\n  - Cons: Complex deployment and handling, duplicate data storage/creation\n\n**Ref**:\n- https://earthly.dev/blog/deployment-strategies/\n\n\n---\n\n## Scaling\n- Vertical (Scale-up/down)\n- Horizontal (Scale-out/in)\n- Diagonal\n\n**Ref**\n- https://sookocheff.com/post/architecture/scaling-with-workload-separation/\n\n\n---\n\nWill be added later:\n- Event-driven Architecture\n- 12 factor app\n- Api design, patterns, life-cycle, management\n- Caching strategies, invalidation\n- Concurrency patterns\n- Distributed tracing\n- Refactoring"},{"metadata":{"title":"Doppalf: RAG powered full-stack AI chatbot like ChatGPT","description":"Build a full-stack RAG-powered AI chatbot like ChatGPT to give LLM your personality with Python, FastAPI, Llamaindex, Cohere, Qdrant, Next.js (Typescript), and Tailwind CSS.","imgName":"doppalf-rag-powered-ai-chatbot/doppalf-rag-powered-ai-chatbot.jpg","date":"May 24, 2024","tags":["python","ai","next-js","react"],"keywords":["python","llm","next-js","llamaindex","tailwind-css","typescript","fastapi","rag","cohere","qdrant","cohere-ai","ai-chatbot","llm","chatbot","chatgpt"],"id":"doppalf-rag-powered-ai-chatbot"},"content":"\n![Doppalf: RAG powered fullstack AI chatbot like ChatGPT](doppalf-rag-powered-ai-chatbot/doppalf-rag-powered-ai-chatbot.jpg)\n\n###### Published on: **May 24, 2024**\n\n# Doppalf: RAG-powered fullstack AI chatbot like ChatGPT\n\nI have built an end-to-end full-stack AI chatbot web application like ChatGPT. It's powered with RAG (Retrieval Augmentation Generation) to give LLM my personality. Meaning, that LLM will behave like me and assume the character of **Lakshmi Narayana**. I have built this application with LLamaindex, Cohere AI, and the Qdrant database. The full tech stack is:\n- **Docker**\n- **Nginx**\n- **UI**:\n  - Next.js (v14)\n  - Typescript\n  - Tailwind CSS\n- **Backend**:\n  - Python (v3.12)\n  - FastAPI\n  - Llamaindex\n  - Cohere AI\n  - Qdrant Cloud\n\n![Doppalf AI:=:100:=:Doppalf AI streaming response](doppalf-rag-powered-ai-chatbot/doppalf-response.gif)\n\n## Architecture\n\n![Doppalf Architecture:=:100:=:Doppalf High level Architecture](doppalf-rag-powered-ai-chatbot/doppalf-arch.png)\n\n\u003e The whole project code can be found on GitHub for [Doppalf](https://github.com/santhalakshminarayana/doppalf).\n\nFor Doppalf, Docker has been used for container orchestration and Nginx as a reverse proxy. This application has a total three services (repos):\n- doppalf-rp (Nginx)\n- doppalf-ui (Next.js)\n- doppalf-ai (Python)\n\nEach of the above repos has an individual **Dockerfile** with optimized configuration. The following are the Dockerfiles for individual services:\n\n### doppalf-ui (Next.js)\n\n```yaml:Dockerfile\nFROM node:21-bullseye-slim AS deps\nWORKDIR /app\nCOPY package*.json ./\nEXPOSE 3001\nENV PORT 3001\nENV HOSTNAME \"0.0.0.0\"\nRUN npm ci\n\n# Development\nFrom deps as dev\nENV NODE_ENV=development\nCOPY . .\nCMD [\"npm\", \"run\", \"dev\"]\n\nFROM node:21-bullseye-slim AS builder\nWORKDIR /app\nCOPY --from=deps /app/node_modules ./node_modules\nCOPY . .\nRUN npm run build\nRUN npm prune --production\n\n# Production\nFROM node:21-bullseye-slim AS prod\nWORKDIR /app\nENV NODE_ENV production\n# Add nextjs user\nRUN addgroup --system --gid 1001 nodejs\nRUN adduser --system --uid 1001 nextjs\n# Set the permission for prerender cache\nRUN mkdir .next\nRUN chown nextjs:nodejs .next\nUSER nextjs\n# Automatically leverage output traces to reduce image size\nCOPY --from=builder --chown=nextjs:nodejs /app/.next/static ./.next/static\nCOPY --from=builder --chown=nextjs:nodejs /app/public ./public\n\nEXPOSE 3001\nENV PORT 3001\nENV HOSTNAME \"0.0.0.0\"\n\nCMD [\"npm\", \"start\"]\n```\n\nThe above **Dockerfile** has a multi-stage build config for both *dev* and *prod* environments. The exact build stage to be used will be determined by configuration from *docker-compose.yaml*. By default, the *dev* stage is used. The above configuration for *prod* is very optimal and reduces the final running docker image size due to Next.js optimizations.\n\n### doppalf-ai (Python)\n\n```yaml:Dockerfile\n# Why bookworm? https://pythonspeed.com/articles/base-image-python-docker-images/\nFROM python:3.12-slim-bookworm as base\nWORKDIR /app\nRUN apt-get update \u0026\u0026 \\\n    apt-get install -y --no-install-recommends gcc\nCOPY requirements.txt .\nRUN pip wheel --no-cache-dir --no-deps --wheel-dir /app/wheels -r requirements.txt\n\nFROM python:3.12-slim-bookworm as build\nENV PYTHONDONTWRITEBYTECODE=1\nENV PYTHONUNBUFFERED=1\nWORKDIR /app\nCOPY --from=base /app/wheels /app/wheels\nCOPY --from=base /app/requirements.txt .\nRUN pip install --no-cache /app/wheels/*\nCOPY . /app\nEXPOSE 4001\nCMD [\"python\", \"main.py\"]\n```\n\nThe above **Dockerfile** pulls Python 3.12 *slim-bookworm* docker image as opposed to the common *alpine* image. You can refer to the linked article for why. Here instead of normally installing requirements through *pip*, we install dependencies through *wheels* which optimizes the docker image build speed.\n\nThe Next.js service will be running on PORT *3001* and the Python service will run on *4001*.\n\n### doppalf-rp (Nginx)\n\nAnd finally, Nginx configuration for forwarding the traffic to individual services are done as \n\n```:includes/proxy.conf\nproxy_set_header Host $host;\nproxy_set_header X-Real-IP $remote_addr;\nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\nproxy_set_header X-Forwarded-Proto $scheme;\nproxy_set_header X-Forwarded-Host $server_name;\nproxy_buffering off;\nproxy_request_buffering off;\nproxy_http_version 1.1;\nproxy_intercept_errors on;\n```\n\n```yaml:Dockerfile\nFrom nginx:stable-alpine\nRUN mkdir -p /run/nginx\nWORKDIR /run/nginx\nCOPY ./nginx.conf /etc/nginx/conf.d/default.conf\nCOPY ./includes /etc/nginx/includes/\nEXPOSE 80\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\n``` \n\nNginx configruation for forwarding rules for different services\n\n```:nginx.conf\nserver {\n    listen 80 default_server;\n    listen [::]:80 default_server;\n    server_name _;\n\n    index index.html;\n\n    location / {\n        proxy_pass http://doppalf-ui-service:3001;\n    }\n\n    location /doppalf-ai/v1 {\n        proxy_pass http://doppalf-ai-service:4001;\n\n        proxy_redirect off;\n        # SSE connection config\n        proxy_set_header Connection '';\n        proxy_cache off;\n    }\n}\n```\n\nSo, the root */* request (127.0.0.1) will be forwarded to the Next.js UI service for the UI page, and any request with the prefix */doppalf-ai/v1* will be forwarded to the Python AI service.\n\n## Doppalf UI\n \nThe UI has been built with Next.js (Typescript) and Tailwind CSS. It's a single-page application that mainly contains an input box for providing Query and the response will be generated like ChatGPT where the UI will show the user and system messages. The AI-generated answer will be streamed like ChatGPT and rendered as Markdown. This has been done by Streaming the text from the Backend as **SSE (Server Sent Events)** and reading those messages in Next.js using using Microsoft's [Fetch Event Source package](https://www.npmjs.com/package/@microsoft/fetch-event-source) because normal browser SSE doesn't support *POST* request.\n\n![Doppalf UI Landing Page:=:100:=:Showing User and System Message](doppalf-rag-powered-ai-chatbot/doppalf-ui-query-response.png)\n\n#### UI Features include:\n- Dark mode\n- Streaming responses like ChatGPT\n- Auto-scroll to the bottom while generating the answer\n- New chat session\n\n\n## Doppalf AI\n\nThe main part of this application is building the RAG pipeline and giving LLM a personal character that answers like me. For this, I have used Llamaindex for building the RAG pipeline, Cohere AI as LLM, and Qdrant Cloud for storing vector embeddings.\n\nThis costs me nothing as they both offer free APIs, you can get free [Cohere API trail Key](https://dashboard.cohere.com/api-keys) and 1 GB cluster for storing vectors in [Qdrant Cloud API Key and URL](https://cloud.qdrant.io).\n\nThe web framework for providing APIs used was FastAPI and its support for streaming the response like SSE without using any extra configuration was a huge thumbs up for this kind of AI Chatbot application.\n\nFor building the RAG pipeline, I have followed the following pipeline process:\n- Load Documents\n- Parse text into Sentences (as nodes) with Window size as 1\n- Get vector embeddings for each node (sentences) with Cohere Embeddings\n- Index the nodes and store the vector embeddings in the Qdrant cloud\n- Persist the index for re-use further runtimes\n- Build a Chat engine from the index with a retrieval strategy as \"Small-to-Big\" and with some buffered chat memory history\n- Provide the retrieved context and use Cohere Rerank for reranking the retrieved nodes\n- Synthesis the response using Cohere\n\n![RAG Pipeline:=:100:=:RAG pipeline](doppalf-rag-powered-ai-chatbot/rag-pipeline.jpg)\n\nThe complete code for the RAG pipeline using Llamaindex is \n\n```python:rag.py\nfrom llama_index.core import load_index_from_storage\nfrom llama_index.core.memory import ChatMemoryBuffer\nfrom llama_index.core.node_parser import SentenceWindowNodeParser\nfrom llama_index.core.postprocessor import MetadataReplacementPostProcessor\nfrom llama_index.embeddings.cohere import CohereEmbedding\nfrom llama_index.llms.cohere import Cohere\nfrom llama_index.postprocessor.cohere_rerank import CohereRerank\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\nfrom qdrant_client import QdrantClient\n\nfrom src.config.env import ENV, env_keys\nfrom src.config.logger import get_logger\n\nfrom .constants import CHAT_PROMPT\n\nenvk = ENV()\nlogger = get_logger()\n\nindex = None\nchat_engine = None\n\ndef load_rag() -\u003e None:\n    global index\n    global chat_engine\n\n    cdir = os.getcwd()\n    docs_dir = envk.get(env_keys.get(\"DOCS_DIR\"))\n    docs_path = os.path.join(cdir, docs_dir)\n\n    # check if any documents are provided for index\n    if not os.path.exists(docs_path):\n        raise FileNotFoundError(f\"Documents dir at path: {docs_path} not exists.\")\n    if not os.listdir(docs_dir):\n        raise FileNotFoundError(f\"Provide documents inside directory: {docs_path} for indexing.\")\n    \n    storage_dir = envk.get(env_keys.get(\"INDEX_STORAGE_DIR\"))\n    storage_path = os.path.join(cdir, storage_dir)\n    \n    cohere_api_key = envk.get(env_keys.get(\"COHERE_API_KEY\"))\n    qdrant_api_key = envk.get(env_keys.get(\"QDRANT_API_KEY\"))\n\n    Settings.llm = Cohere(\n        api_key=cohere_api_key,\n        model=\"command-r-plus\", \n    )\n    Settings.embed_model = CohereEmbedding(\n        cohere_api_key=cohere_api_key,\n        model_name=\"embed-english-v3.0\",\n        input_type=\"search_document\",\n    )\n    \n    qd_client = QdrantClient(\n        envk.get(env_keys.get(\"QDRANT_CLOUD_URL\")),\n        api_key=qdrant_api_key,\n    )\n\n    sentence_node_parser = SentenceWindowNodeParser.from_defaults(\n        window_size=1,\n        window_metadata_key=\"window\",\n        original_text_metadata_key=\"original_text\", \n    )\n\n    vector_store = QdrantVectorStore(\n        client=qd_client, \n        collection_name=envk.get(env_keys.get(\"COLLECTION_NAME\")),\n    )\n\n    # index was previously persisted\n    if os.path.exists(storage_path) and os.listdir(storage_path):\n        logger.debug(\"Using existing index.\")\n        storage_context = StorageContext.from_defaults(\n            vector_store=vector_store, persist_dir=storage_path\n        )\n        \n        index = load_index_from_storage(storage_context)\n\n    else:\n        logger.debug(\"Creating new index for documents.\")\n        reader = SimpleDirectoryReader(input_dir=docs_path)\n        \n        all_docs = []\n        for docs in reader.iter_data():\n            all_docs.extend(docs)\n        \n        for doc in all_docs:\n            logger.debug(f\"id: {doc.doc_id}\\nmetada: {doc.metadata}\")\n\n        nodes = sentence_node_parser.get_nodes_from_documents(all_docs)\n        \n        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n        \n        index = VectorStoreIndex(nodes, storage_context=storage_context)\n\n        index.storage_context.persist(persist_dir=storage_path)\n\n\n    chat_engine = index.as_chat_engine(\n        chat_mode=\"condense_plus_context\",\n        memory=ChatMemoryBuffer.from_defaults(token_limit=int(envk.get(env_keys.get(\"MAX_BUFFER_MEMORY_TOKENS\")))),\n        context_prompt=CHAT_PROMPT,\n        similarity_top_k=3, \n        node_postprocessors=[\n            MetadataReplacementPostProcessor(target_metadata_key=\"window\"),\n            CohereRerank(api_key=cohere_api_key, top_n=3),\n        ],\n        verbose=False,\n    )\n\n\ndef chat(query: str):\n    global chat_engine\n    \n    response = chat_engine.stream_chat(query)\n    for res in response.response_gen:\n        yield f\"data: {json.dumps({\"message\":res})}\\n\\n\"\n```\n\nThe above *load_rag()* function will first check if any previously stored index exists for re-using else it will build one. If no index is stored in the given *storage_path* directory, it will build the index by loading the documents from the *docs_dir* directory. *SentenceWindowNodeParser* is used for parsing the sentences into nodes with a window size of 1. This window size makes the post-retrieval context have surrounding sentences also part of it for better answer synthesis. \n\nFor created nodes, get the vector embeddings from the Cohere embeddings and store them in the Qdrant Cloud. And finally store the index for skipping this whole process again when the application restarts.\n\nFrom the index, build the chat engine with some memory (4096 tokens here) for memorizing past conversations with the user and provide Cohere Re-ranker as post node retrieval processer for reranking the nodes based on the query relevancy. This chat engine retrieves the surrounding context for each retrieved node for more information and finally sends the whole retrieved-context and the query to the LLM for answer generation as a prompt. I have used a custom prompt for making LLM answer as me.\n\n```python:/src/generate/constants.py\nCHAT_PROMPT=(\n    \"You are impersonating the human 'Lakshmi Narayana' and so your name.\"\n    \"So you are Lakshmi Narayana and answers in first person.When asked any question about you, you will answer as if Lakshmi Narayana is answering.\"\n    \"You will answer politely and take the help of the following context for more relevant answers.\"\n    \"If you don't have any sufficient information from the context, use your knowledge to answer.\"\n    \"Or don't hallucinate if you are sure you cannot answer.\"\n    \"Here are the relevant documents for the context:\\n{context_str}\\n\"\n    \"Instruction: Use the previous chat history, or the context above, to interact and help the user and answer as if you are Lakshmi Narayana.\"\n    \"Don't add any additional data if the answer can be derived from context.\"\n    \"Generate the response in markdown format.\"\n)\n```\n\nLLamaindex uses this prompt for context ingestion and sends this to LLM for answer generation.\n\nFinally, the chat generation API is exposed for streaming the response using FastAPI as follows\n\n```python:api.py\nfrom fastapi import APIRouter, HTTPException\nfrom pydantic import BaseModel\nfrom starlette.responses import StreamingResponse\n\nfrom .rag import chat\n\n\nclass GenerateModel(BaseModel):\n    message: str\n    message_id: str\n    role: str\n    timestamp: str\n\n\ngrouter = APIRouter(tags=[\"generate\"])\n\n\n@grouter.post(\"\")\nasync def generate(data: GenerateModel):\n    try:\n        return StreamingResponse(\n            chat(data.message), \n            media_type='text/event-stream',\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=e)\n```\n\nAnd final interactivity with LLM that answers about me like I am talking as \n\n![LLM answering as me:=:100:=:LLM answering like me with history](doppalf-rag-powered-ai-chatbot/query-response-history.png)\n\n**New Chat session**\n\n![New Chat Session:=:100:=:New Chat Session](doppalf-rag-powered-ai-chatbot/new-chat.gif)\n\n---\n\nI will some more features in the future like:\n- Adding or removing the documents dynamically from the UI\n- Voice cloning for speaking out the answer as a character\n- Enhance the LLM answering with more RAG strategies\n\nPlease check out the complete project code in my Github repository for [Doppalf](https://github.com/santhalakshminarayana/doppalf)."},{"metadata":{"title":"Go Gotchas, Tips, and Good Practices for Efficient Go","description":"Review some of the common mistakes we make while writing Go code and learn how to avoid them along with good practices for efficient Go.","imgName":"go-gotchas-and-good-practices/go-gotchas-and-good-practices.jpg","date":"Mar 25, 2024","tags":["go"],"keywords":["go","golang","go-gotchas","go-good-practices","go-best-practices","go-tips","efficient-go"],"id":"go-gotchas-and-good-practices"},"content":"\n![Go Gotchas and Good Practices](go-gotchas-and-good-practices/go-gotchas-and-good-practices.jpg)\n\n###### Published on: **Mar 25, 2024**\n\n# Go Gotchas, Tips, and Good Practices for Efficient Go\n\nEven though Go is a very simple language and we can avoid most of the issues/errors at compilation time because of static typing, lint checks, data race checks, and other analysis checks, we still tend to make some common mistakes.\n\nIn this article, we will review some of the common mistakes/gotchas the new Go developers make and some not-common mistakes even the experienced Go developers cannot avoid if they don't pay attention. And we will also see some good practices while writing Go code to make it more efficient.\n\n### Declaration inside a block in a local-scope\n\nVariables declared inside a block like _if-else/for_ are local scope variables. If you have declared any variable outside of the block and re-declared the variable with the same name and changed it inside the block, after you try to access it thinking the values as changed but it's not the case in Go (maybe in Python). But in Go, it's a bug. This is one of the most common mistakes that many new Go developers make\n\n```go\nfunc main() {\n\tarr := []int{4, -2, 23, 10}\n\tvar v int\n\n\tfor _, v := range arr {\n\t\tif v \u003e= 10 {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tfmt.Println(\"found value \u003e=10:\", v) // prints v as '0'\n}\n```\n\nNow, remove the declaration in the for-loop for _i_ and print\n\n```go\nfunc main() {\n\tarr := []int{4, -2, 23, 10}\n\tvar v int\n\n\tfor _, v = range arr {\n\t\tif v \u003e= 10 {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tfmt.Println(\"found value \u003e=10:\", v) // prints v as '23'\n}\n```\n\n### Pointers in place of values\n\nPointers are everywhere in Go. We can pass variables as values and also as pointers (reference only) to the functions. If the function is passed a pointer, there is a chance that it can modify the value of the pointer leading to an unprecedented situation. The function may pass the value to any other function where it can also change value and a chain of value changes make the program state undesirable. Even if you want to have this behavior like the value of the variable to be changed in the called function, it violates the principle of **Function invariance**. Instead, use struct and change members of the struct when needed.\n\n### General data structures are not good for concurrent access\n\nNormal data structures like int, slices, and maps are not safe for concurrent access by multiple goroutines. We may have to put locks or such mechanisms for safe access, without those proper synchronization mechanisms, we might face into data race situation.\n\nThere exist atomic level operations to perform on primitive data types like int with _sync/atomic_ package, and also _sync.Map_ for concurrent access of Map elements.\n\n### Splitting an empty string will give a slice of length 1\n\nIf the string is empty, and we split it, we expect the length of the resultant array to be **0**. But, in Go, we will get the length as **1** and the first element is the empty string. This is strange in Go so keep in mind while splitting and it's always good to check the string length before splitting.\n\n```go\nfunc StringSplit(s string) []string {\n\treturn strings.Split(s, \",\")\n}\n\nfunc CheckedEmptyStringSplit(s string) []string {\n\tif len(s) == 0 {\n\t\treturn []string{}\n\t}\n\n\treturn StringSplit(s)\n}\n\nfunc main() {\n\ts := \"\"\n\tfmt.Println(len(StringSplit(s))) // prints 1\n\tfmt.Println(len(CheckedEmptyStringSplit(s))) // prints 0\n}\n```\n\n### Wrapping errors while checking with errors.Is\n\nSomewhere down the line of the function stack, we got an error and how can we check the error type compared to exceptions in other programming languages? To check if the returned is of any type, we can wrap the error with custom error type and message and the calling function can check it with _errors.Is_. This is very useful to trace back the error and people usually send string literals without wrapping those as errors.\n\nAlso, treating errors as strings and comparing strings should be avoided instead use _errors.Is_.\n\n### Uninitializing the Map and Structs\n\nThis is a very serious problem in Go. Sometimes we forget to initialize the map and that leads to runtime errors and the program to panic leading it to crash. The zero value of the map or struct is nil\nIn Go, if you declare a map without initializing it, its zero value will be nil, and any further operations on this map raise run time panic because the _nil_ map has not allocated any memory.\n\nThe same can be applied when we use a pointer struct if not initialized.\n\n```go\ntype intPC interface {\n\tPrintContents()\n}\n\ntype st struct {}\n\nfunc (s *st) PrintContents() {}\n\nfunc F(i intPC) {\n\ti.PrintContents() // panics here\n}\n\nfunc main() {\n\tvar s *st\n\tF(s)\n}\n```\n\nThe above program compiles but panics as we have passed the struct pointer that is not pointing to any memory and its value is _nil_.\n\n### Nil Interfaces or Pointers\n\nThis is the same as the above problem that will lead to panic and program crashes. An interface value is _nil_ if its value and dynamic type are _nil_\n\n```go\ntype intPC interface {\n\tPrintContents()\n}\n\nfunc F(i intPC) {\n\tfmt.Printf(\"type %T, value %v\\n\", i, i) // prints \"type \u003cnil\u003e, value \u003cnil\u003e\"\n\ti.PrintContents() // panics here\n}\n\nfunc main() {\n\tvar i intPC\n\tF(i)\n}\n```\n\nThis is a classic example of nil-interface in Go, here the interface variable is not initialized with any concrete type (struct or int) and also not initialized with any memory. So, it's a nil-interface and we can't perform any operations on it. Avoid these types of nil types using proper checks.\n\n### JSON un-marshalling for integer to an interface will be float64\n\nWhile un-marshaling the JSON string into an _interface{}_, the numeric values like int are parsed as float64.\n\n```go\nfunc main() {\n\tjsS := `{\"a\": 10}`\n\n\tm := map[string]interface{}{}\n\n\tjson.Unmarshal([]byte(jsS), \u0026m)\n\n\tfmt.Printf(\"%T\", m[\"a\"]) // float64\n}\n```\n\nUse _json.UseNumber()_ with a custom decoder for parsing numbers in JSON as numbers in Go.\n\n### Un-used function parameters\n\nAs strict as the Go compiler for unused declared variables, it will ignore any function arguments that are not used. This may not be a problem but what if you are passing a huge chunk of data between functions that are not used anywhere creating the memory overhead? If you want to satisfy the interface for that method and there is no use for that argument, simply ignore it with blank identifier _\\__.\n\n```go\nfunc F(param1 int, _ string) {\n\t...\n}\n```\n\n### Using pointers in channels\n\nPassing pointer variables in the channel may create data race conditions as both the sender and receiver hold the reference and any one of them can modify.\n\nAlso passing pointers reduce memory overhead by eliminating copying and moving. So, we have to be careful when passing pointers into the channels. The Receiver should not alter the data that it receives as it is only allowed to consume the data. So, avoid data modification at the receiver side.\n\n### Receiving in closed channel results in zero value of the channel type\n\nIf you try to receive from a closed channel, you will get the zero value of the channel type creating confusion that the sender is still sending.\n\n```go\nfunc main() {\n\tch := make(chan int)\n\tclose(ch)\n\n\tfmt.Println(\u003c-ch) // 0\n}\n```\n\nTo avoid this, either check if the channel is closed by checking if the data received is sent by the channel or not as\n\n```go\nfunc main() {\n\tch := make(chan int)\n\tclose(ch)\n\n\td, ok := \u003c-ch\n\tif !ok {\n\t\tfmt.Println(\"channel closed\")\n\t} else {\n\t\tfmt.Println(d)\n\t}\n}\n```\n\n### Some other tips\n\n- There are no restrictions on interface declaration, it's better to have interface declaration at the client side or at the location where the package is being used. This limits the third-party packages from accessing the whole struct/data.\n- Don't exploit the _Duck typing_ of interfaces and structs in Go. Limit the size of the interface and use multiple interfaces for each set of functions.\n- Even though it is not mandatory to close the channels, it's still best practice to close them as it might free up some memory.\n- Use uni-directional channels instead of bi-directional because uni-directional channels restrict the function to either send or receive but not both creating some form of check.\n- Buffer channels will block sending/reading, so use buffer channels to make multiple goroutines work as a synchronous function or communicate with these buffer channels.\n- Mutexes are slow, so use RWMutex which may be fast for reads at least.\n- Passing large data as an argument to function involves copying and creating the new data. Use pointers and don't modify the data in the called function.\n\n---\n\nThese are some of the common mistakes we make in Go and also good practices that I felt worth sharing. Also check out my previous article on [Advanced Go](https://santhalakshminarayana.github.io/blog/advanced-golang-memory-model-concurrency) about memory model and concurrency in Go.\n\n### References\n\n- https://go.dev/wiki/ExperienceReports\n"},{"metadata":{"title":"Advanced Go: Internals, Memory Model, Garbage Collection and Concurrency","description":"Deep dive into the Go's Memory model, the internals of Go's data structures, Garbage collection and Concurrency model.","imgName":"advanced-golang/advanced-golang.jpg","date":"Mar 23, 2024","tags":["go","concurrency"],"keywords":["go","golang","advanced-go","concurrency","memory-model","go-concurrency","go-internals","garbage-collection","go-csp","csp"],"id":"advanced-golang-memory-model-concurrency"},"content":"\n![Advanced Go: Memory Model, Concurrency and Garbage Collection](advanced-golang/advanced-golang.jpg)\n\n###### Published on: **Mar 23, 2024**\n\n# Advanced Go: Internals, Memory Model, Garbage Collection and Concurrency\n\nSince the release of Go in 2009, Go has gained a lot of traction among developers and become the de-facto choice of programming language for writing efficient Microservices and also command-line tools. Go's popularity comes with its simplicity and how effectively it solves complex concurrency problems with its simple concurrency model. Programmers coming from C/C++ backgrounds can easily start developing applications in Go even with less than 2-3 days of learning. That's much simple Golang is. Golang has been developed keeping in mind modern programming language paradigms and principles and to avoid legacy problems that can't be avoided by other languages. The way Golang demands writing and structuring the code lets developers avoid boring Object-oriented patterns.\n\nEven though developers don't need to understand how Golang internally works, it's better to learn what the components and techniques implemented in Golang make it one of the best modern programming languages. In this blog, we will delve into some of the important concepts that make a better understanding of Golang and its internals.\n\n\u003e I will be very brief about the following concepts as each one of them requires separate writing and mostly it will be like notes with some important points. The readers are encouraged to follow the links that will be provided for each concept to learn further.\n\n## Go vs C/C++\n\nThe developers of Go created the programming language along the lines of **C/C++**. Even though Go is a compiled language and most of the time it is close to the speed of C/C++, Go is not a worthy contestant for C/C++ in terms of speed. Check this [Go vs C++](https://benchmarksgame-team.pages.debian.net/benchmarksgame/fastest/go-gpp.html) comparison. But for normal day-to-day tasks like writing APIs, handling network connections, and processing medium-sized data (for larger-size, prefer Rust), Go can be 1.5 times in comparison speed to C/C++ but not orders in comparison. Most of the time this speed difference is not noticeable. This makes Go the best choice for writing Microservices or web applications compared to Java, C#, or Python. Earlier Go's compiler was written in C but now changed to pure Go.\n\nEven though Go is a compiled language and inspired by C/C++, it avoids some common compiler practices as the Go compiler doesn't do optimizations like C/C++ due to its adaptation of [better memory model compared to C/C++](https://go.dev/ref/mem#badcompiler) (more of this in a later section). The compiler optimizations done in C++ might result in dangerous pointer operations giving unexpected computations and sometimes leading to program crashes. One other important thing that makes Go slower than C/C++ is the Garbage collection implemented in Golang. So, Go has to interfere in the program lifecycle to maintain the liveness of the objects, schedule them for cleaning in the next cycle and other operations add some overhead where there is no automatic garbage collection in C++ but only manual de-allocation which causes problems sometimes (this is one of the problems that Rust solves).\n\nAnd the most important part of Golang is compilation speed compared to C++. Go's superior compilation makes it the preferred heavy-size application that involves multiple iterations of compilation and deployment to test. It's like a trade-off to C++ as the latter gives performance but takes so much time in compilation where as Go is reasonable speed and compiles very fast.\n\n## Go Memory model\n\nFor any programming language memory model is one of the most important components. The techniques and principles used in the memory model determine how the programming language should be designed and written.\n\nA memory model speaks about how the program handles the data race and also the operations performed at the memory location and atomic level like what happens when there is simultaneous read or write operations happening at the same memory location or what if two or more parties are writing to the same memory location.\n\nBefore going forward on memory models, it's worth learning how Go treats References. [There are no references in the Go](https://dave.cheney.net/2017/04/29/there-is-no-pass-by-reference-in-go). People confuse with **\u0026 operator** in Go with the references in C++. There are only pointers in Go where the variable points to the same location of memory but doesn't share the same memory. Why is this? Because Go is strict about dangling pointers that we face in C++. So, to avoid the dangling memory object Go avoids variables with pointers directly referencing to memory location. Go passes data by value for functions, no reference data passing.\n\nEarlier languages like C/C++, Python, and Java were developed when the programs were developed only for single-threaded applications. Go, being a modern language where harvesting all computing resources is essential like utilizing all CPU cores, has been designed to avoid problems involving simultaneous access to the same memory location. Some languages like C/C++ and Java solve this problem by using Locks and Mutexes. Even though Go has native support for Locks and Mutexes, it's not the goal for Go to solve this problem by using them. The Golang inherently solves this problem through better synchronization, restrictions of data race conditions, and a better-designed memory model. By definition, all Python native operations are atomic as it only single-threaded, but in a language like Go where concurrency is the main weapon, it has to do something extraordinary to avoid data race conditions.\n\nThe gist of the [Go's memory model](https://research.swtch.com/gomm) is, that when multiple goroutines are performing read/write operations on the same memory location, the accessing variables should only see the value before or after the value has been written but not in between. For example,\n\n```text\nIf A, B, C, and D are the three variables accessing the same location that has a value of 40\n\nIf A is reading\nB is Writing it as 50\nC is writing it as 60\nD is also reading\n\nThen, the following could be one of the outcomes without data race conditions\n1. A will read the value as 40\n2. B, in the meantime changes the values to 50\n\nIf operation C is performed before D, then\n3. C also changes the value from 50 to 60\n4. D will eventually read 60\n\nelse,\n3. D will read 50\n4. C will write it as 60\n\nHere all operations are performed without race conditions and all are atomic even though all are happening at the same time.\n```\n\nIf the same thing happened in C/C++, the outcome is undetermined like\n\n```text\nA might read the value as 40\n\nWhile B is writing, C also writes to the same location,\nand D might read the value as 35 or 240 or even as -3002.\n\nThis behavior is unexpected and undetermined.\n```\n\nHow Go does this is very simple, it organizes the operation by goroutines as follows:\n\n- Each goroutine execution is a set of memory operations\n- Each Go program execution is a set of goroutine executions in a synchronized behavior where any write operation is executed before any read operation that comes after without executing them both\n- Any write is happening, no other write operation is performed on the same location.\n\nAs discussed in the above section, to avoid data race conditions that happen when multiple goroutines access the same memory location, the compiler optimizations were excluded in Go.\n\nAnother thing is, that Go implements the memory model with one heap, multiple stacks, and a single **constants and instructions** memory section. Stack borrows memory from the limited heap. We can dynamically increase the heap memory and the stacks are nothing but the goroutines which we see in later sections.\n\n## Goroutines and Concurrency\n\nGo is a Concurrency focused programming language contrary to multi-programming/parallel. Check out this famous video on [Concurrency is not Parallelism](https://www.youtube.com/watch?v=oV9rvDllKEg) by Rob Pike who is one of the creators of Go. Go concurrency model is based on [Communicating sequential processes (CSP)](https://en.wikipedia.org/wiki/Communicating_sequential_processes). The famous quote/Go idiom **[Do not communicate by sharing memory; instead, share memory by communicating](https://go.dev/blog/codelab-share)**. It simply means that other programming languages that were developed when the application was running with only a single thread era, those programming languages avoid data race conditions by using locks/mutexes to communicate between multiple threads. Later synchronized data structures like Queues and Pipes came into existence to solve these problems. Go being a modern programming language looks at this problem differently by taking inspiration from CSP where the process/goroutine that needs to send/notify the data to other process/goroutine is performed by completely passing the ownership of the memory in the synchronized data structure called **Channels**. The goroutine1 passes the data into this channel where goroutine2 listens and reads where the data ownership is completely handed over to goroutine2.\n\nEven though Go is a concurrent programming language, we can do parallel programming with multiple Goroutines. This can be done because Go multiplexes the multiple goroutine execution to multiple OS threads for multiple cores in the machine. This makes goroutines work in parallel whereas they are processed concurrently by the scheduler. This was a game changer in the computing industry where multiple programming languages started implementing concurrency with Coroutines which have been in existence since the 1950s. Python has these coroutines, Java also introduced lightweight threads, and the big brother C++ also introduced coroutines.\n\nJust like coroutines, goroutines are very lightweight, and the memory overhead is very minimal making goroutines the perfect choice for concurrency compared to threads. 90% of concurrency problems can be solved by using only goroutines and channels. Also, switching between goroutines is done by the process whereas switching between threads is done by OS. This makes goroutines faster.\n\nIt's good to have the following points to remember while using goroutines and concurrency:\n\n- Select takes the first non-blocking case in a pseudo-random order. So, we cannot expect which block will execute in order as it can be random.\n- Avoid using low-level data structures like Mutexes instead use the _sync_ package for atomic-level access to primitive data types/structures.\n- Atomic operations are slow, so use only when required. And don't switch between atomic and non-atomic functions.\n- Apply concurrency at call-site: This means if there are multiple synchronous functions, then organize these functions as asynchronous calls by implementing concurrency at the function that is calling these synchronous functions.\n\nGo starts goroutines with small stack memory (2kb size) and dynamically stores the data in a heap when data is grown. A stack per goroutine. This makes, scheduling goroutines very easy. Just like functions that are stored as stacks, when one goroutine spins another goroutine, the execution state is stored for the first goroutine in the stack, and the newly created goroutine is executed as a new stack in a new CPU core. This way of treating goroutines as stacks makes the scheduler suspend the goroutine execution, and continuing the execution is very easy even if it can continue the execution in a separate core completely because the goroutine state is stored in the stack.\n\n## Garbage collection\n\nWith GC, Go provides security, and fast memory access, as most of the memory is allocated in a stack. Whereas in C++, memory allocation and de-allocation in the heap should be managed by the program itself.\n\nAs in any language, the local variables (non-pointer) of a function are cleaned right away when the function is out-of-scope. So, Go doesn't care about this cleaning of local variables. As we all know these variables are stored in the stack these do not need to be looked up for recycling as the process is dynamic. GC handles the memory stored in the heap which is finite in size and the compiler should take care of this heap memory from time to time to avoid the out-of-memory issue (out-of-memory issue is not avoidable in some cases and manual cleaning is highly preferred instead of relying on GC completely). The typical data structures that are stored in the heap are Slices, Maps, and unbounded Channels whose size can be dynamically increased over time in the programming lifecycle. So typically variables with pointers to these data structures are stored in the heap.\n\nOne of the popular GC techniques involves maintaining the object reference count like in Python and cleaning the memory when the reference is 0. Go approaches a different technique called [**Mark-and-Sweep**](https://tip.golang.org/doc/gc-guide#Tracing_Garbage_Collection) where GC traces all objects that are referred by the pointers as live heap and sweeps the memory held by non-live heap object for allocation of new data.\n\nFor a GC to run and do its work, it needs CPU time but frequent GC makes the program run slow. But, if GC executes in long intervals, the memory in the heap might pile up leading to out-of-memory for newly coming memory allocations. So, GOCC maintains a trade-off between CPU and memory with GC frequency. It's very complex to define how GC works because there are numerous parameters to control the frequency of GC depending on the application's needs. Read further about how [GC frequency is calculated](https://tip.golang.org/doc/gc-guide#Tracing_Garbage_Collection). Simply put, we can think GC will execute every 2 seconds (let's assume), and if in the meantime, when the heap memory is increased than the defined threshold at 1.5 seconds, then GC will trigger automatically.\n\nFor large and memory-intensive applications, it's recommended to run the GC cycle manually or define the behavior of the GC cycle and its frequency by setting parameters like target memory heap size and others.\n\n## Internals of Slices, Maps, and Channels\n\nAs discussed above there are no references in Go and Maps, Slices, and Channels are simply pointers but not actual data, their zero values are nil. This means, the compiler allocates the values of these data structures in the memory if they are pre-allocated size or else in the heap if they are dynamic and gives the pointer reference to the variables. If the pre-allocated size for Slices or Maps increases, they are moved to heap memory.\n\nWe know Slices are the pointers to the underlying Array storage. But, Maps and Channels are direct pointers to the underlying heap data structure that holds these values. So, these all are pointers to some data sources.\n\nMaps and Slices are passed to functions by values and those values are pointers so that changes to duplicate variables change the source (not to be confused with pass-by-reference). Why is this? because inside the function, when the slice is grown/shrunk, a new array will be created by copying the old one and the new one will be referenced.\n\nOne more important thing is unlike Slices that are just pointers to backing arrays, for maps, the data is allocated arbitrarily in something called bucket memory structures when the map grows/shrinks. So, we cannot take the direct pointers to the map at that instance because the content of the map can be moved to another location if it shrinks/grows and if we have any reference to that old memory then it becomes a dangling pointer and Go doesn't allow dangling pointers. As the values of the keys can be changed dynamically, we cannot access the address to keys with pointers but values can be pointers.\n\nFor channels, one of the most important points to remember is, that when the sender has sent the data and holds the pointer to it, the sender should not modify the data holding by that pointer. Also, it's recommended to use read-only channels when the channels are passed to any functions.\n\n---\n\nThe concepts discussed in this blog are very abstract and you're encouraged to delve deeper. Unlike Java or Python, there is very little to learn about Go and we do not even need to look further at what is not provided in the official documentation as everything has taken care of by Go itself making it's a simple and powerful language to use.\n\n### References\n\n- [Effective Go](https://go.dev/doc/effective_go)\n- [The Go Memory Model](https://go.dev/ref/mem)\n- [Concurrency is not Parallelism](https://www.youtube.com/watch?v=oV9rvDllKEg)\n- [GO Advanced concurrency](https://github.com/alextanhongpin/go-advance-concurrency)\n- [Concurrency Patterns In Go](https://www.youtube.com/watch?v=YEKjSzIwAdA)\n- [Go Concurrency Patterns](https://www.youtube.com/watch?v=f6kdp27TYZs)\n- [Advanced Go Concurrency Patterns](https://www.youtube.com/watch?v=QDDwwePbDtw)\n- [Rethinking Classical Concurrency Patterns](https://www.youtube.com/watch?v=5zXAHh5tJqQ)\n- [Memory Optimization and Garbage Collector Management in Go](https://betterprogramming.pub/memory-optimization-and-garbage-collector-management-in-go-71da4612a960)\n- https://dave.cheney.net/2017/04/30/if-a-map-isnt-a-reference-variable-what-is-it\n- https://stackoverflow.com/questions/32495402/why-does-go-forbid-taking-the-address-of-map-member-yet-allows-slice-el\n"},{"metadata":{"title":"Asynchronous or Concurrency patterns in Python with Asyncio","description":"Effective implementation of asynchronous or concurrency patterns like Background task and Worker Pool in Python using Asyncio.","imgName":"concurrency-patterns-python/concurrency-patterns-python.jpg","date":"Mar 6, 2024","tags":["concurrency","python-performance","python"],"keywords":["concurrency","asyncio","python-concurrency","worker-pool","job-queue","worker-queue","background-thread","background-task","background-job","python-performance","python-optimize","python","fast-python","speed"],"id":"concurrency-patterns-python"},"content":"\n![Asynchronous or Concurrency Patterns in Python](concurrency-patterns-python/concurrency-patterns-python.jpg)\n\n###### Published on: **Mar 6, 2024**\n\n# Asynchronous or Concurrency Patterns in Python with Asyncio\n\nMost software development tasks require an asynchronous way of handling things like running background tasks, processing multiple tasks at a time, applying the same operations on huge data, distributing tasks to free workers, etc. In this blog, we will see how can we achieve some of these patterns with Python.\n\nBefore delving into the concept, the most important thing we need to understand is _concurrency vs parallelism_. Concurrency is about handling multiple tasks at a time and this handling might be in parallel or not. Whereas parallelism is about doing multiple tasks parallelly/simultaneously at a time. Parallelism can be termed as concurrency but not vice-versa.\n\nIn my earlier blog posts about [Super Fast Python: Multiprocessing](https://santhalakshminarayana.github.io/blog/super-fast-python-multi-processing), I have described how can achieve parallelism with Python.\n\nOne can ask, in parallelism, if we can do multiple things at a time, why do we need to focus on concurrency at all? The answer to this question simply depends on the program language in the discussion. Some program languages like C/C++ are built to directly use multiple cores/vCPUs of the host machine for parallelism, while other languages like Golang can still utilize multiple cores but with concurrency, not parallel. Both of these types of languages can do both concurrency and parallelism very smoothly. But we have a special child like Python, whose creators put so many restrictions (which are thoughtful at creation) to be a better resource utiliser. GIL in Python makes the Python interpreter run only one thread at a time and cannot multiplex the multiple threads to multiple cores. I have already discussed this in my previous post about [Why Python is Slow](https://santhalakshminarayana.github.io/blog/super-fast-python-why-python-slow).\n\nTo utilize multiple cores, we have to create at least one process per core. The creation and memory management of the process is time-consuming and comes with its overheads. So, a general rule of thumb is to use multi-programming for CPU-intensive tasks, and I/O bound tasks like network calls, and reading from disk are handled concurrently with threads. As we can use multi-threading in Python, creating and managing a huge number of threads is not optimal in Python or any high-level language. The solution to create and manage huge numbers of thread-like objects say in thousands or 100s of thousands, we can use Co-routines which can be termed as virtual threads or small threads whose memory footprint is very minimal compared to threads. And we can spawn thousands of these in a few microseconds.\n\nCoroutines are part of core Python and are handled with an in-built package called [asyncio](https://docs.python.org/3/library/asyncio.html)\n\nThe goal of this article is to demonstrate some concurrency patterns rather than learning about coroutines. So, it's expected to have some basic background with _asyncio_.\n\nIn the following sections, we cover the two most used concurrency patterns:\n\n- Background tasks with Ticker\n- WorkerPool pattern\n- Pipeline or Chain processing\n\n## Ticker \u0026 Background tasks\n\nLet's say we have a stream of data coming and stored in some data sources like Database, Redis, or in the Cloud. And we want to analyze that data periodically let's say every 1 day and send an email notification to the customers with the analyzed report. This is a basic CRON job that we execute in Linux.\n\nIf you don't know what the is ticker, it is a kind of background task where the program will be notified for every time interval that we need to do some task. Tickers are all over any periodic notification or job updates.\n\nWe will use a ticker for our coroutine to come into the foreground and call a callback that it receives with callback arguments. Let's look at the following ticker class\n\n```python:ticker.py\nfrom threading import Event, Thread\nfrom typing import Any, Callable, Optional\n\nclass Ticker(Thread):\n    def __init__(\n        self,\n        is_daemon: bool,\n        tick_period: int,\n        callback: Callable,\n        **cbkwargs: Optional[Any]\n    ):\n        Thread.__init__(self)\n        self.daemon = is_daemon\n        self.tick_period = tick_period\n        self.task_kill_event = Event()\n        self.callback = callback\n        self.cbkwargs = cbkwargs\n\n    def kill(self):\n        self.task_kill_event.set()\n\n    def run(self):\n        while True:\n            is_killed = self.task_kill_event.wait(self.tick_period)\n            if is_killed:\n                print(\"Exiting ticker.\")\n                break\n\n            # call the callback\n            try:\n                self.callback(**self.cbkwargs)\n            except Exception as e:\n                print(f\"Exception for callback: {e}\")\n```\n\nThis class inherits the _Thread_ class to implement custom thread functionality. In the constructor, we pass the _is_daemon_ flag that makes the current thread run as a daemon or not. Usually for background tasks that do not stick to the application lifecycle, they are not meant to be daemon. And other params like _tick_period_ which says the time interval for the ticker, _callback_ is a callback function, and _cbkwargs_ are callback key arguments to pass to this callback function. We will speak about _self.task_kill_event = Event()_ later.\n\nThis class has two methods _kill_ and _run_ where _run_ is an overridden _Thread_ class function.\n\n[_threading.Event()_](https://docs.python.org/3/library/threading.html#event-objects) object is used for communication between threads. It's simply a boolean flag that one thread signals the other thread that listens to this event object states. We use an event object to signal for the background task to exit. This is what the _kill_ function is. If we call this function, it will set the event to be **True** signaling the listing thread.\n\nThe overriding _run_ method is a classic implementation of a background running thread, it will run in a loop until it receives the _is_killed_ to be **True** which we set by calling _kill_. _self.task_kill_event.wait(self.tick_period)_ is where the magic happens. Here we will wait for the specified ticker period for that event to be set. After the specified interval period, if we don't receive the event signal, the **is_killed** will be False, and we will call the callback with the keyword arguments.\n\nWhy don't we use a simple _thread.sleep_ for sleeping in the background? Let's say, that while waiting for a ticker period, we get the event signal, then we have to right away exit the thread. For example, if the ticker period is for 10 seconds, and while in the 6th second, we get the signal, we will exit there instead of waiting for the other 4 seconds. This can't be achieved with thread sleep. This is very useful if we are running larger ticker periods let's for days or weeks.\n\nLet's look at how we can run this ticker thread for every time inteval.\n\n```python:ticker_example.py\nimport asyncio\nfrom ticker import Ticker\n\nprocess_called = 0\ndef process(task_for=\"\"):\n    global process_called\n    process_called += 1\n    print(\"process called for: \", process_called, \" times\")\n\nasync def stop_ticker(kill_func: Callable, after_time: int):\n    await asyncio.sleep(after_time)\n    kill_func()\n    # call any cleanup functions for graceful exiting\n    await asyncio.sleep(1)\n\nif __name__==\"__main__\":\n    cbkargs = {\"task_for\": \"data-pre-process\"}\n    ticker = Ticker(True, 3, process, **cbkargs)\n    ticker.start()\n\n    ev_loop = asyncio.get_event_loop()\n    ev_loop.run_until_complete(stop_ticker(ticker.kill, 7))\n```\n\n```shell:Output\nprocess called for:  1  times\nprocess called for:  2  times\nExiting ticker.\n```\n\nIn the above example, we have imported the _Ticker_ class and defined a function _process_ which we pass as a callback to this ticker. This function will be called for after every ticker period and this is where we do any periodic processing like sending notifications.\n\nThe line _ticker = Ticker(True, 3, process, \\*\\*cbkargs)_ initializes the ticker with **3** second time interval. We have defined another function _stop_ticker_ which we will use to kill the ticker after some time. The stop ticker function will wait for some time (7 seconds here) and kill the ticker.\n\nAs we can see in the output, the ticker called the callback every 3 seconds and got stopped by the parent thread which is our main thread here through _stop_ticker_ function.\n\n---\n\nThis is a very effective way of defining a ticker in Python with a simple thread mechanism. Now we will see how we can define a worker pool for processing multiple jobs by multiple workers concurrently.\n\n## Worker pool/Job queue pattern\n\nWe can define worker pool concurrency patterns with threads alone using multi-threading. But there are limitations with threads, like:\n\n- Threads are synchronous, meaning when a thread is paused it blocks the execution also, leaving other threads for starvation\n- we cannot create a huge number of threads due to various reasons like memory, context switching, etc.\n\nSo, we can use a coroutine to do this job because we can have an asynchronous way of doing things if have to call any API or wait to load the huge dataset, we can pause the current coroutine and let others claim the CPU. As Python is single-threaded, meaning only 1 thread can run at a time, this un-blocking way of organizing your tasks makes your Python code efficient and performant. And we can create thousands of coroutines in quick time and they occupy very little memory.\n\nThe worker pool pattern is a simple and the most widely used concurrency pattern for distributing multiple tasks or jobs (n-jobs) to multiple workers (m-workers). Usually _n \u003e= m_. In this pattern, we have a list of jobs to be processed, and they are to be read from a static queue or a dynamic stream. And we have to distribute those jobs to idle workers who take the job and process it. Worker may or may not return the result based on the type of the Job. Idle workers are very hungry and they compete with other idle workers to take the incoming Job or un-attended Job.\n\n![Worker-pool pattern with Job Queue:=:70:=:Worker pool with Job Queue. Each idle worker takes one job at a time and processes the Job. After the job is complete, the worker will continue to take any un-attended Job.](concurrency-patterns-python/job-queue-worker-pool.jpg)\n\nAs in the above image, the jobs are stored in some data structure say a Job Queue, and we have a pool of workers. We usually incorporate a scheduler/distributor to assign the Jobs to idle Workers. So, at a time, if we have **m** workers, then we can process **m** Jobs concurrently. If we can access multiple cores, then these Jobs are processed parallelly like in Golang. Because Python has some overhead for forking multiple processes, we will stick to single-threaded concurrent execution of Jobs by multiple Workers. The following code implements this worker pool pattern where we have to process the n-number of Jobs.\n\n```python:worker_pool.py\nimport asyncio\nfrom asyncio.queues import Queue\nfrom threading import Thread\nfrom typing import Any, Callable, List\n\n\nclass WorkerPool(Thread):\n    def __init__(\n        self,\n        is_daemon: bool,\n        jobs: List[Any],\n        n_workers: int,\n        callback: Callable[[List[Any]], None],\n    ):\n        Thread.__init__(self)\n        self.daemon = is_daemon\n        self.jobs = jobs\n\n        self.event_loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(self.event_loop)\n\n        self.job_queue = Queue()\n        self.n_workers = n_workers\n        self.callback = callback\n\n    async def distribute(self, w_name: str):\n        while self.job_queue.qsize() \u003e 0:\n            job = await self.job_queue.get()\n            try:\n                # call the callback with the Job details\n                await self.callback(job)\n            except Exception as e:\n                raise Exception(f\"Exception for callback: {e}\")\n            finally:\n                # mark the current job as done\n                # will get an exception if it's already done by the current worker or any other worker\n                try:\n                    self.job_queue.task_done()\n                except ValueError:\n                    print(f\"{w_name}: Task already done.\")\n                    return\n\n    async def job_wp(self):\n        # add jobs to the job-queue.\n        # this job queue can be concurrently accessed by multiple workers\n        for i in range(len(self.jobs)):\n            await self.job_queue.put(self.jobs[i])\n\n        tasks = []\n\n        # create multiple workers that take un-attended jobs from the job queue\n        # and continuously process them until all jobs are executed\n        for i in range(self.n_workers):\n            tasks.append(self.distribute(f\"Worker: {i+1}\"))\n\n        await asyncio.gather(*tasks)\n\n    def run(self):\n        try:\n            self.event_loop.run_until_complete(self.job_wp())\n        except Exception as e:\n            print(f\"Error distributing jobs: {e}\")\n        finally:\n            self.event_loop.close()\n```\n\nHere, we extend the _Thread_ class to execute WorkerPool in a separate thread. The constructor takes the list of job items, _n_workers_ for how many workers to spawn, and a callback function to call with the job detail.\n\n_WorkerPool_ class defines two methods _job_wp_ and _distribute_. _job_wp_ creates a JobQueue with all jobs and also creates multiple workers each with a name **Worker: i** where _i_ is ith worker. This function will wait for all workers to complete processing all Jobs.\n\n_distribute_ function takes the unattended job and calls the callback with job details as parameters to the callback function. We can also extend this to pass arguments. After executing the job, we finally mark that job as done.\n\nExample of processing Jobs by multiple workers:\n\n```python:worker_pool_example.py\nfrom random import randint\nfrom time import time\nfrom worker_pool import WorkerPool\n\nasync def process_job(job_detail: Any):\n    time_to = randint(1, 4)\n    print(\"Processing job:\", job_detail, \"will take:\", time_to, \"seconds\")\n    await asyncio.sleep(time_to)\n    print(\"Completed job:\", job_detail)\n\nif __name__==\"__main__\":\n    wp = WorkerPool(True, list(range(1, 11)), 3, process_job)\n    atime = time()\n    wp.start()\n    wp.join()\n    print(\"time taken:\", str(int(time()-atime)) + \"s\")\n```\n\nHere, we initialize the WorkerPool class with **10** jobs and **3** workers along with a callback _process_job_. This _process_job_ function might take any of [1, 4] seconds to complete.\n\n```shell:Output\nProcessing job: 1 will take: 3 seconds\nProcessing job: 2 will take: 1 seconds\nProcessing job: 3 will take: 1 seconds\nCompleted job: 2\nProcessing job: 4 will take: 4 seconds\nCompleted job: 3\nProcessing job: 5 will take: 4 seconds\nCompleted job: 1\nProcessing job: 6 will take: 3 seconds\nCompleted job: 4\nProcessing job: 7 will take: 1 seconds\nCompleted job: 5\nProcessing job: 8 will take: 1 seconds\nCompleted job: 6\nProcessing job: 9 will take: 2 seconds\nCompleted job: 7\nProcessing job: 10 will take: 1 seconds\nCompleted job: 8\nCompleted job: 10\nCompleted job: 9\ntime taken: 8s\n```\n\nWith this asynchronous processing of Jobs, we have processed all the jobs whose collective processing time requires **21 seconds** in **8 seconds** thanks to asyncio whereas with un-optimized multi-threading, it might take the same **21 seconds** due to threads blocking nature.\n\n---\n\n## Pipeline or Chain processing\n\nPipeline pattern or Chain processing is a simple concurrency pattern of executing the multiple jobs consecutively where the input for the current job is the output of the previous job. For example, let's say we have to analyze incoming data about customer ratings (scale: 1-5) for our service. And we have to do the following steps:\n\n- Analyse the feedback\n- If the rating is below 3, we have to store this rating for later analyses\n- Later generate the thank you message for the customer\n- Finally, based on the preference notification channel (Email or SMS) we will send a thank you message.\n\nThis whole process needs a couple of logical decisions to take at each level of processing. Based on multi-logic flow, we can divide the above process into small functions and we execute each function sequentially based on where the next function call is decided based on the current input data.\n\n![Pipeline or Chain processing concurrency patter:=:100:=:Processing pipeline/flow](concurrency-patterns-python/pipeline-or-chain-concurrency-processing.jpg)\n\nIf we have multiple customer ratings to be analyzed we can combine the **WorkerPool** pattern with this Pipeline pattern.\n\n```python:pipeline.py\nimoport asyncio\nimport random\nfrom worker_pool import WorkerPool\n\ncommuincation_type = [\"email\", \"sms\"]\n\nasync def process_rating(job: list):\n    rating, user, idx = job\n    print(f\"1. processing rating, for item: {idx}, rating: {rating} by user: {user}\")\n    # some processing\n    await asyncio.sleep(1)\n    if rating \u003c= 2:\n        await store_for_analyses(rating, idx)\n    else:\n        await form_thankyou_message(user, idx)\n\nasync def store_for_analyses(rating: int, idx: int):\n    print(f\"1a. storing for analyses, for item: {idx}\")\n    # store in some datastore\n    await asyncio.sleep(1)\n\nasync def form_thankyou_message(user: str, idx: int):\n    print(f\"2. forming thank-you message, for item: {idx}\")\n    # a DB call to fetch the user details\n    await asyncio.sleep(1)\n\n    # communication medium may be {email, sms}\n    cs = random.choices(commuincation_type, weights=[0.5, 0.5], k=1)[0]\n\n    match cs:\n        case \"email\":\n            await send_email_message(user, idx)\n        case \"sms\":\n            await send_sms_message(user, idx)\n        case _:\n            return\n\nasync def send_email_message(user: str, idx: int):\n    print(f\"2a. sending email message, for item: {idx}, for user: {user}\")\n    # sendinig might take time\n    await asyncio.sleep(1)\n\nasync def send_sms_message(user: str, idx: int):\n    print(f\"2b. sending sms message, for item: {idx}, for user: {user}\")\n    # sendinig might take time\n    await asyncio.sleep(1)\n\n\nif __name__==\"__main__\":\n    # create ratings, users, and items\n    n = 10\n    ratings = random.choices([1, 2, 3, 4, 5], weights=[0.25, 0.25, 0.15, 0.15, 0.2], k=n)\n    items = list(range(1, n+1))\n    users = list(map(lambda x: f\"User {x}\", items))\n\n    jobs = []\n    for i in range(n):\n        jobs.append([ratings[i], users[i], items[i]])\n\n    wp = WorkerPool(True, jobs, 3, process_rating)\n    wp.start()\n    wp.join()\n```\n\nHere we define a list of functions that are depicted in the image above. Each function will take input from the previous function and apply its decision logic to call which function next.\n\nIn the example, we have created 10 rating items to be processed by 3 workers. The output of the processing is\n\n```:Output\n1. processing rating, for item: 1, rating: 4 by user: User 1\n1. processing rating, for item: 2, rating: 3 by user: User 2\n1. processing rating, for item: 3, rating: 3 by user: User 3\n2. forming thank-you message, for item: 1\n2. forming thank-you message, for item: 2\n2. forming thank-you message, for item: 3\n2b. sending sms message, for item: 1, for user: User 1\n2a. sending email message, for item: 2, for user: User 2\n2a. sending email message, for item: 3, for user: User 3\n1. processing rating, for item: 4, rating: 2 by user: User 4\n1. processing rating, for item: 5, rating: 5 by user: User 5\n1. processing rating, for item: 6, rating: 4 by user: User 6\n1a. storing for analyses, for item: 4\n2. forming thank-you message, for item: 5\n2. forming thank-you message, for item: 6\n1. processing rating, for item: 7, rating: 5 by user: User 7\n2a. sending email message, for item: 5, for user: User 5\n2b. sending sms message, for item: 6, for user: User 6\n2. forming thank-you message, for item: 7\n1. processing rating, for item: 8, rating: 5 by user: User 8\n1. processing rating, for item: 9, rating: 1 by user: User 9\n2b. sending sms message, for item: 7, for user: User 7\n2. forming thank-you message, for item: 8\n1a. storing for analyses, for item: 9\n1. processing rating, for item: 10, rating: 5 by user: User 10\n2b. sending sms message, for item: 8, for user: User 8\n2. forming thank-you message, for item: 10\n2b. sending sms message, for item: 10, for user: User 10\n```\n\n---\n\n## Custom coroutine\n\nIf you want an asynchronous function in a separate thread with the async capabilities, we can run a coroutine as follows\n\n```python:coroutine.py\nimport asyncio\nfrom threading import Thread\nfrom typing import Any, Callable, List, Optional\n\nclass CustomCoroutine(Thread):\n    def __init__(\n        self,\n        is_daemon: bool,\n        callback: Callable[[List[Any]], None],\n        **cbkwargs: Optional[Any]\n    ):\n        Thread.__init__(self)\n        self.daemon = is_daemon\n        self.event_loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(self.event_loop)\n        self.callback = callback\n        self.cbkwargs = cbkwargs\n\n    async def callback(self) -\u003e None:\n        try:\n            await self.callback(**self.cbkwargs)\n        except Exception as e:\n            print(f\"Error calling callback: {e}\")\n\n    def run(self) -\u003e None:\n        try:\n            self.event_loop.run_until_complete(self.callback())\n        finally:\n            self.event_loop.close()\n```\n\n---\n\nI hope the above concurrency patterns give you some understanding of organizing your Python code for better performance and optimization. Other concurrency patterns that are worth noting are:\n\n- **Monitor pattern**: **n** number of threads waiting on some condition to be true, if the condition is not true, those threads need to be in a sleep state and pushed to the wait queue, and they have to be notified when the condition becomes true.\n- **Double Checked locking**: for creating concurrent objects. (Ex: Singleton design pattern)\n- **Barrier pattern**: All concurrently executing threads must wait for others to complete and wait at a point called barrier.\n- **Reactor pattern**: In an event-driven system, a service handler accepts events from multiple incoming requests and demultiplexes to respective non-blocking handlers.\n"},{"metadata":{"title":"Super fast Python (Part-5): Numba","description":"Speed up Numerical computations and functions in Python with Numba and Numpy.","imgName":"super-fast-python-numba/super-fast-python-numba.jpg","date":"Dec 25, 2022","tags":["python-performance","python"],"keywords":["numba","python-performance","python-optimize","python","fast-python","speed","jit","numba-numpy"],"id":"super-fast-python-numba"},"content":"\n![Super fast Python: Numba](super-fast-python-numba/super-fast-python-numba.jpg)\n\n###### Published on: **Dec 25, 2022**\n\n# Super fast Python (Part-5): Numba\n\nThis is the fifth and last post in the series on Python performance and Optimization. The series points out the utilization of inbuilt libraries, low-level code conversions, and other Python implementations to speed-up Python. The other posts included in this series are\n\n- (Part-1): [Why Python is slow?](https://santhalakshminarayana.github.io/blog/super-fast-python-why-python-slow)\n- (Part-2): [Good practices to write fast Python code](https://santhalakshminarayana.github.io/blog/super-fast-python-good-practices)\n- (Part-3): [Multi-processing in Python](https://santhalakshminarayana.github.io/blog/super-fast-python-multi-processing)\n- (Part-4): [Use Cython to get speed as fast as C](https://santhalakshminarayana.github.io/blog/super-fast-python-cython)\n- (Part-5): Use Numba to speed up Python Functions (this post)\n\nIn the last post about [Cython to speed-up Python code](https://santhalakshminarayana.github.io/blog/super-fast-python-cython), we discussed writing Python code in C-style, compiling that code separately into an object file, and using that generated file as an import directly into Python. But, not all people would feel comfortable writing C-style code or even some might not know C at all. So, to deal with such cases and get the performant efficient code to speed-up Python, one can use [Numba](https://numba.pydata.org/) instead. Numba translates Python code to machine code that executes almost as fast as C/C++ if optimized correctly.\n\n## What is Numba?\n\nNumba is a JIT (just-in-time) compiler that takes Python byte code and compiles it into machine code directly using [LLVM](https://llvm.org/) compiling mechanism. JIT is a type of interpreter that compiles frequently called code into machine code and caches that generated machine code to be used later for faster execution type. Here, Numba also takes Python code and generated machine code which the Python interpreter calls directly instead of interpreting and converting to machine code each time. Numba works best for numerical calculations, Array and Numpy operations, and loops. With Numba, we can write vectorized operations and parallelized loops to run on either CPU or GPU.\n\nNumba decorators are one of the many ways to invoke the JIT compilation. Numba provides different decorators to compile code in different modes and types, the common decorators used in Numba are:\n\n- @jit - invoke JIT compilation for the provided function\n- @njit - @jit decorator with enabling strict no-python mode\n- @vectorize - convert normal functions into Numpy like **ufuncs**\n- @guvectorize - generalized **ufuncs** for higher dimensional arrays\n- @stencil - make a function behave as a kernel for a **stencil** like operation\n\nNumba also provides different options to pass for some of these decorators to configure the JIT compilation behavior\n\n- nopython\n- parallel\n- cache\n- nogil\n- fastmath\n- boundscheck\n- error_model\n- cuda\n\n## Numba @jit\n\n**@jit** decorator takes the Python function that needs to machine code compiled. When we make a call to the function we provided to **@jit**, upon the first time calling, Numba compiles the function, caches the machine code, and this machine code is directly used for the execution. As compilation takes time, the first-time call to the function gives some latency. But, for consecutive function calls in the same runtime, just the cached machine code is used instead of re-compiling every time.\n\nLet's consider the following simple function _solve_expression_ as an example. _solve_expression_ takes some arguments, checks some conditions, and calculates the final polynomial expression.\n\n```python\ndef solve_expression(x, a, b, c, d):\n    A, B, C, D = a, b, c, d\n    if a \u003e 10.1:\n        A = 2 * a\n    if 2.6 \u003c= b \u003c 8.3:\n        B = b - 1/b\n    if c \u003e 4.5:\n        C = 4\n    if d \u003c 9.0:\n        D = d ** 2\n\n    return A*(x**3) + B*(x**2) + C*(x) + D\n```\n\nNow, use the **@jit** decorator to compile this function into machine code as\n\n```python\nfrom numba import jit\n\n@jit\ndef solve_expression(x, a, b, c, d):\n    A, B, C, D = a, b, c, d\n    if a \u003e 10.1:\n        A = 2 * a\n    if 2.6 \u003c= b \u003c 8.3:\n        B = b - 1/b\n    if c \u003e 4.5:\n        C = 4\n    if d \u003c 9.0:\n        D = d ** 2\n\n    return A*(x**3) + B*(x**2) + C*(x) + D\n```\n\nIn the code snippet, we have imported the **jit** function decorator and decorated _solve_expression_ with it.\n\n```python\nx, a, b, c, d = 2, 13, 1.2, 4, 7\n\nres = solve_expression(x, a, b, c, d)\n```\n\n\u003e Inspect the Intermediate Representation (IR) of the function using solve_expression.inspect_types()\n\nAs Numba **@jit** defers the JIT compilation until it encounters the first call to the function, the function call with arguments _solve_expression(x, a, b, c, d)_ takes some time for executions. But, function calls later at this point will be fast.\n\nNow compare the speeds of the normal Python function and Numba JIT decorated function. Using _solve_expression.py_func()_, we can invoke the normal python function of this JIT decorated function.\n\n```python\n%% timeit\nres = solve_expression.py_func(x, a, b, c, d)\n\n'''Output\n912 ns ± 3.47 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n'''\n```\n\nThe normal Python function takes approx. 900 nanoseconds.\n\n```python\n%%timeit\nres = solve_expression(x, a, b, c, d)\n\n'''Output\n277 ns ± 1.36 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n'''\n```\n\nAnd the Numba version takes approx. 280 nanoseconds. The Numba version is 3x times faster than the pure Python function.\n\n### Compilation options\n\nFor **@jit** decorator, we can pass multiple options to configure the compilation behavior\n\n- **nopython**: if True, enables no-python mode making code execution without Python interpreter interference\n- **nogil**: if True, releases the GIL (only when _nopython=True_) inside the compiled function, useful for concurrent execution such as threads\n- **cache**: if True, store the compiled code in local storage and use this code whenever the function is called instead of re-compiling for every runtime\n- **parallel**: if True, enables automatic parallelization\n- **fastmath**: if True, uses faster math operations but less safe floating-point operations\n\nApart from these, there are some more options available. You can check all options at [@jit reference](https://numba.readthedocs.io/en/stable/reference/jit-compilation.html#jit-functions).\n\n## Lazy and Eager compilation\n\nNote that, as Python function arguments can take any time of arguments, Numba compiles the function for that specific type of the argument passed. If a new type of argument is passed to the function while calling, Numba re-compiles the code for that specific type.\n\n### Lazy compilation\n\nThe above _solve_expression()_ function takes any type of argument. If we provide a function like this Numba calculates the optimization steps to be done based on the argument types provided at the first function call. In this way, Numba infers the argument types and compiles the specific version of the same function for different types.\n\nEx: if we change the argument types like this\n\n```python\n# previous values, x, a, b, c, d = 2, 13, 1.2, 4, 7\n# new values\nx, a, b, c, d = 3.9, 12, 5, 9.1, 14\n\nres = solve_expression(x, a, b, c, d)\n```\n\nAs the previous compiled function expects types for _x=int, a=int, b=float, c=int, d=int_, and in the latest function call we have changed some argument types. So, Numba re-compiles the function for new argument types. This mode of the compilation of code is called lazy compilation because Numba compiles for specific argument types only if it encounters them.\n\n### Eager compilation\n\nFor function overloading, we can specify function signatures with argument types and return types in a list with the least significant precision at the top. [Numba types](https://numba.readthedocs.io/en/stable/reference/types.html) follow Numpy convention types with different precision levels.\n\n```python\n@jit(['int32(int32, int32)',\n      'i4(int32, int64)',\n      '(f4, f8)',\n      'f8(f4, f4)'])\ndef func1(a, b):\n    return a + b\n```\n\nIn the above function, we passed function signatures as a list of strings. The syntax is return type is specified first and argument types are specified after. It is allowed to have no return type specified. Numba will infer the return type automatically and use that specification. Calling the function with argument types not provided in the list raises an error.\n\n## @njit or @jit(nopython=True)\n\nNumba **@jit** operates in two modes _nopython_ and _object_. In **nopython** mode, no interference of the Python interpreter is required and execution is very fast compared to normal mode. **object** mode is the same as calling function without **@jit**.\n\nNormally with the **@jit** decorator, Numba tries to compile in **nopython** mode. If any part of the code cannot be compiled due to the presence of code that is not supported by Numba like a heterogenous dictionary, some string methods, etc, then Numba fallbacks to **object** or normal Python mode for compilation. But still, it will improve the performance when loops are involved. If there is no code to optimize, **@jit** in object mode runs slower than the normal Python version as Numba compilation involves several function call overheads.\n\nWe can enable the strict _nopython_ mode by passing the option to **@jit** as _@jit(nopython=True)_. Numba also provides a separate decorator for this option. _@njit_ decorator is an alias for _@jit(nopython=True)_.\n\n```python\n@jit(nopython=True)\ndef f(a, b):\n\treturn a + b\n\n# or\n\n@njit\ndef f(a, b):\n\treturn a + b\n```\n\nWith **nopython** mode, if any code is present that requires object mode compilation, Numba will raise an error. The primary goal is to write functions that can be implemented in strict no-python mode.\n\n---\n\n## Numba and Numpy\n\nNumba is best for Numpy arrays and supports some [Numpy features](https://numba.readthedocs.io/en/stable/reference/numpysupported.html) in no-python mode.\n\n```python\n@njit(['int64[:](int64[:, :], int64[:, :])'])\ndef f(a, b):\n    c = np.empty(a.shape[0], dtype='int64')\n\n    for i in range(a.shape[0]):\n        c[i] = a[i].sum() * b[i].sum()\n\n    return c\n```\n\nThe function takes 2 2D Numpy int64 arrays as arguments with the return type as an array of float64. The function calculates the sum of the product of the sum of each row of _a_ and _b_.\n\n```python\nx, y, n = 1000, 1000, 1_000_000\nl, h = 0, 100\na = np.random.randint(l, h, n).reshape(x, y)\nb = np.random.randint(l, h, n).reshape(x, y)\n```\n\nIf we compare both normal Numpy calculation of the above function and Numba compiled code,\n\n```python\n%%timeit\n# normal Python calculation\nres = a.sum(axis=1) * b.sum(axis=1)\n\n'''Output:\n1.4 ms ± 7.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n'''\n```\n\n```python\n%%timeit\n# Numba compiled function\nres = f(a, b)\n\n'''Output:\n1.19 ms ± 15.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n'''\n```\n\nNumba compiled version takes **1.19 ms** which is slightly faster than normal calculation with Numpy **1.4ms**.\n\n## @vectorize decorator\n\nOne of the main reasons for Numpy speed is that most of the Numpy functions are [ufunc](https://numpy.org/doc/stable/user/basics.ufuncs.html)s (universal functions) that are vectorized and implemented inside compiled layer of Numpy and hence the speed. We might run into a situation where we cannot find any existing Numpy functions for use and write a workaround by combining Numpy functions into a single operation. This new operation is not optimized and we might lose the speed that Numpy provides due to custom loops. To solve this problem, we can make any function as **ufunc** that comes with vectorization and speed.\n\nWith [@vectorize](https://numba.readthedocs.io/en/stable/user/vectorize.html), Numba provides functionality to create custom vectorized universal functions. The universal function takes scalar values and returns a scalar value and these functions are applied over any Numpy arrays where array values are passed as single scalar values and an outside loop is automatically generated.\n\n```python\n@vectorize(['float64(int64, int64)'])\ndef v_expr(a, b):\n    return (a**2) + (a*3) + (b/2) + 10\n```\n\nHere, we created a simple universal function that takes two scalar values and returns a calculated expression.\n\n```python\nn = 1_000_000\nl, h = 0, 100\na = np.random.randint(l, h, n)\nb = np.random.randint(l, h, n)\n```\n\nThe speed comparison of the Numpy expression and vectorized ufunc function gives\n\n```python\n%%timeit\n# general way of calculating Numpy expression\nres = (a**2) + (a*3) + (b/2) + 10\n\n'''Output:\n8.66 ms ± 71.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n'''\n```\n\n```python\n%%timeit\n# calculate expression with ufunc\nres = v_expr(a, b)\n\n'''Output:\n2.17 ms ± 96.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n'''\n```\n\nthe optimized function **v_expr** takes **2.17 ms** which is 4x faster than the normal calculation with Numpy.\n\n---\n\nIn this blog, we discussed basic Numba decorators and compared Numba compiled functions with normal Python functions. Explore other features of Numba like\n\n- [automatic parallelization](https://numba.readthedocs.io/en/stable/user/parallel.html)\n- [loop parallelization with prange](https://numba.readthedocs.io/en/stable/user/parallel.html#explicit-parallel-loops)\n- [stencil kernel implementation for arrays](https://numba.readthedocs.io/en/stable/user/stencil.html)\n- [ahead-of-time compilation](https://numba.readthedocs.io/en/stable/user/pycc.html)\n\nRemember that not every function can be passed to Numba as there are some limitations like\n\n- Numba only supports a subset of Python code like classes, multi-dimensional dictionaries, etc, which are not supported yet.\n- As object mode compilation takes more time than the normal Python mode in some cases, it is better to check the speed of both normal and compiled code execution speed.\n- Support for external libraries like Pandas is not supported.\n- As Numba re-implements some Numpy APIs, there may be different behavior expected.\n\n---\n\n### References\n\n- [https://github.com/ContinuumIO/gtc2020-numba](https://github.com/ContinuumIO/gtc2020-numba)\n- [https://www.nvidia.com/en-us/glossary/data-science/numba/](https://www.nvidia.com/en-us/glossary/data-science/numba/)\n- [https://www.chrisvoncsefalvay.com/2019/03/23/jit-fast/](https://www.chrisvoncsefalvay.com/2019/03/23/jit-fast/)\n- [https://towardsdatascience.com/numba-weapon-of-mass-optimization-43cdeb76c7da](https://towardsdatascience.com/numba-weapon-of-mass-optimization-43cdeb76c7da)\n- [https://www.infoworld.com/article/3622013/speed-up-your-python-with-numba.html](https://www.infoworld.com/article/3622013/speed-up-your-python-with-numba.html)\n- [https://coderzcolumn.com/tutorials/python/numba](https://coderzcolumn.com/tutorials/python/numba)\n- [https://towardsdatascience.com/numpy-ufuncs-the-magic-behind-vectorized-functions-8cc3ba56aa2c](https://towardsdatascience.com/numpy-ufuncs-the-magic-behind-vectorized-functions-8cc3ba56aa2c)\n"},{"metadata":{"title":"Super fast Python (Part-4): Cython","description":"Convert slow Python code to run as fast as C/C++ using Cython.","imgName":"super-fast-python-cython/super-fast-python-cython.jpg","date":"Nov 18, 2022","tags":["python-performance","python"],"keywords":["cython","python-performance","python-optimize","python","fast-python","speed"],"id":"super-fast-python-cython"},"content":"\n![Super fast Python: Cython](super-fast-python-cython/super-fast-python-cython.jpg)\n\n###### Published on: **Nov 18, 2022**\n\n# Super fast Python (Part-4): Cython\n\nThis is the fourth post in the series on Python performance and Optimization. The series points out the utilization of inbuilt libraries, low-level code conversions, and other Python implementations to speed-up Python. The other posts included in this series are\n\n- (Part-1): [Why Python is slow?](https://santhalakshminarayana.github.io/blog/super-fast-python-why-python-slow)\n- (Part-2): [Good practices to write fast Python code](https://santhalakshminarayana.github.io/blog/super-fast-python-good-practices)\n- (Part-3): [Multi-processing in Python](https://santhalakshminarayana.github.io/blog/super-fast-python-multi-processing)\n- (Part-4): Use Cython to get speed as fast as C (this post)\n- (Part-5): [Use Numba to speed up Python Functions and Numeric calculations](https://santhalakshminarayana.github.io/blog/super-fast-python-numba)\n\nIn the last post, we discussed **multiprocessing** to optimize Python code by utilizing parallel computing with multiple CPU cores. Multi-processing is useful when we can split certain parts of code into parallel tasks and then execute them parallelly.\n\nImagine when we don't have any parallelizable code and it is taking huge time in Python compared to **C/C++**, how to optimize the code then? One thing we can do is to convert the Python code into a low-level programming language like C/C++ and [embed that C/C++ code in Python](https://docs.python.org/3/extending/extending.html). Understanding and writing C/C++ code is hard for someone who is not familiar with C/C++. Fortunately, we can get the performance speed as fast as C/C++ by writing the Python code in [Cython](https://cython.org/) which is a superset of Python that provides functionality to write C-Extensions for Python.\n\n## How Cython can improve Speed?\n\nIn our previous discussion on [Why Python is slow?](https://santhalakshminarayana.github.io/blog/super-fast-python-why-python-slow), we learned that the major speed issues arise in Python due to\n\n- interpretation of generated bytecode\n- dynamic types and their management\n\nWith Cython, we can take care of the above problems with both compiled code instead of interpretation and static typing instead of dynamic typing.\n\nCython translates the Python code into C extension code and compiles the C code into an object code that can be imported directly into Python. Also, we can make some changes like adding static types that improve the execution speed drastically over dynamic typing. Cython also supports the usage of C/C++ libraries and functions that are super-fast compared to Python libraries and functions.\n\n### Install Cython\n\n[Install the Cython](https://cython.readthedocs.io/en/stable/src/quickstart/install.html) with Pip as follows\n\n```bash\npip install Cython\n```\n\n\u003e To compile the C code into an object file, we need a C compiler. Ubuntu comes with **gcc** by default. For other platforms like Windows, install the C compiler if not installed previously.\n\n---\n\n## Cython usage\n\nThere are multiple ways to use Cython like building manually, using in Jupyter as an extension, or importing directly like a Python module without compilation using _pyximport_.\n\n### Build a Cython module manually\n\nWe write the Cython code in a file with the extension _.pyx_ instead of the normal Python extension _.py_.\n\nThe compiler translates '.pyx' Cython file into a '.c' C file and then compiles that C file to a sharable object file '.so' (or '.pyd' on Windows). We tell the compiler those build instructions and compilation options by writing a [setup.py](https://cython.readthedocs.io/en/stable/src/userguide/source_files_and_compilation.html#basic-setup-py) file.\n\n### Cython compilation\n\n- Translate the _.pyx_ source code to a _.c_ file with additional wrappers of Python extension code.\n- Compile the _.c_ file with a C compiler to a platform-specific shared object file _.so_ that can be imported directly into Python.\n\n```python:calculate_y_cython.pyx\ndef fx(x, a, b, c):\n    d = (a + b) * c\n    return a*x + b*(x**2) + c*(x**3) + d\n\ndef y(x, n, a, b, c):\n    af = fx(x, a, b, c)**2\n    k = abs(n//2 - x)\n    bf = fx(k, a, b, c)\n    return (af-bf)/(n-1 + 1e-12)\n\ndef calculate_y_cython(n):\n    ys = []\n    a, b, c = 2, 5, -4\n    for i in range(n):\n        ys.append(y(i, n, a, b, c))\n\n    return ys\n```\n\nThe above file _calculate_y.pyx_ contains Cython code that looks the same as Python without any optimizations. Now to compile the above file, write a _setup.py_ file as following\n\n```python:setup.py\nfrom distutils.core import setup\nfrom Cython.Build import cythonize\n\nsetup(\n    name='Calculate Expression Y',\n    ext_modules=cythonize(\n        \"calculate_y_cython.pyx\",\n        compiler_directives={\"language_level\": \"3\"},\n    )\n)\n```\n\nIn the above _setup.py_, for the **setup()**, we have passed the optional name to our Cython module for _name_ and the path to the _.pyx_ file for the _ext_module_ parameter.\n\nBuild the sharable object file '.so' using the following command in the terminal\n\n```bash\npython setup.py build_ext --inplace\nor\npython setup.py build_ext --inplace --quiet\n```\n\nThis will generate a translated '.c' C file and a compiled '.so' file.\n\nWe can import the above compiled _calculate_y_cython_ module directly into Python runtime.\n\n```python:main.py\nfrom time import perf_counter\nfrom calculate_y_cython import calculate_y_cython\n\ndef fx(x, a, b, c):\n    d = (a + b) * c\n    return a*x + b*(x**2) + c*(x**3) + d\n\ndef y(x, n, a, b, c):\n    af = fx(x, a, b, c)**2\n    k = abs(n//2 - x)\n    bf = fx(k, a, b, c)\n    return (af-bf)/(n-1 + 1e-12)\n\ndef calculate_y_py(n):\n    ys = []\n    a, b, c = 2, 5, -4\n    for i in range(n):\n        ys.append(y(i, n, a, b, c))\n\n    return ys\n\ndef main():\n    n = 100000\n    # Python implementation\n    atime = perf_counter()\n    res = calculate_y_py(n)\n    print(f'Python Time: {perf_counter()-atime:.2}')\n\n    atime = perf_counter()\n    res = calculate_y_cython(n)\n    print(f'Cython Time: {perf_counter()-atime:.2}')\n\nif __name__==\"__main__\":\n    main()\n\n\"\"\"Output:\nPython Time: 0.19\nCython Time: 0.16\n\"\"\"\n```\n\n\u003e At the time of writing, the latest Python version is Python 3.11 which is incredibly faster (30-60% in some cases) than earlier versions. So, to understand the Cython potential, I'm running the scripts in Python 3.8.10 on my old system with 8GB Intel(R) i5-8250U 1.60GHz CPU on HP Laptop 15-da0xxx.\n\nIn the above _main.py_, we have imported the _calculate_y_cython_ module at line 2. If we look at the output to check the time taken for normal Python implementation and Cython version, they are **0.19** and **0.16** seconds respectively. The time difference is very low and we didn't gain much from Cython because we haven't done any optimization steps like\n\n- static typing\n- limit calling Python's libraries\n- reducing Python's PyObject usage\n\n### Cython Annotations\n\nCython provides an easy way to check where we can optimize our Cython code by using Cython annotate feature. With the following command (_-a_ denotes annotate and _-3_ denotes **language_level** which is Python3), cython generates an HTML file that we can open in the browser to check for optimizable code. Another option is to pass the **annotate=True** parameter to _cythonize()_ function call in **ext_modules** in _setup_.\n\n```bash\ncython -a calculate_y_cython.pyx -3\n```\n\nIf we open the generated HTML file in the browser, it will look like this\n\n![Cython Annotation:=:80](super-fast-python-cython/cython-annotations.png)\n\nThe more yellow lines the more interaction with the Python interpreter. Our goal should be converting as many yellow lines as to white lines that denote pure Cython code. We discuss the optimization part in a later section.\n\n### Cython as an extension in Jupyter\n\nCython can be imported and used in Jupyter directly as an extension without the need for any additional build/compilation steps.\n\nFirst load the Cython extension using _%load_ext cython_, and then, for the cell that is to be Cythonized, use the magic command _%%cython_ at the top of that cell as shown in the following image.\n\n![Cython Jupyter:=:40](super-fast-python-cython/cython-jupyter.png)\n\nWe can show the interactive Cython annotations in Jupyter just like we have generated the HTML file above. To show annotations, pass annotate option to _%%cython -a_ magic command.\n\n### Import '.pyx' using pyximport\n\nWhile developing or debugging, for each change in the '.pyx' file, running _setup.py_ is a repetitive task and cumbersome. Instead, we can dynamically import '.pyx' to Python directly using [pyximport](https://cython.readthedocs.io/en/stable/src/userguide/source_files_and_compilation.html#pyximport) without any external build and compilation. _pyximport_ takes care of compiling and building in the background without calling _cythonize()_ internally. So, while importing the '.pyx' file, it will take some time to be imported as a regular Python module.\n\n```python\nimport pyximport\npyximport.install(language_level=3)\n\nfrom calculate_y_cython import calculate_y_cython\n```\n\nThough it is easy to work with Cython using **pyximport**, there are some [limitations with pyximport](https://cython.readthedocs.io/en/stable/src/userguide/source_files_and_compilation.html#limitations) and it is also not flexible as normal setup.\n\n\u003e It is not recommended to use **pyximport** while distributing Python packages and modules.\n\n---\n\n## Optimize Cython\n\nIn the previous section, we have seen in the Cython annotations HTML file that many areas in the code need to be optimized for more speed. There are several ways to improve speed like\n\n- define static types\n- use C-libraries and functions\n- utilize OpenMP for parallel computing\n\n### Define static types to variables\n\nSince Python is a dynamic typing language, we can define C-like data types for Python variables in Cython. To declare C variables, prefix the **cdef** keyword to the variable declaration which is the same as declaring variables in C.\n\nThe syntax for variable declaration is\n\n```python\ncdef type variable_name = initilization_value {optional}\n```\n\nThe **type** can be any of the acceptable C data types.\n\n### Define function definitions\n\nPython functions are defined using **def** and [C functions in Cython](https://cython.readthedocs.io/en/latest/src/userguide/language_basics.html#python-functions-vs-c-functions) are defined with the keyword **cdef**. The difference between **def** and **cdef** is that with the former declaration, it can be called from anywhere both local and external modules where as **cdef** functions are only module level. Also, Cython wraps the **def** function that is defined inside '.pyx' into a Python object. There is another declaration called **cpdef** which behaves as **def** when called from outside the module and behaves as **cdef** inside the module, so it is faster inside the same module function call.\n\nCython also provides support for writing function definitions just like C. We can define parameter types and return types.\n\n```python\ncdef(or cpdef) type function_name(type parameter1, ...)\n```\n\nIf no type is specified, the parameters and return values are treated as Python objects which need Python interpretation and they are slow.\n\nNow, make some changes to _calculate_y_cython.pyx_ with static type declarations and function definitions as,\n\n\u003e [Call the C functions in Cython](https://cython.readthedocs.io/en/stable/src/tutorial/external.html) inplace of Python functions that reduces the Python interaction.\n\n```python:calculate_y_cython.pyx\nctypedef long int li\nctypedef long long int lli\n\ncdef lli fx(li x, int a, int b, int c):\n    cdef int d = (a + b) * c\n    return a*x + b*(x**2) + c*(x**3) + d\n\ncdef double y(li x, li n, int a, int b, int c):\n    cdef:\n        lli af = fx(x, a, b, c)**2\n        li k = abs(n//2 - x)\n        lli bf = fx(k, a, b, c)\n    return (af-bf)/(n-1 + 1e-12)\n\ncpdef calculate_y_cython(li n):\n    ys = []\n    cdef:\n        int a = 2, b = 5, c = -4\n        li i = 0\n    for i in range(n):\n        ys.append(y(i, n, a, b, c))\n\n    return ys\n```\n\n\u003e speed can be improved further by disabling bound checking(@cython.boundscheck(False)) and negative indexing(@cython.wraparound(False)) [compiler directive instructions](https://cython.readthedocs.io/en/stable/src/userguide/source_files_and_compilation.html#compiler-directives).\n\nCheck the annotations for the Cython code, most of the yellow lines are now changed to white and only list operations are dark yellow because lists are Python objects. In a later section, we discuss optimizing lists using C arrays.\n\n![Optimized Cython Annotation:=:70](super-fast-python-cython/optimized_cython.jpg)\n\nIf we check the time for the optimized Cython code,\n\n```python\n%%timeit -n 100\nn = 100000\nres = calculate_y_cython(n)\n\n\"\"\"Output:\n2.04 ms ± 166 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\"\"\"\n```\n\nit takes **0.002** seconds which is approximately **100x** times faster than the normal Python version that takes **0.19** seconds. By adding only static type and some tweaks, we made Python 100x faster. The powers of Cython are not limited to only static typing. We can further speed up the code with Numpy.\n\n---\n\n## 1000x times faster with MemoryView and Numpy\n\nIn the Cython annotation snippet above, we see that the list operations of **ys** are still yellow as _list()_ is a python object and Cython cannot optimize Python objects interaction.\n\nTo solve this, we can convert the list type to [memoryview arrays](https://cython.readthedocs.io/en/stable/src/tutorial/array.html), and operations like _append()_ to indexing just like looping in C.\n\n```python:calculate_y_cython.pyx\ncimport cython\nimport numpy as np\ncimport numpy as np\n\nctypedef long int li\nctypedef long long int lli\n\n@cython.boundscheck(False)  # Deactivate bounds checking\n@cython.wraparound(False)   # Deactivate negative indexing\ncdef lli fx(li x, int a, int b, int c):\n    cdef int d = (a + b) * c\n    return a*x + b*(x**2) + c*(x**3) + d\n\n@cython.boundscheck(False)\n@cython.wraparound(False)\ncdef double y(li x, li n, int a, int b, int c):\n    cdef:\n        lli af = fx(x, a, b, c)**2\n        li k = abs(n//2 - x)\n        lli bf = fx(k, a, b, c)\n    return (af-bf)/(n-1 + 1e-12)\n\n@cython.boundscheck(False)\n@cython.wraparound(False)\ncpdef calculate_y_cython(li n):\n    cdef double[:] ys = np.empty(n, dtype=np.float64)\n    cdef:\n        int a = 2, b = 5, c = -4\n        li i = 0\n    for i in range(n):\n        ys[i] = y(i, n, a, b, c)\n    return ys\n```\n\nBefore building the object code, we need to change [build instructions to link Numpy](https://cython.readthedocs.io/en/stable/src/tutorial/numpy.html) as a dependency like following\n\n```python:setup.py\nfrom distutils.core import setup\nfrom distutils.extension import Extension\nfrom Cython.Build import cythonize\nimport numpy\n\next_modules = [\n    Extension(\"calculate_y_function\",\n              sources=[\"calculate_y_cython.pyx\"],\n              libraries=[\"m\"],\n              compiler_directives={\"language_level\": \"3\"},\n              )\n]\n\nsetup(name=\"calculate Y function\",\n      ext_modules=cythonize(ext_modules),\n      include_dirs=[numpy.get_include()])\n```\n\nThe parameter _include_dirs_ includes external libraries like Numpy here.\n\n```python\n%%timeit -n 1000\nn = 100000\nres = calculate_y_cython(n)\n\n\"\"\"Output:\n208 µs ± 11.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n\"\"\"\n```\n\nThe fully optimized version of Cython runs at **0.0002** seconds which is approximately **1000x** faster than the normal Python code.\n\nThe usage of Numpy and MemoryViews in Cython needs separate discussion and I will write about Classes, C-math, Numpy, MemoryViews, and OpenMP in the Advanced Cython series later.\n\n---\n\nCython is a very powerful extension that we can use to speed up Python code. Sometimes it may not be possible to convert Python objects straight away like dictionaries (use C++ maps), so it's better to use Cython for repetitive tasks like loops, general functions with simple statements (like declaration and usage only), and math operations.\n\nLearn more about Cython by referencing\n\n- [Speeding up basic object operations in Cython\n  ](http://blog.behnel.de/posts/tuning-basic-object-operations-in-cython.html)\n- [Faster Python made easier with Cython’s pure Python mode](https://www.infoworld.com/article/3648539/faster-python-made-easier-with-cythons-pure-python-mode.html)\n- [An Introduction to Just Enough Cython to be Useful](https://www.peterbaumgartner.com/blog/intro-to-just-enough-cython-to-be-useful/)\n- [Cython notes and tips](https://nicolas-hug.com/blog/cython_notes)\n- [(Github Issue) Improve cython interface with a more user-friendly compiling interface](https://github.com/cython/cython/issues/3974)\n"},{"metadata":{"title":"Super fast Python (Part-3): Multi-processing","description":"Make computations in Python faster with Multi-processing .","imgName":"super-fast-python-multi-processing/super-fast-python-multi-processing.jpg","date":"Nov 12, 2022","tags":["python-performance","python"],"keywords":["python-multiprocessing","python-performance","python-optimize","python","fast-python","speed"],"id":"super-fast-python-multi-processing"},"content":"\n![Super fast Python: Multi-processing](super-fast-python-multi-processing/super-fast-python-multi-processing.jpg)\n\n###### Published on: **Nov 12, 2022**\n\n# Super fast Python (Part-3): Multi-processing\n\nThis is the third post in the series on Python performance and Optimization. The series points out the utilization of inbuilt libraries, low-level code conversions, and other Python implementations to speed-up Python. The other posts included in this series are\n\n- (Part-1): [Why Python is slow?](https://santhalakshminarayana.github.io/blog/super-fast-python-why-python-slow)\n- (Part-2): [Good practices to write fast Python code](https://santhalakshminarayana.github.io/blog/super-fast-python-good-practices)\n- (Part-3): Multi-processing in Python (this post)\n- (Part-4): [Use Cython to get speed as fast as C](https://santhalakshminarayana.github.io/blog/super-fast-python-cython)\n- (Part-5): [Use Numba to speed up Python Functions and Numeric calculations](https://santhalakshminarayana.github.io/blog/super-fast-python-numba)\n\n## Problems with GIL\n\nIn the previous post on [why python is slow?](https://santhalakshminarayana.github.io/blog/super-fast-python-why-python-slow), we discussed that the problems with [Global Interpreter Lock (GIL) in Python](https://santhalakshminarayana.github.io/blog/super-fast-python-why-python-slow#gil-good-and-bad). [GIL](https://realpython.com/python-gil/) is part of Python's design that ensures thread-safe in Python. But, GIL only allows the interpreter to run with only one thread which makes multi-threading hard and slow in Python.\n\nThis makes multi-threading confined to use in cases like where [system releases the GIL](https://tenthousandmeters.com/blog/python-behind-the-scenes-13-the-gil-and-its-effects-on-python-multithreading/) for tasks where it waits for external processes to complete and return result/status like I/O operations, network requests, etc. To achieve better concurrency in Python, use [coroutines with asyncio](https://docs.python.org/3/library/asyncio-task.html).\n\nSo, even if the system has multiple cores and supports 1000s of threads, multi-threading is not suitable for CPU-intensive tasks. But, how to do [parallel computing](https://hpc.llnl.gov/documentation/tutorials/introduction-parallel-computing-tutorial) in Python? The answer is Multi-programming. Multi-programming creates multiple sub-processes (maximum processes can be created = no.of cores) where each process will have its interpreter with GIL and independent memory space. In processes, GIL won't be a problem because each one will have its interpreter to run the bytecode and the CPython manages multiple sub-processes. But, there are some overheads with processes like spawning/forking a process with its own memory space is slower than creating a thread and each process is separate which makes inter-process communication slow.\n\nAlso, there are external libraries like Numpy that release GIL and compute the tasks faster. [Numpy releases GIL](https://iotespresso.com/numpy-releases-gil-what-does-that-mean/) - means most of the Numpy computations are executed in C-libraries that don't require an interpreter which makes Numpy can release the GIL for that particular C-library.\n\n---\n\n[multiprocessing](https://docs.python.org/3/library/multiprocessing.html) module in Python offers a variety of APIs for achieving multiprocessing. In this blog, we discuss [mulitprocessing.Pool](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool) class that takes multiple numbers of tasks and executes them parallelly by distributing tasks among multiple cores/workers.\n\n\u003e [concurrent.futures.ProcessPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor) provides a higher level interface on top of **multiprocessing** and offers additonal functions to control the process pool.\n\n## Multiprocessing Pool\n\n### Why and what is a pool?\n\nImagine we have an 8-cores CPU and create a process by allocating each process to each core for every task and executing them parallelly. If no.of tasks is \u003c= no.of cores, then the parallel computing works smoothly. If we increase the no.of tasks to let's say 100 and create a process for each task and then execute them parallelly, the system has to manage the lifecycle of each process and ensure that processes don't hang up or reach a deadlock state. If we have numerous processes that the system can't handle we run into problems like system crash or hang.\n\nA pool is like a strategy in parallel computing to make sure that only a certain number of workers (or cores in the CPU) are allocated to run tasks given in any number. A pool is really helpful to prevent the system from the crash while serving a very huge number of processes that may overkill the CPU if not properly managed. We create a maximum of n-workers for a system with n-cores and distribute the task to every worker to run parallelly. The main difference is that in normal execution, for every task we create a separate process, but in pool processing, first, we create a pool of processes, then distribute the tasks among the processes in the pool.\n\nIf we have 100 tasks and a pool with 5 processes, distribute the 100 tasks to the process pool by running a batch of 20 tasks per process.\n\nConsider a function that takes around **1-second** to complete. And we have to call the function for 10 different input parameters.\n\nThe following example shows **fun_1()** as a function that takes a parameter **a**, sleeps for 1 second, and then returns the square of **a**.\n\n\u003e Why prefer [time.perf_counter()](https://peps.python.org/pep-0418/) over time.time()? time.time() is not monotonic - which means that the system can change the clock time with synchronization to the internet or other update. Where time.perf_counter() is system-wide and the reference point is unknown so only the results between consecutive calls are considered.\n\nNow, call the function 10 times by passing the elements in the range(0..10). If we call the function without parallel processing, the total time will be around 10 seconds because every function call takes 1 second, and for 10 function calls it will be around 10 seconds.\n\n```python\nfrom os import getpid\nfrom time import perf_counter, sleep\nfrom multiprocessing import Pool, cpu_count\n\n# function that sleeps for 1 second\ndef fun_1(a):\n    print('WORKER ID:', getpid())\n    sleep(1)\n    return a * a\n\n# call fun_1 for elements in range(0..10)\n\n# without multiprocessing\natime = perf_counter()\nprint('without multiprocessing')\n\nres = [fun_1(i) for i in range(10)]\n\nprint(f\"time taken without multiprocessing: {perf_counter()-atime}\\n\")\n\n# with multiprocessing\natime = perf_counter()\nprint('with multiprocessing')\n\nwith Pool(processes=cpu_count()) as pool:\n    res = pool.map(fun_1, range(10))\n\nprint(f\"time taken with multiprocessing: {perf_counter()-atime}\")\n```\n\n```markdown:output\nwithout multiprocessing\nWORKER ID: 32093\nWORKER ID: 32093\nWORKER ID: 32093\nWORKER ID: 32093\nWORKER ID: 32093\nWORKER ID: 32093\nWORKER ID: 32093\nWORKER ID: 32093\nWORKER ID: 32093\nWORKER ID: 32093\ntime taken without multiprocessing: 10.019481873001496\n\nwith multiprocessing\nWORKER ID: 33097\nWORKER ID: 33099\nWORKER ID: 33098\nWORKER ID: 33100\nWORKER ID: 33101\nWORKER ID: 33103\nWORKER ID: 33102\nWORKER ID: 33096\nWORKER ID: 33098\nWORKER ID: 33101\ntime taken with multiprocessing: 2.077159455002402\n```\n\nIn the above code, we applied parallel computing by passing tasks (calling **fun\\_()**) to the multiprocessing pool.\n\nThe pool class constructor takes the following arguments and returns the pool object\n\n- **processes**: no.of workers, default is all cores available in the system. We can check the available cores by calling _multiprocessing.cpu_count()_ (mine is 8)\n- **initializer**: initializer function that will be called by every worker when it starts\n- **initargs**: arguments to the initializer function above\n- **maxtasksperchild**: max. no.of tasks a worker should execute before being replaced by another worker process. This behavior releases the workers that are using system resources for a very long time\n\nWe discuss **initializer** and **initargs** in the later section on sharing data between processes.\n\nThe pool object has several methods like apply, map, starmap, imap, etc, that serves different purpose for applying parallel computing. In the above snippet, we used Pool.map() function (same as normal map()) that takes a function and an iterable and calls the function by passing each element in the iterable as an argument. The function should accept only an argument. The Pool.map() is a blocking call and it distributes the tasks among the available cores by dividing the tasks into chunks.\n\nPool.map() takes the following arguments\n\n- **func**: the function to be executed parallelly that takes one argument\n- **iterable**: an iterable whose elements are passed as arguments to the function above\n- **chunksize**: chunk size number that used to divide the tasks\n\n\u003e based on the nature of tasks, different **chunksize** numbers [schedules tasks differently](https://stackoverflow.com/questions/53751050/multiprocessing-understanding-logic-behind-chunksize)\n\n\u003e In UNIX and such systems, [multiprocessing creates processes](https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods) using a fork, and in windows, it starts processes using spawn. There are some [additional restrictions](https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods) one should be aware of when the process start method is other than fork like protecting the entry point of the program by creating processes inside _if \\_\\_name\\_\\_==\\_\\_main\\_\\__ because spawn creates new python interpreter for each process.\n\nAs we have 8 cores available and 10 tasks at hand, Pool.map() distributes the tasks among the 8 workers by dividing tasks into chunks like [2, 2, 1, 1, 1, 1, 1, 1] and pass them to workers.\n\nThe total time taken with multiprocessing is 2 seconds which is 5x times faster than without multiprocessing (10 seconds).\n\n\u003e for large iterable size use Pool.imap() with chunksize for better efficiency\n\n\u003e for multiple parameters function, use Pool.starmap() or Pool.map() with partial functions\n\n### Process vs Pool\n\nUse process, when there is a small number of tasks and each task takes a large amount of time. Use a pool, when there is a large number of tasks and each task takes a small amount of time.\n\n---\n\n## Sharing data between processes\n\n\u003e It is strongly recommended to not share data between processes\n\n### Share data with global variables (copy-on-write)\n\nProcesses created using multiprocessing run independently with their own memory space and they don't have access to the parent process's local data but they [inherit the global data of the parent](https://superfastpython.com/multiprocessing-inherit-global-variables-in-python/) (only when fork start method used).\n\nSince child processes get a snapshot of the parent process's global data, we can utilize this behavior in a way such that we can make sharable data available to child processes by making the data global.\n\nFor the following example, we will share an integer value **x** and an array of integers **a** to calculate the sum of the product of each element in **a** with an element in the list of numbers from 0..n, n = 100000. And then compute the product of **x** and the whole sum.\n\n```python\n# function to be applied for each element\ndef sum_product(e):\n    w_sum = sum([x*i for i in a])\n    return w_sum * e\n\n# pool initializer function\ndef pool_initializer(X, A):\n    global x\n    x = X\n    global a\n    a = A\n\nn = 100000\nX = 3\nA = [2, 4, 6, 8, 10, 12]\n\nwith Pool(processes=cpu_count(), initializer=pool_initializer, initargs=(X, A)) as pool:\n    res = pool.map(sum_product, range(n))\n```\n\nIn the above snippet, we initialized the pool with the _pool_initializer_ function, this function will be called for every worker after they start and any data globalized inside this initialization function is available to all child processes.\n\n\u003e Even though we don't need the initializer function here to share data as we can make data global at the top level and all child processes would get a snapshot of the global data, there are other cases where the initializer function is useful. Like reading a file and sharing data, making a database connection, etc, which might work on variable parameters that we can pass in pool **initargs**.\n\nSharing data by leveraging global variables and copy-on-write data is only useful for read-only data. If any child process changes the global variable value, they don't reflect in the parent process.\n\n```python\nfrom random import randint\n\ndef random_increment(i):\n    rand_n = randint(0, i)\n    a[rand_n] += 1\n    print(f'child {i}: {a}')\n\nn = 10\na = list(range(n))\n\nwith Pool(processes=cpu_count()) as pool:\n    pool.map(random_increment, range(n))\n\nprint(f'parent {a}')\n```\n\nIn the above program, we have initialized a global variable **a** which is a list. We applied the function _random_increment()_ for **n** tasks, and, _random_increment()_ function takes the number **i** and increments the list **a** at the index generated randomly in the range(0..i).\n\nAs **a** is global, we expect the list **a** change in the parent process too, but it is not true as each child process is created by copying a snapshot of the parent data and each child will have their own memory.\n\nLook at the output of the above program below to check that, in the parent process, the value of **a** has not changed.\n\n```markdown:Output\nchild 2: [1, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nchild 1: [1, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nchild 3: [0, 2, 2, 3, 4, 5, 6, 7, 8, 9]\nchild 8: [1, 1, 3, 3, 4, 5, 6, 7, 8, 9]\nchild 7: [1, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nchild 6: [0, 1, 2, 3, 5, 5, 6, 7, 8, 9]\nchild 0: [1, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nchild 4: [0, 2, 2, 3, 4, 5, 6, 7, 8, 9]\nchild 9: [1, 2, 2, 3, 4, 5, 6, 7, 8, 9]\nchild 5: [0, 1, 2, 4, 4, 5, 6, 7, 8, 9]\nparent [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n```\n\nThis is because, the data sharing happens here by following a mechanism called **copy-on-write** where the OS instead of copying and creating the new memory for the global data, just facilitates access to the parent processes memory as long as the child process doesn't change the data. As we have seen in the above example, when the child process tries to change the data, OS just allocates the new memory by copying the shared data and then writes the changes to it. So, the child's process data changes are not reflected in the parent's process because both memory locations are different. This mechanism is vice-versa means that, after forking, when the data in the parent's process changes, they are not reflected in the child's process.\n\n### Share data using shared memory\n\nTo avoid the problems like above, **multiprocessing** provides a mechanism to share data. To share the data between child processes, one must use sockets or shared files. But, to share simple values or arrays, multiprocessing provides a mechanism called **shared ctypes** to share data safely between processes.\n\nWith [multiprocessing.sharedctypes](https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing.sharedctypes), we can allocate the **ctypes** object from the shared memory and then the child processes can inherit them. [cytpes](https://docs.python.org/3/library/ctypes.html#fundamental-data-types) are nothing but primitive **C** compatible data types and **ctypes** module provides a wrapper around these data types to use in Python. The shared memory here is nothing but the underlying memory buffer and we will discuss later how can utilize this fact to share large Numpy arrays.\n\nMultiprocessing provides two types of shared ctypes, one for read-and-write values and arrays, and the other one for read-only values and arrays.\n\nTo manipulate data in child processes, use\n\n- **multiprocessing.Value**: to share only single value variables such as numbers or strings\n- **multiprocessing.Array**: to share an array of values of the same data type in **C**\n\nTo share read-only data, use\n\n- **multiprocessing.RawValue**: to share only single value variables\n- **multiprocessing.RawArray**: to share array of values\n\nThe main difference between raw type and normal type is that the latter provides an automatic synchronization mechanism with locks to provide process-safe data sharing.\n\n```python\nfrom multiprocessing import Array\n\ndef random_increment(i):\n    rand_n = randint(0, i)\n    a[rand_n] += 1\n\n    print(f'child {i}: {list(a)}')\n\ndef initializer_func(A):\n    global a\n    a = A\n\nn = 10\nA = Array('i', range(n))\n\nwith Pool(processes=cpu_count(), initializer=initializer_func, initargs=(A,)) as pool:\n    pool.map(random_increment, range(n))\n\nprint(f'parent: {list(A)}')\n```\n\nIn the above code, we have passed **multiprocessing.Array** instead of the normal list. **Array** is initialized with the data type ('i') and the iterable. We can also initialize the **Array** in other ways like defining the size first and assign later.\n\n```python\nA = Array('i', 10)\nA = range(10)\n```\n\nWith **Array**, we can now see that the global variable **A** is updated and reflected in the parent process. Also with **Array**, we can get automatic synchronization with locks that manage by multiprocessing so that the shared data is process-safe and only one process can access the data.\n\n```markdown:Output\nchild 3: [2, 1, 3, 4, 4, 5, 6, 7, 8, 9]\nchild 7: [2, 1, 3, 5, 5, 7, 6, 7, 8, 9]\nchild 5: [2, 1, 3, 5, 5, 5, 6, 7, 8, 9]\nchild 2: [2, 1, 3, 3, 4, 5, 6, 7, 8, 9]\nchild 0: [1, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nchild 1: [2, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nchild 4: [2, 1, 3, 5, 4, 5, 6, 7, 8, 9]\nchild 8: [2, 1, 3, 5, 5, 8, 6, 7, 8, 9]\nchild 9: [2, 1, 3, 5, 5, 8, 6, 8, 8, 9]\nchild 6: [2, 1, 3, 5, 5, 6, 6, 7, 8, 9]\nparent: [2, 1, 3, 5, 5, 8, 6, 8, 8, 9]\n```\n\n---\n\n## Share large Numpy arrays between child processes\n\nFor each task, as we pass data as arguments to the processes, the data will be pickled so that it can be sent from one process to other. If we pass large amounts of data, pickling involves a huge amount of memory and consumes more time. So, it is better to [share large data between child processes](https://research.wmz.ninja/articles/2018/03/on-sharing-large-arrays-when-using-pythons-multiprocessing.html) instead of passing the data individually.\n\nAs we have seen in the above approach that copy-on-write sharing is good for read-only and to work with data that changes inside the child process, we have to share the data using a shared array. In this section, we will discuss sharing large Numpy arrays using a shared array.\n\nWe know that shared arrays only support primitive **C** data types and mainly 1D objects. But, how to share the Numpy arrays which are multidimensional in general to the child processes as shared ctypes do not directly support Numpy arrays? We can share Numpy arrays with some work around that is explained below.\n\nThe shared ctypes objects we get are allocated from [buffer memory](https://docs.python.org/3/c-api/buffer.html) memory. A [buffer protocol](http://jakevdp.github.io/blog/2014/05/05/introduction-to-the-python-buffer-protocol/) is a framework in Python designed to provide a mechanism for Python objects to share their data among objects. Any Python objects that are implemented in C-APIs can export a set of functions called the **buffer interface**. With this buffer interface, [any object can expose its data to other objects](https://towardsdatascience.com/loading-binary-data-to-numpy-pandas-9caa03eb0672) directly without the need for copying.\n\nAs shared ctypes objects are just wrappers around the C data types whose memory was allocated from buffer memory. This is the reason why the child process's changes reflect in the parent process because the shared memory passed to child processes are implemented as [memoryview](https://docs.python.org/3/library/stdtypes.html#memoryview) objects.\n\nConsider the following example where we first create a large Numpy array of shape (1000, 1000), and using the process pool we assign the row number to every element in that row. As shared array doesn't support multidimensional and custom data types, a common approach is to pass the Numpy array to every worker and assign the row with a row number. But, passing the large arrays between processes take huge memory and latency. So, we utilize the buffer protocol here. Since shared array objects are provided with a buffer interface, we can rely on this shared memory and make it global so that there is no need to pass Numpy arrays as arguments to each process.\n\n```python\ndef assign_int(i):\n    arr = np.frombuffer(np_x.get_obj(), dtype=np.int32).reshape(np_x_shape)\n    arr[i, :] = i\n\ndef pool_initializer(X, X_shape):\n    global np_x\n    np_x = X\n    global np_x_shape\n    np_x_shape = X_shape\n\nX_shape = (1000, 1000)\ndata = np.zeros(X_shape, dtype=np.int32)\nX = Array('i', X_shape[0] * X_shape[1])\n# X as a Numpy array\nX_np = np.frombuffer(X.get_obj(), dtype=np.int32).reshape(X_shape)\n# copy data to the shared array\nnp.copyto(X_np, data)\n\nwith Pool(processes=cpu_count(), initializer=pool_initializer, initargs=(X, X_shape)) as pool:\n    pool.map(assign_int, range(X_shape[0]))\n\nprint(f'Numpy array X_np:\\n{X_np}')\n```\n\nFirst, we defined the shape of the array **X_Shape**, and then created a Numpy array with zeros of shape **X_Shape**. Second, we have created a synchronized shared array and assigned it to **X**. Third, we wrapped the shared array as Numpy using **np.frombuffer**.\n\n**np.frombuffer** takes a 1-dimensional buffer array and interprets it into a Numpy array so we can manipulate the array easily. We used _X.get_obj()_ instead **X** because **X** is a synchronized wrapper around the raw array and calling the **get_obj()** returns the raw buffer object.\n\n\u003e For raw arrays (RawArray), objects should be passed normally without calling get_obj().\n\nWhy use [**np.frombuffer**](https://www.educba.com/numpy-frombuffer/)? Since Numpy arrays take a large amount of memory space, we don't want to create a copy that again takes large memory. So, using **np.frombuffer** by utilizing the buffer protocol, we can get a Numpy array which is just a wrapper around the buffer memory.\n\nIn **initiargs**, we should pass the shared array instead Numpy wrapper. We also pass the shape of the Numpy array as we need to reshape the 1D shared array into a multidimensional array in the worker function _assign_int()_.\n\nIn the worker function _assign_int()_, again we interpreted the buffer as a Numpy array and assigned the row values at row number **i**.\n\nAt last, in the parent process, we can check the multiprocessing assignment of the Numpy array.\n\n```markdown:Output\nNumpy array X_np:\n[[  0   0   0 ...   0   0   0]\n [  1   1   1 ...   1   1   1]\n [  2   2   2 ...   2   2   2]\n ...\n [997 997 997 ... 997 997 997]\n [998 998 998 ... 998 998 998]\n [999 999 999 ... 999 999 999]]\n```\n\n---\n\nEven though **multiprocessing** looks easy and flexible to use, there are some issues one can face with **multiprocessing** if not careful,\n\n- [Why your multiprocessing Pool is stuck](https://pythonspeed.com/articles/python-multiprocessing/)\n- [Multiprocessing Best Practices](https://superfastpython.com/multiprocessing-best-practices/)\n- [Handling Hang in Python Multiprocessing](https://sefiks.com/2021/07/05/handling-hang-in-python-multiprocessing/)\n- [The Parallelism Blues: when faster code is slower](https://pythonspeed.com/articles/parallelism-slower/)\n- [Things I Wish They Told Me About Multiprocessing in Python](https://www.cloudcity.io/blog/2019/02/27/things-i-wish-they-told-me-about-multiprocessing-in-python/)\n- [Exception Handling in Methods of the Multiprocessing Pool Class in Python](https://towardsdatascience.com/exception-handling-in-methods-of-the-multiprocessing-pool-class-in-python-7fbb73746c26)\n\nIn this blog, we discussed Python's **multiprocessing** module with the Pool function. Python released other modules for simple concurrent processing and better data sharing,\n\n- [Managers](https://docs.python.org/3/library/multiprocessing.html#managers)\n- [Shared Memroy](https://docs.python.org/3/library/multiprocessing.shared_memory.html)\n- [Concurrent Futures](https://docs.python.org/3/library/concurrent.futures.html)\n\nOn top of internal libraries, there are multiple good external libraries available for parallel and concurrent processing,\n\n- [Joblib](https://joblib.readthedocs.io/en/latest/parallel.html)\n- [IPyParallel](https://ipyparallel.readthedocs.io/en/latest/tutorial/intro.html)\n- [Ray](https://docs.ray.io/en/master/index.html)\n- [Dask](https://www.dask.org/)\n\nFor comparison among internal and external libraries,\n\n- [Parallelizing Python Code](https://www.anyscale.com/blog/parallelizing-python-code)\n- [Multiprocessing In Python: Core vs libraries](https://cosmiccoding.com.au/tutorials/multiprocessing)\n- [Sharing big NumPy arrays across python processes](https://luis-sena.medium.com/sharing-big-numpy-arrays-across-python-processes-abf0dc2a0ab2)\n"},{"metadata":{"title":"Super fast Python (Part-2): Good Practices","description":"Write Python programs by following good practices to run code incredibly faster.","imgName":"super-fast-python-good-practices/super-fast-python-good-practices.jpg","date":"Nov 9, 2022","tags":["python-performance","python"],"keywords":["python-performance","python-optimize","python","fast-python","speed"],"id":"super-fast-python-good-practices"},"content":"\n![Super fast Python: Good Practices](super-fast-python-good-practices/super-fast-python-good-practices.jpg)\n\n###### Published on: **Nov 9, 2022**\n\n# Super fast Python (Part-2): Good practices\n\nIn the earlier post on [why Python is slow?](https://santhalakshminarayana.github.io/blog/super-fast-python-why-python-slow), we discussed slowness is in Python due to its internal design of some essential components like GIL, dynamic typing, and interpretation.\n\nIn this blog, we discuss some good practices to speed up Python incredibly faster.\n\nThis is the second post in the series on Python performance and Optimization. The series points out the utilization of inbuilt libraries, low-level code conversions, and other Python implementations to speed-up Python. The other posts included in this series are\n\n- (Part-1): [Why Python is slow?](https://santhalakshminarayana.github.io/blog/super-fast-python-why-python-slow)\n- (Part-2): Good practices to write fast Python code (this post)\n- (Part-3): [Multi-processing in Python](https://santhalakshminarayana.github.io/blog/super-fast-python-multi-processing)\n- (Part-4): [Use Cython to get speed as fast as C](https://santhalakshminarayana.github.io/blog/super-fast-python-cython)\n- (Part-5): [Use Numba to speed up Python Functions and Numeric calculations](https://santhalakshminarayana.github.io/blog/super-fast-python-numba)\n\nThe following section describes the various good practices one can use to make Python super speed (up to 30% or more) without any external support like PyPy, Cython, Numpy, etc.\n\n## Python good practices for super fast code\n\n### Use built-in data structures and libraries\n\nAs Python data types are implemented directly in **C**, using the built-in types like list, map, and trees, compared to custom types we define, really helps the program to run faster.\n\nAlso, use built-in libraries for common algorithms like counting the duplicates, summing all list elements, finding the maximum element, etc, because these are all already written in **C** and compiled which makes these functions run faster than custom functions we write.\n\n```python\nfrom random import randint\n\nrand_nums = [randint(1, 100) for _ in range(100000)]\n```\n\nCreate 100000 random numbers between 1 and 100.\n\n```python\n%%timeit\ncc = 0\nfor i in range(len(rand_nums)):\n    cc += rand_nums[i]\n```\n\n```python:output\n6.02 ms ± 94 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n```\n\nManually summing up the all numbers over running a loop takes approximately 6ms.\n\n```python\n%%timeit\ncc = sum(rand_nums)\n```\n\n```python:output\n332 µs ± 15 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n```\n\nUsing built-in _sum()_ takes only 0.3ms approximately.\n\n### Local Variables vs Global Variables\n\nWhen we call a function (a routine), the system pauses the code execution at the call site in the current routine (say **main()**) where the call has been made and places the called function at the top of the call stack. Imagine if we have defined numerous global variables and made multiple function calls. The system has to make sure that all these global variables should be available for any routine placed in the call stack at all times. The system has to provide a lookup mechanism for both local and global variables for each routine. And with global variables, this lookup mechanism may take some time than local variables.\n\n### Import the sub-modules and functions directly\n\nWhen importing any module to use its sub-modules, classes, or functions, import them directly instead of importing just the module. When accessing objects using _._, it triggers dictionary lookup using \\_\\_\\_getattribute\\_\\__. If we call the object multiple times using _.\\_, that may increase the program time.\n\n```python\n# instead of this\nimport abc\ndef_obj = abc.Def()\n\n# do this\nfrom abc import Def\ndef_obj = Def()\n```\n\n### Limit the usage of '.'\n\nSpeaking of the lookup time with the module's object references, the same can be applied to the referencing of properties and functions of an object(both custom and in-built).\n\n```python\n%%timeit\nll = []\nfor i in range(len(rand_nums)):\n    ll.append(rand_nums[i])\n```\n\n```python:output\n6.67 ms ± 84.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n```\n\nAdding the list elements by appending with _list.append()_ takes more time than following code because the function is assigned to a variable (functions are first-class citizens in Python) and used inside the loop. This simple practice avoids referencing the functions with **'.'** too often and finally limits the need for dictionary lookup.\n\n```python\n%%timeit\nll = []\nll_append = ll.append\nfor i in range(len(rand_nums)):\n    ll_append(rand_nums[i])\n```\n\n```python:output\n5.45 ms ± 116 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n```\n\n### Avoid writing functions unnecessary\n\nIt's good to have code separability by using functions for each independent task. But, as functions in Python are relatively more expensive than C/C++ due to boxing and unboxing dynamic variables and other factors, limit writing functions for unnecessary cases like one-liners.\n\n### Don't wrap lambdas around functions\n\nOveruse or misuse of lambdas is not a good practice. It's common to wrap functions inside lambdas which do the same thing without wrapping.\n\nConsider the following two functions for sorting a list based on absolute values.\n\n```python\ndef fun_sort_with_lambda(l):\n    return sorted(l, key=lambda x: abs(x))\n\ndef fun_sort_without_lambda(l):\n    return sorted(l, key=abs)\n```\n\nIf we look at the CPython bytecode for the above functions with lambda expression passed as a key and with _abs_ function object as a key,\n\n```python\n\u003e\u003e\u003e from dis import dis\n\u003e\u003e\u003e dis(fun_sort_with_lambda)\n  2           0 LOAD_GLOBAL              0 (sorted)\n              2 LOAD_FAST                0 (l)\n              4 LOAD_CONST               1 (\u003ccode object \u003clambda\u003e at 0x7fc51b3a19d0, file \"\u003cipython-input-62-c4147c242c71\u003e\", line 2\u003e)\n              6 LOAD_CONST               2 ('fun_sort_with_lambda.\u003clocals\u003e.\u003clambda\u003e')\n              8 MAKE_FUNCTION            0\n             10 LOAD_CONST               3 (('key',))\n             12 CALL_FUNCTION_KW         2\n             14 RETURN_VALUE\n\nDisassembly of \u003ccode object \u003clambda\u003e at 0x7fc51b3a19d0, file \"\u003cipython-input-62-c4147c242c71\u003e\", line 2\u003e:\n  2           0 LOAD_GLOBAL              0 (abs)\n              2 LOAD_FAST                0 (x)\n              4 CALL_FUNCTION            1\n              6 RETURN_VALUE\n\n\u003e\u003e\u003e dis(fun_sort_without_lambda)\n  2           0 LOAD_GLOBAL              0 (sorted)\n              2 LOAD_FAST                0 (l)\n              4 LOAD_GLOBAL              1 (abs)\n              6 LOAD_CONST               1 (('key',))\n              8 CALL_FUNCTION_KW         2\n             10 RETURN_VALUE\n```\n\nfor the function _fun_sort_with_lambda_, there is an additional function has been generated for lambda. We can avoid this function generation without using lambda as we can see in function _fun_sort_without_lambda_.\n\n### List comprehension is fast\n\nWhen operating over lists like data structures, list comprehension is faster than traditional methods like looping, functional programming, etc.\n\n```python\n%%timeit\nrand_nums = []\nfor _ in range(1000):\n    rand_nums.append(randint(1, 100))\n```\n\n```output\n603 µs ± 15.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n```\n\nThe list comprehension version of the above code snippet runs faster.\n\n```python:output\n%%timeit\nrand_nums = [randint(1, 100) for _ in range(1000)]\n```\n\n```python:output\n565 µs ± 11 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n```\n\n---\n\nThe optimization practices are not limited to the above approaches. We can check how much time it is taking for each line by using libraries like [CProfile](https://docs.python.org/3/library/profile.html) and making changes to run faster. In the next blog, we discuss how to improve Python computing efficiency using multiprocessing.\n\n---\n\n### References\n\n- [Python Speed](https://wiki.python.org/moin/PythonSpeed)\n- [Python Optimization](https://aglowiditsolutions.com/blog/python-optimization/)\n- [Making Python Programs Blazingly Fast](https://martinheinz.dev/blog/13)\n"},{"metadata":{"title":"Super fast Python (Part-1): Why Python is Slow?","description":"Why Python is slow compared to C/C++ or Java? And where can we improve our code to run fast?","imgName":"super-fast-python-why-python-slow/super-fast-python-why-python-slow.jpg","date":"Nov 7, 2022","tags":["python-performance","python"],"keywords":["python-performance","python-optimize","python","fast-python","speed","slow-python"],"id":"super-fast-python-why-python-slow"},"content":"\n![Super fast Python: Why Python is Slow?](super-fast-python-why-python-slow/super-fast-python-why-python-slow.jpg)\n\n###### Published on: **Nov 7, 2022**\n\n# Super fast Python (Part-1): Why Python is Slow?\n\nPython is an interpreted, high-level, and dynamically typed programming language. Developers prefer Python because of its easy-to-learn, flexibility, fast development, easy-to-code, readability, and many other development choices. From nowhere in the 2000s to the most used programming language right now, Python has come a long way with help of a strong community. But, there has been always a discussion on Python's choice for computation-intensive tasks like Machine Learning as Python is generally slow compared to widely used languages like C/C++ and Java. Even with the development of computation-efficient libraries and packages like Numpy, still Python is slow for general usage.\n\nPython's core development team has been working to make Python as fast as C/C++. They set a goal to make each Python release significantly faster than the earlier release. The current [Python 3.11 is up to 10-60 percent faster than Python 3.10](https://docs.python.org/3.11/whatsnew/3.11.html#faster-cpython). Let's hope we will reach a state where Python is at least at the same speed level as Java if not C++.\n\nOne common practice to tackle the speed issues in production is to upgrade the hardware resources or upscale the cloud infrastructure that increases the project budget. As Python's core development team is trying to improve Python, it's up to us to leverage the core libraries and code practices to make code faster at the developer's end and eventually use fewer resources and budget.\n\nThis is the first post in the series on Python performance and Optimization. The series points out the utilization of inbuilt libraries, low-level code conversions, and other Python implementations to speed-up Python. The other posts included in this series are\n\n- (Part-1): Why Python is slow? (this post)\n- (Part-2): [Good practices to write fast Python code](https://santhalakshminarayana.github.io/blog/super-fast-python-good-practices)\n- (Part-3): [Multi-processing in Python](https://santhalakshminarayana.github.io/blog/super-fast-python-multi-processing)\n- (Part-4): [Use Cython to get speed as fast as C](https://santhalakshminarayana.github.io/blog/super-fast-python-cython)\n- (Part-5): [Use Numba to speed up Python Functions and Numeric calculations](https://santhalakshminarayana.github.io/blog/super-fast-python-numba)\n\n---\n\n## Why Python is slow?\n\nBefore looking into how we can optimize the Python code, we should look at first why Python is slow.\n\nSome of the reasons for Python's slowness is due to its design of core details like how code executes, type-inference, and memory management.\n\n### Python implementation by interpretation\n\nPython is a programming language that talks about syntax and rules to write programs. Executing the code written is done by the [programming language implementation](https://en.wikipedia.org/wiki/Programming_language_implementation). This can be of two categories - Compilation and Interpretation. **CPython** is a Python implementation written in **C** that applies an interpretation approach to execute the python code written. CPython is the default runtime and reference implementation of Python and there are other runtimes like PyPy, Cython, Jython, etc., that take different execution approaches for different use cases. CPython is both an interpreter (widely represented) and a compiler as it complies the Python code to Python bytecode and then interprets it for the specific platform using Python Virtual Machine (PVM).\n\nCPython uses Global Interpreter Lock (GIL) on each CPython interpreter process. This means, within a single process, only a single thread processes the Python bytecode. We will see later why this behavior is both good and bad.\n\nAs interpretation is usually slow compared to compilation it is understood that Python is slow, but maybe up to 2x-3x times slower than C/C++. But it is not true, Java, which also interprets the code, is still many times faster than Python and this asks questions on what are other factors for slowness in Python.\n\nChanging the runtime of Python from interpretation to compilation like Cython and embedded C-code, or JIT with PyPy, we can improve the speed of Python many times. We will talk about Cython, and how to use it with Python, in a later article.\n\n### Comes the dynamically typing comes the problem\n\nOne of the beautiful features of Python is dynamic typing and many people like the way it is. But with dynamic typing, there is an additional burden on the interpreter to keep track of the type of variables that makes less scope for optimization. As Java is statically typed, the interpreter can optimize the bytecode generation and can interpret it faster than Python.\n\n```python\na = 1 # a as int\nb = a * 2 # b as int\n\na = 'python ' # a as string\n# c as a string with operations\n# of string and int\nc = a * b\n```\n\nIn the above snippet, the interpreter has to keep track of the _type_ of **a** from top to bottom. If not dynamic typing is supported in Python, we have to declare types for every variable and the operation **c = a \\* b** is not possible then. So, with flexible support from Python, there is some overhead with the interpreter too.\n\n### That is an object, this too\n\nIn Python, everything is an object. Even functions too. If one can remember how objects are referenced in C++ (yes, the pointers), apply the same concept in Python for everything including built-in primitive (not exactly) types which are objects too and are specially taken care of.\n\nAssigning memory to the objects is done by creating actual memory to hold an object and a reference is given to the variables that point to the real object in the memory. Every time the variable changes its value, instead of changing the value in the memory, a new object is created and the variable is given the new location of the object that holds the new value. As CPython is implemented in C, objects created in Python are called PyObjects which are struct in C that refers to all Python objects.\n\n```python\n\u003e\u003e\u003e a = 10\n\u003e\u003e\u003e print(id(a))\n9801536\n\n\u003e\u003e\u003e a = 12\n\u003e\u003e\u003e print(id(a))\n9801600\n```\n\nWe can see the address of **a** changes every time we change/assign the value because of how Python manages variables and their values. In C/C++, for variables, the addressing is done by creating an actual location for the variable instead of the value, and if the value changes, it just overwrites the previous value.\n\nPython has to create new objects, keep their references, delete unused objects, and repeat the cycle. This continuous cycle of object creation and deletion whenever variable values changes make runtime slow.\n\nThe major overhead that needs to address is Python's way of handling collections like Lists. In _C_, we create the arrays with fixed memory and the address of that array is fixed, and continuous memory allocation happens for all the elements starting from the fixed memory point. But in Python, for each value, an [object is created](http://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/) anywhere in the memory (not continuous), and the list holds only references to these objects. Working on these arbitrary memory locations make things complex and eventually takes more time.\n\n```python\n\u003e\u003e\u003e l = [1, 2, 3, 4]\n\u003e\u003e\u003e for i in l:\n        print(id(i))\n\nOUTPUT:\n-------\n\n9801248\n9801280\n9801312\n9801344\n```\n\nIn the above snippet, we can observe that the addresses for each element in the list are not the same or continuous.\n\nOne of the reasons why Numpy is faster is because it creates fixed memory of the array elements. The below snippet prints out the memory address of each element in the array and they are the same, meaning Numpy create an array with continuous memory address and they are easy to operate.\n\n```python\n\u003e\u003e\u003e a = np.array([1, 2, 3, 4])\n\u003e\u003e\u003e for i in a:\n        print(i.__array_interface__['data'][0])\n\nOUTPUT:\n-------\n\n37936064\n37936064\n37936064\n37936064\n```\n\n### GIL, good and bad:=:gil-good-and-bad\n\nIn **C**, there is no inbuilt support for garbage collection. One has to manually de-allocate/free the memory. Unlike C, Python does garbage collection by using a mechanism called reference count. For every PyObject, Python keeps track of a count of how many references are pointing to the current object. If no reference is pointing to the current object, then, Python frees the memory by deleting the unused object.\n\nEarlier we talked about GIL that, in the current process, GIL locks the process to have only a single thread to run the Python interpreter. This does not mean we cannot use more threads, but it is hard and takes more time compared to multi-threading in other languages because CPython creates a separate environment for each thread. This limits multi-threading in Python to use only in situations where threads wait for external processes to complete like I/O operations, network requests, etc.\n\nWhen multiple threads access the same resource, it is hard to keep track of the reference count of objects. GIL is required in CPython to ensure thread-safe as it allows only a single thread to process. As Python is a general-purpose language, in most cases there is no need to implement multi-threading, and there is no problem with GIL.\n\nBut when there is a need to optimize CPU-intensive tasks, one must use multi-processing to utilize all cores and reduce the overall speed. We will talk about different use cases for multi-threading and multi-processing in the later part of this series.\n\n---\n\nNow, we have learned that there are a lot of things that contribute to the slowness of Python compared to other languages. We cannot change the design of Python but we can change how we write the code by using good libraries, code snippets, and changing implementations.\n\nIn the upcoming posts, we discuss how we can use both internal and external libraries like _multiprocessing_, _Cython_, and _Numba_ to make Python blazingly faster.\n"},{"metadata":{"title":"Image Enhancement using Retinex Algorithms","description":"Enhance low-light images using Retinex algorithms with Fast Fourier Transform in Python.","imgName":"retinex-image-enhancement/retinex-image-enhancement.jpg","date":"Mar 23, 2022","tags":["ai","image-processing","color-science","opencv","python"],"keywords":["retinex","image-enhance","retinex-image-enhance","image-processing","python","color-correct","color-enhance","color-balance"],"id":"retinex-image-enhancement"},"content":"\n![Image Enhancement using Retinex Algorithms](retinex-image-enhancement/retinex-image-enhancement.jpg)\n\n###### Published on: **Mar 23, 2022**\n\n# Image Enhancement using Retinex Algorithms\n\nIn the previous blog [Retinex theory of Color Vision](https://santhalakshminarayana.github.io/blog/retinex-theory-of-color-vision), we discussed the theory behind the Retinex model and other studies related to the human visual system of color constancy explained by the Retinex. Even though Retinex failed to accurately define the human color constancy, over the years the Retinex has been modified and used in many image processing applications mainly image color/contrast enhancement, dynamic image compression, shadow removal, and color balancing. In this blog, we apply the Retinex model and its modification algorithms to enhance the low-light color images.\n\n### Types of Retinex\n\nBased on the approach of computing the lightness of an image, Retinex algorithms are broadly categorized into four types as shown in the following image.\n\n![Retinex types:=:80](retinex-image-enhancement/retinex-types.jpg)\n\nAmong the above four types, **Center Surround Retinex** algorithms are widely used in image processing for image enhancement. In this blog, we discuss applying the following four algorithms from the center-surround category\n\n- Single-Scale Retinex\n- Multi-Scale Retinex\n- Multi-Scale Retinex with Color Restoration\n- Multi-Scale Retinex with Color Preservation\n\n## Retinex model and Lightness computation\n\nIn the previous blog about [Retinex theory](https://santhalakshminarayana.github.io/blog/retinex-theory-of-color-vision), we derived an equation that, the color of an image/scene that we perceive is equal to the illumination on the scene product the reflectance of the scene. When we formulate the previous statement in equation form, that gives\n\n$$\nRetinex(I)=Reflectance(r)*Illumination(S)\n$$\n\nwhere **I** is the retinex image, **r** is the reflectance of the surface, and **S** is the illumination on the surface. And the lightness of an image, which is computed in our visual system (Retina + Cortex) is computed by reflectance and illumination. If we assume illumination is uniform or smooth, then lightness depends only on reflectance value at a given position.\n\nWith this assumption, Land and McCann tried to compute the relative reflectance of an image to estimate the relative lightness of the scene. First, they proposed an algorithm that involves random paths and relative ratios to compute lightness. But that algorithm accuracy highly depends on the number of paths taken, and, it takes so much time to compute the number of paths increases. So, Land published another method to calculate the lightness of a pixel in which the lightness of a pixel is the ratio between the value of a pixel and the average of the surrounding pixels considering the density of these surrounding pixels to have density proportional to the inverse of the square distance.\n\n$$\nL_{(x,y)}=\\frac{I_{(x,y)}}{F_{(x,y)}*I_{(x,y)}} \\\\\nx\\in\\{0...M-1\\}, \\hspace2ex y\\in\\{0...N-1\\} \\\\\nM=no.of \\hspace1ex rows \\hspace2ex N=no.of \\hspace1ex columns\n$$\n\nwhere $L_{(x,y)}$ is lightness of image at pixel position $(x,y)$, and $F_{(x,y)}*I_{(x,y)}$ is the average of the surrounding pixels at $(x,y)$ given by the center surround function $F_{(x,y)}$.\n\nAs the retinex image is equal to the reflectance of the scene under no effect of illumination, the retinex image (**R**) is approximately equal to the relative lightness. That gives\n\n$$\nR_{(x,y)} \\approx \\log(I_{(x,y)}) - \\log(F_{(x,y)}*I_{(x,y)})\n$$\n\n---\n\n## Single Scale Retinex\n\n**Single-scale retinex (SSR)** is defined as the difference between the image at a given pixel (x,y) and the center-surround average of that pixel (x,y).\n\nThe calculation of the above surrounding pixels average is equal to the inverse square spatial function for a given pixel. And we can use any high pass filter to calculate the surrounding average (Ex. Gaussian distribution) that satisfies the above conditions.\n\nIf we consider Gaussian function ($G_{\\sigma}$) as center-surround function, then retinex image for each i-th channel in an image is,\n\n$$\nSSR_i{(x,y)} = \\log(I_i{(x,y)}) - \\log(G_{\\sigma}*I_i){(x,y)})\n$$\n\nThat is the Single-scale retinex of an image is estimated by taking the logarithm difference of image and point-surround filter of the image at position (x,y). The operation $(G_{\\sigma}*I_i)(x,y)$ is nothing but gaussian blur of an image with given scale ($\\sigma$). The implementation in python is pretty straight-forward as\n\n```python\nimport numpy as np\nimport cv2\n\ndef get_ksize(sigma):\n    # opencv calculates ksize from sigma as\n    # sigma = 0.3*((ksize-1)*0.5 - 1) + 0.8\n    # then ksize from sigma is\n    # ksize = ((sigma - 0.8)/0.15) + 2.0\n\n    return int(((sigma - 0.8)/0.15) + 2.0)\n\ndef get_gaussian_blur(img, ksize=0, sigma=5):\n    # if ksize == 0, then compute ksize from sigma\n    if ksize == 0:\n        ksize = get_ksize(sigma)\n\n    # Gaussian 2D-kernel can be seperable into 2-orthogonal vectors\n    # then compute full kernel by taking outer product or simply mul(V, V.T)\n    sep_k = cv2.getGaussianKernel(ksize, sigma)\n\n    # if ksize \u003e= 11, then convolution is computed by applying fourier transform\n    return cv2.filter2D(img, -1, np.outer(sep_k, sep_k))\n\ndef ssr(img, sigma):\n\t# Single-scale retinex of an image\n    # SSR(x, y) = log(I(x, y)) - log(I(x, y)*F(x, y))\n    # F = surrounding function, here Gaussian\n\n    return np.log10(img) - np.log10(get_gaussian_blur(img, ksize=0, sigma=sigma) + 1.0)\n```\n\nThe function _ssr()_ takes arguments of an image, for which retinex has to be estimated, and sigma value for Gaussian distribution. It returns the single-scale retinex image by subtracting the image and gaussian blur of an image.\n\n_get_gaussian_blur()_ gives Gaussian blur of an image. In this function, we are not using normal _cv2.GaussianBlur()_ method but calculating Gaussian blur using _cv2.filter2D()_ using linear seperability of Gaussian distribution kernels. The reason is, _cv2.GaussianBlur()_ is too slow for large kernels. But _cv2.filter2D()_ applies filter to an image using [fast-Fourier transform](https://en.wikipedia.org/wiki/Fast_Fourier_transform) if _kernel size (ksize)_ \u003e 11. **fast Fourier transform** computes the convolution in a very few milliseconds than the traditional convolution that takes days or even years to complete.\n\n\u003e As [OpenCV implementation of fast-Fourier transform](https://docs.opencv.org/4.5.5/de/dbc/tutorial_py_fourier_transform.html) is faster than NumPy implementation, we proceed with OpenCV default Fourier transform implementations instead of NumPy or SciPy.\n\n![Single scale retinex:=:100](retinex-image-enhancement/single-scale-retinex.jpg)\n\nIn the above image, for SSR with $\\sigma$=15, the output image has some corner areas enhanced but the color is not preserved and it all looks very dark and gray. In **SSR(80)** image, there has been some improvement in contrast enhancement than **SSR(15)**, but the corner regions became dark and the overall picture looks gloomy. And finally, in **SSR(250)** image, a significant contrast improvement is done compared to both **SSR(15)** and **SSR(80)**, but the corner regions became darker than the original image, and only the center region has been enhanced.\n\nIt is great that with simple subtraction of image and gaussian image we have enhanced parts of an image to look better but still not a good solution for overall performance. And results are different for different scale values and it is hard to select any single scale value that gives the best results as enhancement purely depends on the nature of the image.\n\n---\n\n## Multi Scale Retinex\n\nAs the choice of $\\sigma$ varies for different images for good results and different scale values enhance different parts of an image, we can combine **SSR** of different scales and give weightage for each scale and take a summation of all weighted-SSR images. This method is called **Multi-scale retinex (MSR)** of an image and it is defined as the weighted average of **n** single-scale retinex images for different $\\sigma$ values.\n\n$$\nMSR_i{(x,y)} = \\sum_{n=1}^Nw_{n}SSR_i{(x,y)}\n$$\n\nAs the output $MSR_i{(x,y)}$ image might contain negative real values and the range of values is not suitable for image representaion i.e not in range **[0-255]**, normalize the **MSR** output for range **[0-255]** given by the following equation\n\n$$\nMSR_i{(x,y)} = 255\\frac{MSR_i{(x,y)} - \\min(MSR_i)}{\\max(MSR_i) - \\min(MSR_i)}\n$$\n\nwhere for channel $i$, $MSR_i{(x,y)}$ is the pixel value at position $(x,y)$, $\\min(MSR_i)$ is minimum value of the channel, and $\\max(MSR_i)$ is maximum value of the channel\n\nSo, Multi-scale retinex is calculated by taking single-scale retinex for different scales and it is computed by the following function\n\n```python\ndef msr(img, sigma_scales=[15, 80, 250]):\n    # Multi-scale retinex of an image\n    # MSR(x,y) = sum(weight[i]*SSR(x,y, scale[i])), i = {1..n} scales\n\n    msr = np.zeros(img.shape)\n    # for each sigma scale compute SSR\n    for sigma in sigma_scales:\n        msr += ssr(img, sigma)\n\n    # divide MSR by weights of each scale\n    # here we use equal weights\n    msr = msr / len(sigma_scales)\n\n    # computed MSR could be in range [-k, +l], k and l could be any real value\n    # so normalize the MSR image values in range [0, 255]\n    msr = cv2.normalize(msr, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8UC3)\n\n    return msr\n```\n\nWe give default sigma scale values as **[15, 80, 250]** with equal weights which preserve high, middle, and low frequencies of an image respectively. _msr()_ computes the **MSR** of an image by calculating **SSR** for each $\\sigma$ scale. In the end, normalization of the **MSR** output is done because the computed values could be negative real values and not in a suitable range **[0-255]** for image representation.\n\n![Multi scale retinex:=:100](retinex-image-enhancement/multi-scale-retinex.jpg)\n\nThe **MSR** image for the above input image with scales **[15, 80, 250]** has many areas enhanced especially in the corner regions which are darker in the input image given by **SSR(15)**. And the retinex output has contrast separation comparatively better than the input image added by **SSR(80)** and **SSR(250)**, but the output image has an overall gray appeal.\n\nThe output **MSR** image is looking gray because retinex assumes the scene as in [Gray world](https://en.wikipedia.org/wiki/Color_normalization#Grey_world) where the average of all colors in the scene is close to gray color. If this assumption is failed in the input image, that is if there are colors that dominate the scene or some colors may not present, whatever the reason could be if the overall color average is not close to gray, then taking the average of multiple single-scale retinex images with equal weights gives output image with colors less saturated and close to gray color. So, we have to modify the Multi-scale retinex to preserve the color of the image.\n\n---\n\n## Multi Scale Retinex with Color Restoration\n\nAs the **MSR** of an image looks colorless, the Multi-scale retinex output is multiplied with **Color-restoration function (CRF)** of chromaticity to restore the original colors of the input image approximately. And this method of calculating the Color-restoration function and applying it to Multi-scale retinex output is called **Multi-scale retinex with Color Restoration (MSRCR)**.\n\n$$\nMSRCR_i{(x,y)} = MSR_i{(x,y)}*CRF_i{(x,y)}\n$$\n\nwhere $CRF_i{(x,y)}$ is color restoration vlaue for pixel (x,y) at i-th channel. The color restoration function is defined as\n\n$$\nCRF_i{(x,y)} = \\beta[\\log(\\alpha*I'_i{(x,y)}]\n$$\n\nwhere for i-th channel, at position $(x,y)$, **CRF** depends on the ratio composition of the pixel at (x,y) for that channel value to the sum of all channel values which is equal to calculating chromaticity coordinates. Chromaticity coordinates are calculated as\n\n$$\nI'_i{(x,y)} = \\frac{I_i{(x,y)}}{\\sum_{c=0}^{k-1} I_c{(x,y)}}\n$$\n\nwhere $k$ equals to no. of image channels, $\\alpha$ is to control non-linearity, and $\\beta$ is to control total gain. The above equation can be written as\n\n$$\nCRF_i{(x,y)} = \\beta[\\log(\\alpha*I_i{(x,y)}) - \\log(\\sum_{c=0}^{k-1} I_c{(x,y)})] \\\\\n$$\n\nTo achieve better contrast results, the **MSRCR** equation is modified to include **gain (G)** and **offset (b)** values.\n\n$$\nMSRCR_i{(x,y)} = G[MSR_i{(x,y)}*CRF_i{(x,y)} - b]\n$$\n\nThe gain and offset values are introduced to transform the contrast range of the **MSRCR** image to distribute uniformly and to attenuate tails forming in the histogram graph. But these values are not general and don't work for every image. And to stretch the contrast, the final **MSRCR** image is [contrast stretched](https://en.wikipedia.org/wiki/Histogram_equalization) using [Simplest Color Balance](http://www.ipol.im/pub/art/2011/llmps-scb/) algorithm with a clipping percentage of 1% at both ends.\n\nWe use default values suggested by different publishers for the above variables as $\\alpha=125, \\beta=46, G=192, b=-30$\n\n```python\ndef color_balance(img, low_per, high_per):\n    '''Contrast stretch img by histogram equilization with black and white cap'''\n\n    tot_pix = img.shape[1] * img.shape[0]\n    # no.of pixels to black-out and white-out\n    low_count = tot_pix * low_per / 100\n    high_count = tot_pix * (100 - high_per) / 100\n\n    # channels of image\n    ch_list = []\n    if len(img.shape) == 2:\n        ch_list = [img]\n    else:\n        ch_list = cv2.split(img)\n\n    cs_img = []\n    # for each channel, apply contrast-stretch\n    for i in range(len(ch_list)):\n        ch = ch_list[i]\n        # cummulative histogram sum of channel\n        cum_hist_sum = np.cumsum(cv2.calcHist([ch], [0], None, [256], (0, 256)))\n\n        # find indices for blacking and whiting out pixels\n        li, hi = np.searchsorted(cum_hist_sum, (low_count, high_count))\n        if (li == hi):\n            cs_img.append(ch)\n            continue\n        # lut with min-max normalization for [0-255] bins\n        lut = np.array([0 if i \u003c li\n                        else (255 if i \u003e hi else round((i - li) / (hi - li) * 255))\n                        for i in np.arange(0, 256)], dtype = 'uint8')\n        # constrast-stretch channel\n        cs_ch = cv2.LUT(ch, lut)\n        cs_img.append(cs_ch)\n\n    if len(cs_img) == 1:\n        return np.squeeze(cs_img)\n    elif len(cs_img) \u003e 1:\n        return cv2.merge(cs_img)\n    return None\n\ndef msrcr(img, sigma_scales=[15, 80, 250], alpha=125, beta=46, G=192, b=-30, low_per=1, high_per=1):\n    # Multi-scale retinex with Color Restoration\n    # MSRCR(x,y) = G * [MSR(x,y)*CRF(x,y) - b], G=gain and b=offset\n    # CRF(x,y) = beta*[log(alpha*I(x,y) - log(I'(x,y))]\n    # I'(x,y) = sum(Ic(x,y)), c={0...k-1}, k=no.of channels\n\n    img = img.astype(np.float64) + 1.0\n    # Multi-scale retinex and don't normalize the output\n    msr_img = msr(img, sigma_scales, apply_normalization=False)\n    # Color-restoration function\n    crf = beta * (np.log10(alpha * img) - np.log10(np.sum(img, axis=2, keepdims=True)))\n    # MSRCR\n    msrcr = G * (msr_img*crf - b)\n    # normalize MSRCR\n    msrcr = cv2.normalize(msrcr, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8UC3)\n    # color balance the final MSRCR to flat the histogram distribution with tails on both sides\n    msrcr = color_balance(msrcr, low_per, high_per)\n\n    return msrcr\n```\n\n![Multi scale retinex with color restoration - MSRCR:=:100](retinex-image-enhancement/msrcr.jpg)\n\nIn the above **MSRCR** output image, the color contrast has been improved compared to Multi-scale retinex. Still, the color contrast is not as close to the original image. With using default values for gain, offset, and others, there might be a chance that some pixels will over-saturate and some will under-saturate. And working around different value settings for different images is not expected behavior from a good enhancement algorithm. The main drawback of this algorithm is to control at least 6 variables which is not a general working method.\n\n---\n\n## Multi Scale Retinex with Color Preservation\n\nIn the previous algorithm **MSRCR**, color restoration was the main issue, and to address that we have introduced many variables and operations. All these calculations are computed directly on each channel value that changes chromaticity coordinates/color composition and the final result is unwanted colors, reversed color order, and sometimes grayish region due to surrounding average. To maintain the chromaticity/color composition as it is and also enhance the color contrast globally on the image, we can apply Multi-scale retinex on **intensity** image-channel which is just an addition of all image channels divided by the total number of channels.\n\n$$\nInt_{(x,y)}=\\frac{\\sum_{c=0}^{k-1}I_c{(x,y)}}{k}\n$$\n\nwhere $k$ is no. of image channels.\n\nAfter applying Multi-scale retinex to the intensity image, apply contrast stretch like applied in **MSRCR** to set the image values in the range **[0-255]** with uniform distribution of histogram values.\n\n$$\nRInt_{(x,y)}=MSR(Int_{(x,y)})\n$$\n\nafter the **MSR**, apply the color balance step with a percentage clipping of 1% on both sides.\n\nFinally, preserving the initial chromaticity ratio between each image-channel and intensity image, multiply the ratio with enhance-intensity image channel for each image-channel to get the whole image.\n\nAs we are applying retinex enhancement only on intensity-image, each color changes proportional to the ratio between enhance-intensity and normal intensity values. This keeps the relative intensity between surrounding colors locally and globally and gives better results than **MSRCR**.\n\n$$\nR_i{(x,y)}=I_i{(x,y)}\\frac{RInt_i{(x,y)}}{Int_{(x,y)}}=I_i{(x,y)}*A_{(x,y)}\n$$\n\n$$\nA_{(x,y)}=\\min(\\frac{MAX\\_VALUE}{B}, \\frac{RInt_{(x,y)}}{Int_{(x,y)}})\n$$\n\n$$\nB_{(x,y)}=\\max(I_c{(x,y)}, c\\in\\{0...k-1\\})\n$$\n\n```python\ndef msrcp(img, sigma_scales=[15, 80, 250], low_per=1, high_per=1):\n    # Multi-scale retinex with Color Preservation\n    # Int(x,y) = sum(Ic(x,y))/3, c={0...k-1}, k=no.of channels\n    # MSR_Int(x,y) = MSR(Int(x,y)), and apply color balance\n    # B(x,y) = MAX_VALUE/max(Ic(x,y))\n    # A(x,y) = max(B(x,y), MSR_Int(x,y)/Int(x,y))\n    # MSRCP = A*I\n\n    # Intensity image (Int)\n    int_img = (np.sum(img, axis=2) / img.shape[2]) + 1.0\n    # Multi-scale retinex of intensity image (MSR)\n    msr_int = msr(int_img, sigma_scales)\n    # color balance of MSR\n    msr_cb = color_balance(msr_int, low_per, high_per)\n\n    # B = MAX/max(Ic)\n    B = 256.0 / (np.max(img, axis=2) + 1.0)\n    # BB = stack(B, MSR/Int)\n    BB = np.array([B, msr_cb/int_img])\n    # A = min(BB)\n    A = np.min(BB, axis=0)\n    # MSRCP = A*I\n    msrcp = np.clip(np.expand_dims(A, 2) * img, 0.0, 255.0)\n\n    return msrcp.astype(np.uint8)\n```\n\n![Multi scale retinex with color preservation - MSRCP:=:100](retinex-image-enhancement/msrcp.jpg)\n\nThe final **MSRCP** image above is looking much better than the **MSRCR** image by preserving relative color intensities in surrounding areas like stand boards have colors enhanced with maintaining relative color ratios between surroundings. The dark regions around the corner are transformed to bright colors but unwanted gray blocks are also upscaled due to a lack of texture information.\n\n---\n\n### Result of other images\n\nThe output results for other images are\n\n![Retinex image enhancement result:=:100](retinex-image-enhancement/retinex-result-1.jpg)\n\n![Retinex color enhancement result:=:100](retinex-image-enhancement/retinex-result-2.jpg)\n\nThe **MSRCP** output images are better than **MSRCR** if the scene has different colors like in images 4th, 5th, and 7th image. Under mono-color illumination like in the 6th image, **MSRCP** gave a highly saturated blue image while the output from **MSRCR** looks decent. Choice of the algorithm depends on the scene conditions like illumination, object colors, etc.\n\n---\n\nThe Retinex algorithms presented in the above sections are very basic but gave good results. Most of the super-enhance, image super-resolution, and image-compression models have an underlying model structure based on Retinex. There are other modified Retinex algorithms developed by Gimp, NASA, etc, which give industry-level results. Without training deep-learning models, we can use these Retinex algorithms to implement simple image-enhance filters where we can save resources and time.\n\n---\n\n### References\n\n- [Retinex Theory of Color Vision](https://santhalakshminarayana.github.io/blog/retinex-theory-of-color-vision)\n- [Multiscale Retinex](http://www.ipol.im/pub/art/2014/107/)\n"},{"metadata":{"title":"Retinex Theory of Color Vision","description":"Retinex theory explains the color constancy of Human Visual System that used in many Image processing applications.","imgName":"retinex-theory/retinex-theory.jpg","date":"Feb 28, 2022","tags":["color-science"],"keywords":["retinex","color-constancy","retinex-theory","mondrian","human-visual-system","image-processing"],"id":"retinex-theory-of-color-vision"},"content":"\n![Retinex Theory](retinex-theory/retinex-theory.jpg)\n\n###### Published on: **Feb 08, 2022**\n\n# Retinex Theory of Color Vision\n\nRetinex Color Theory first proposed by Edwin Land in 1964 aimed to explain the color constancy of the Human Visual System. Retinex theory tries to explain how our visual system works for color changes and perceives object/scene color despite changes in illumination.\n\nColor constancy is a phenomenon in which the human visual system adapts to illumination/lighting changes and identifies the relative color of the object correctly like we can observe the color of an apple as red in both bright light and dark light.\n\n\u003e To understand how the human visual system sees color, read my later post about [Color Science](https://santhalakshminarayana.github.io/blog/color-science).\n\n\u003e Read more about Color Constancy and adaptation in the previous article on [Chromatic Adaptation](https://santhalakshminarayana.github.io/blog/chromatic-adaptation).\n\nWith Retinex, Land challenged the conventional color theory that states color sensation occurs due to the mixing of colors and discount of illumination. Land, his colleagues, and other researchers worked for many decades and still working to improve the Retinex theory of understanding Color vision that speaks about color processing, color sensation, and color constancy in our visual system and also using the Retinex model in image processing applications.\n\n---\n\n## Red and White image experiment\n\nIn 1959, Land conducted a series of experiments based on the principles of color formation by mixing 3 primary colors (red, blue, green). A color can be seen with the sensitivity of Cones cells that responds to primary color intensities coming from the surface. The Cones cells have three types of photoreceptor cells which are Long(L), Medium(M), and Short(S) cones that respond to three overlapping bands of wavelengths of the visual spectrum and stimulate the colors red, blue, and green respectively.\n\nAn image is a composition of these 3 primary colors and our eye stimulates L, M, and S cone cells to perceive color by interleaving these three color sensations. And Rods are a type of photoreceptor cells that are activated only when there is a low level of illumination threshold(dark light) on the scene below where cones cannot stimulate the cells and rods sense object colors as shades of gray(black and white ranges). As an image is a combination of three colors and if we can separate these three colors and make image color (red, blue, green) channels, and if viewed each channel image, it looks like a black and white image. This black and white channel image tell how much composition does have of each color.\n\nNow, according to classical color theory principles, to form a color, one does have to mix all primary colors in any composition. But Land discovered that with only single primary color, a whole wide gamut of colors of a scene can be formed and showed results in his published paper [Experiments in Color Vision](http://www.millenuvole.org/f/Fotografia/Per-quali-ragioni-vediamo-i-colori/edwin_land_1959.pdf).\n\n![Experiments in Color Vision:=:75:=:Original scene red and green channels black-and-white transparencies recording.](retinex-theory/red-green-channel-image-recording.jpg)\n\nHe did this by projecting a red channel transparent black-and-white image through a red filter and a green channel transparent black-and-white image without a filter (that is white light) which were earlier taken from the original scene with various objects in color. By superimposition of light coming from the red filter and no filter (green channel image), it was observed that with increasing intensity of red light, colors appearing by superimposition covered a wide gamut of colors of the original scene but not accurate colors.\n\n![Experiments in Color Vision:=:75:=:Projecting red image recording through a red filter and green image transparent through no filter (white light). Observe that, even with the only red color channel, we can see some range of colors compared to the original image.](retinex-theory/red-and-white-image-experiment.jpg)\n\nThe colors formed are shades of red and a wide range of saturated colors. As this experiment showed different colors formation with only red and white light, Land called this experiment a **Red-and-White image**.\n\n\u003e Before proceeding further reading it's better to have an understanding of terms related to light like **illumination**, **radiation energy**, **radiant flux**, **radiance**, **surface reflectance**, etc.\n\n### Lightness\n\nLightness is one of the most important concepts that Land proposed to understand the color vision of humans. Before understanding the colorful world, he started uncovering non-colorful details like black-and-white sensations we observe in dark vision which is when rods are activated and identifies scene color in the range from white to black.\n\nIn the dark vision, for color objects, our eyes perceive objects color of red, blue, and black as darker, green and white as lighter. The color of an object in night vision is not determined by the amount of light energy we are getting at the eye, but the nature of objects which have the amount of composition of the above colors. So whether increased or decreased illumination on the scene or radiant energy coming to the eye, the color perception is close to the object's true nature in the range of white-to-black colors. Lighter objects look light and darker objects look dark irrespective of radiation flux reaching the eye. Thus we can say colors of scene/image when only one photoreceptor (here rods) is functioning are the only range of white-to-black colors. And the lightness of a point in a scene is simply how much lighter it is compared to the lightest point in the scene.\n\nSo, the lightness value of an image/scene tells the black and white range value when only one photoreceptor is activated. You can imagine lightness as a single image channel that consists of only black and white colors as described in the above section. And this behavior can be extended to cones cells and each L, M, and S cone produce lightness values independently. The lightness of an area would be affected by the surrounding area's lightness like a small area when surrounded by a large area, the small area lightness depends on the large area if it is lighter or darker.\n\nIn our visual system, irrespective of illumination and flux reaching the eye, when light falls on the retina and it is observed by L, M, and S visual pigments independently and produce a response to the receptor system that converts these responses to produce lightness of that particular cell sensation which doesn't to relate the amount of flux energy reaching the eye.\n\nNow one can assume this lightness as red, blue, and green sensitivity values of a scene that processed independently by three visual receptor cells. If rods and L cones are activated with an illumination level that cuts off M and S cone's responsivity, then the resultant combination is a composition of black-and-white values from rods and red values from L cone cells.\n\nJohn McCann and Jeanne Benton reproduced the above Red-and-White image experiment by observing the color sensations of an eye when only rods and L cells are activated. This experiment is similar to projecting a red channel image through a red filter and a green channel image through no filter (white light). Rods cells activated when the scene was illuminated with a 550nm narrow waveband of light and long-wave receptor cells (L) activated with a 656nm narrow band of light. As L cones sense red color and rod cells sense black-and-white, the results observed of the scene were similar to the original experiment and various close colors of the original scene were observed.\n\n---\n\n## Retinex\n\nThe location of production of lightness in the visual system is uncertain that is it would be formed either in the retina or cortex or in both. As there is uncertainty about the location, Land coined the term **Retinex** which is a combination of retina and cortex. And Retinex is a biological mechanism of converting light to lightness that can happen in any part of the visual system.\n\nThe experiments above raise questions that what makes the color? From the above two-receptor experiment which produced color even though medium and short receptors haven't observed anything, Land concluded that independent lightness of each photoreceptor cell are compared to each other and the color is formed by comparing lightness values of different regions of the scene which contradicts the then existing belief of color formation by mixing of response by photoreceptors.\n\nFor color constancy, Land stated that as the lightness of three independent receptors is independent of flux reaching the eye (and also illumination), no matter what the illumination projected on the scene out visual system produces lightness for each of three wavebands and comparison of these lightness values makes color. So we achieve color constancy because of lightness produced independently of flux. The previous statement is quite convincing to answer the human color constancy behavior. But how one can measure lightness? how to prove color constancy with lightness independent of flux? We will cover these questions in the following experiment involving the color Mondrian.\n\n### Retinex system respone\n\nBefore proceeding to the color Mondrian experiment, let's first discuss Retinex responses that are equal to the human visual response that converts light energy to lightness for three independent receptor cells. To measure the visual sensitivity response of each cell, a device is developed using film-filter combinations to match the responsiveness of photoreceptor cells.\n\n![Spectral sensitivities of visual receptor cells:=:70:=:Spectral sensitivities of each photoreceptor cell for each waveband range in spectrum.](retinex-theory/spectral-sensitivities-retinex.jpg)\n\nIn the above diagram, the visual pigment sensitivity of the human eye is represented. The solid gray curve for rods and solid black curves for each cone cell. Broken line curves represent the matching sensitivities of the device for each of the photoreceptor cells. With this device that produces sensitivities like the human visual system, black-and-white images for each cell can be produced, and the resultant formation record/image represents respective lightness for that cell. And whenever there is lightness, Retinex comes in. So, the lightness records are called Retinex records which are black-and-white lightness of each cell responsiveness.\n\n---\n\n## Color Mondrian experiment\n\nTo prove that the human visual system adapts to illumination changes and performs color constancy, the Color Mondrian experiment was conducted by Land by preparing two identical boards filled with different color papers of various rectangular sizes. The resultant board formation is called **Mondrian** because the arrangement of color papers in different area sizes resembles the artwork of the Dutch painter Piete Mondrian.\n\n![Color Mondrian:=:30:=:Color Mondrian](retinex-theory/mondrian.jpg)\n\nThe two identical Mondrian boards are illuminated by three (red, green, blue) light projectors equipped with 670nm long-band, 540nm medium-band, and 450nm short-band wave filters. The illumination settings of these three different projectors are controlled to project any overall illumination conditions by changing light intensity. A photometer is used to read the amount of radiation reflected from any region of Mondrian.\n\n![Color Mondrian projection experiment:=:75:=:Two identical Mondrians are illuminated focusing green and red areas respectively.](retinex-theory/color-mondrian-projection.jpg)\n\nInitially left side illumination is on and the right side is projection is off. Then select an area on the left Mondrian say green, change the illumination settings of the three projectors so that the green area looks pure green on the left side. Read radiance values from the photometer of the light that reaches the eye. Radiance values are measured one by one by turning off the other two projectors. Assume **(X, Y, Z)** is the radiance values for long, medium, and short waveband projectors coming from the left side.\n\nNow on the right side, select an area other than green say red patch. For each of the projectors on the right side, turn on one projector and keep the other two projectors off. Change illumination so that the radiance value measure is the same as that coming on left side projectors. For example, on the long wave projector and turn-off medium and short waveband projectors. Increase/decrease the intensity of the long-wave projector so that the radiance value measured is equal to **X**. Repeat the same for other medium and short wavebands so that respective radiance values are equal to **Y** and **Z**. After reading values individually, turn on all three projectors and observe the color of the selected patch. It looks the same as red color even though the same quanta catch on the retina.\n\nThis observation that despite the same light energy reaching the eye, we can identify the patch color closely as green and red. This result challenges the principle that different colors are recognized by the eye depending on the amount of light intensity for each of the long, medium, and short wavebands contain. Here despite identical light energy (X, Y, and Z radiances) falling on the retina, we can identify different colors.\n\nThis makes Land conclude that color sensation in the human visual system processes each of long, medium, and short wavebands individually and forms another informative image called lightness image (as described above) for each of the three cones cells. The lightness images calculated are independent of the amount of light intensity reaching the eye.\n\nLightness is independent of flux because it can be observed that when only one receptor is active, let's take a long wave, despite the change in intensity of light, red color appears light. For medium and short waveband, red color is observed as darker and very dark respectively. So that for every color, the level of lightness for each of the three receptors is constant despite the change in light energy.\n\nNow we know lightness is independent of light illumination and we have to examine how the reflectance from the surface affects the color sensation under different illuminations.\n\n---\n\n## Color Mondrian and Munsell Chip matching experiment\n\nJohn J. McCann, Suzanne P. McKee, and Thomas H. Taylor experimented to study responses of observers if asked to match Mondrian areas to respective color in Munsell's book of color (which contains color swatch for hundreds of colors).\n\n![Color Mondrian and Munsell chip matching:=:75:=:Observers view Mondrian and Munsell chips through left and right eyes resepectively](retinex-theory/mondrian-munsell-view.jpg)\n\nThe color Mondrian is made with 17 different colors and projected under various illuminations and observers are asked to match the corresponding Munsell chip for every Mondrian color. Munsell's book is viewed under constant white illumination so that the white color looks pure white. The color chips are viewed by surrounding large gray paper which has a square cut to look exactly one chip at a time. Observers view Mondrian through the left eye and Munsell chips through the right eye.\n\nFirst observers are asked to match 17 Munsell chips for each of the Mondrian color patches under illumination settings that lit gray area in Mondrain as pure gray. Now record the triplet of radiance energies coming to the eye from the gray region on Mondrian say **(X, Y, Z)** for long, medium, and short wavebands, and also read radiance triplet for the gray chip in Munsell chip. After reading, select another patch, take the red region on Mondrian, and change illumination settings that the radiance triplet coming from the red region is the same as **(x, Y, Z)**, Note the matched Munsell chips for each of the Mondrian areas under this illumination that red region sent **(X, Y, Z)** radiance triplet. Repeat the experiment for adjusting illumination for yellow, blue, and green so that triplet energies coming from each of the respective areas is the same as **(X, Y, Z)** and note down the matched 17 chips and radiance triplet values. Throughout the experiment, only illumination settings for Mondrian change, and Munsell's book is viewed under constant illumination. Several observers are asked to match colors and matching colors are averaged over all experiments. And results showed that observers matched color chips approximately to several Mondrian color patches even under different illuminations. This shows color constancy under different illuminations by observers.\n\nRadiances are measured by a combination of photomultiplier and the retinex filters which measure different waveband responses that are close to the human visual system. This photomultiplier-retinex combination measures the integrated radiation flux for a given area for different wavebands. The resultant value is called integrated radiance as it gives radiation energy integrated for an area.\n\nIllumination values and light triplet integrated radiance values reaching the eye are known but don't know the reflectance from the surface. The reflectance of a surface is the amount of light reflected divided by the amount of light incident on the surface. Reflectance in general is not a comparison value and doesn't give a relative measurement. As white surface reflects almost all amount light, McCann and his colleagues used white as a comparison for forming scaled reflectance values. First, they illuminated the white surface with illumination that had been used for illuminating Munsell's book of color. They recorded integrated radiance values for each of the wavebands coming from a white surface. Now for any surface, reflectance can be obtained by dividing radiance values with radiance values for the white surface under the same illumination. The reflectance values are expressed as a percentage in the range 0-100 with 100 percent being maximum reflectance that is white surface reflectance.\n\n### Lightness Sensation\n\nObservers are asked to arrange different shades of black and white color patches in a range from darkest to lightest, and reflectance values are recorded for every color in the range. A function has been derived based on several arrangements by different observers, and the function gives the relation between black-and-white colors and reflectance value. Based on reflectance values for a single narrow-band wave, lightness sensations are scaled from 0-10 equally spaced intervals as shown in the following figure\n\n![Lightness Sensation:=:60:=:Lightness sensation vs Reflectance for shades of black-and-white colors](retinex-theory/lightness-sensation.jpg)\n\nReflectance values for various Munsell chips are determined under the same white illumination. It is observed that reflectance values for the Mondrian area and corresponding matched Munsell chips are close.\n\nNow back to the experiment, a very close relation between reflectance and illumination is observed that the light energy reaching the eye is equal to reflectance product illumination.\n\n![Reflectance product Illumination:=:90](retinex-theory/reflectance-illumination-relation.jpg)\n\nIn the above image, the left side charts denote Mondrian color paper values and the right side denotes matched Munsell chip values. The bar chart denoting reflectance has values in scaled reflectance in the range of 0.0-1.0 for each of the three narrow-bands 630nm, 530nm, and 450nm. For both Mondrian and Munsell, light energy at the eye (recorded by photometer) is equal to the product of reflectance and illuminant.\n\nFrom this data, the relation between surface reflectance, illumination of surface, and color sensation in the eye can be described as\n\n$$\nI=R*S\n$$\n\nwhere I is the surface image, R is the reflectance of the surface, and S is the illumination on the surface.\n\n---\n\nRetinex theory talks about how lightness is crucial for color constancy. But how does our visual system calculate the lightness of a scene for each of the receptor cells? How visual system scales these lightness values like lightness sensation and reflectance relation? Land stated that our visual system compares the lightness images and senses a color. The color of an object also depends on the surrounding colors and edges. This comparison of lightness across each receptor cell and for every region gives the sensation of color. And human visual system obtains these lightness values from only flux reaching the eye from a scene by comparing narrow-waveband response ratios at adjacent edges and surrounding areas that are independent of illumination and reflectance.\n\nFrom a lightness image, the eye compares different regions of the image and calculates the sequential product of ratios of reflectances to estimate maximum and minimum lightness values for that lightness image. These sequential products also give relative lightness between two regions of the image. Our visual system has evolved over the years to adapt to these changes and maintain color constancy.\n\nEven though Edwin Land's Retinex theory conceptualizes how the human visual system adapts to color constancy, the accuracy of the theory has not been proved yet, and many scientists disproved the relation between lightness and color adaptation. After many decades, there is still research going on Retinex theory and its applications mainly in image processing.\n\n---\n\n### References\n\n- [Colour vision: Is colour constancy real?](https://www.cell.com/action/showPdf?pii=S0960-9822%2899%2980354-6)\n- [Experiments in Color Vision - Edwin Land](http://www.millenuvole.org/f/Fotografia/Per-quali-ragioni-vediamo-i-colori/edwin_land_1959.pdf)\n- [The Retinex Theory of Color Vision - Edwin Land](http://perceptionstuff.weebly.com/uploads/2/8/4/7/2847832/land-retinex_theory.pdf)\n- [Lightness and Retinex Theory - Edwin Land and John J. McCann](https://www.cs.jhu.edu/~misha/ReadingSeminar/Papers/Land71.pdf)\n- [Retinex at 50](https://www.spiedigitallibrary.org/journals/journal-of-electronic-imaging/volume-26/issue-03/031204/Retinex-at-50--color-theory-and-spatial-algorithms-a/10.1117/1.JEI.26.3.031204.full?SSO=1)\n- [Retinex: Physics and the Theory of Color Vision](https://aip.scitation.org/doi/pdf/10.1063/1.4822794)\n- [Quantitative studies in Retinex Theory](https://www.semanticscholar.org/paper/Quantitative-studies-in-retinex-theory-a-comparison-McCann-McKee/a69002b2e276e111db8f4f18b690e03cbbeb9c4b)\n- [Color Vision Is a Spatial Process: The Retinex Theory](http://link.springer.com/content/pdf/10.1007%2F978-3-319-56010-6_3.pdf)\n- [Analysis of the retinex theory of color vision](https://color2.psych.upenn.edu/brainard/papers/retinex.pdf)\n"},{"metadata":{"title":"Chromatic adaptation (Color constancy)","description":"Color constancy of an image using Chromatic adaptation technique in Python.","imgName":"chromatic-adaptation/chromatic-adaptation.jpg","date":"Nov 26, 2021","tags":["image-processing","color-science","opencv","python"],"keywords":["color-science","color-constancy","chromatic-adaptation","gray-world","opencv","python"],"id":"chromatic-adaptation"},"content":"\n![Color constancy of an image using Chromatic adaptation](chromatic-adaptation/chromatic-adaptation.jpg)\n\n###### Published on: **Nov 26, 2021**\n\n# Color constancy of an image using Chromatic adaptation\n\n## Color constancy\n\nColor constancy is the tendency of the human color system that ensures the color perception of objects is relatively constant under varying illumination conditions. It means we observe the same object colors for different light conditions.\n\nLike for example, under a greenish light source, the color of the apple looks relatively red even though the intensity of the green wavelength is greater than other colors. It's unknown how we (and some other animals) have this color perception system.\n\nSeveral mechanisms like metamerism, chromatic adaptation, memory map, etc. are involved in achieving color constancy. Metamerism happens when two object looks the same color even though the reflectance wavelengths are not the same but overall color is the same. Our memory map of object colors helps our eyes to adjust the color perception after some time under different illuminations.\n\nColor constancy is not true in every object case, for some objects under certain illumination conditions, it could be inverse. Color constancy happens in most of the cases for objects if we have seen them before and for other objects/surfaces, we observe the color of major intensity wavelength. Color constancy depends heavily on the illumination source and neighborhood surface.\n\n![Checker Shadow Illusion:=:50:=:Even though both A and B are the same gray color, A appears darker than B because of shadow from the cylinder which changes illumination on B](chromatic-adaptation/checker-shadow-illusion.jpg)\n\nSeveral theories like photoreceptor sensitivity, retinex, color map, etc, try to measure the color constancy using different mechanisms.\n\n## Chromatic adaptation\n\nChromatic adaptation is one of the mechanisms involved in color constancy and is defined as the ability of human color perception to adjust the retina/cortex sensitivity for changes in the illuminant color system. Chromatic adaptation is closely related to the adjustment of cone sensitivity that happens in our eyes when the illumination changes. Like our eyes adjust to see the objects as constant colors under different day-light conditions like sunrise, mid-day, and sunset. In all these conditions, we perceive object colors as the same. This could be opposite in some conditions where we see opposite colors instead of relative colors.\n\nIn computer vision, computation color constancy is achieving human color constancy using different methods based on color constancy mechanisms. And chromatic adaptation technique is one of the methods to achieve computational color constancy based on illumination.\n\n### Von Kries transform\n\nThe Von Kries chromatic adaptation technique is based on the LMS cone sensitivity response function. LMS cones responses in the retina are independent of each other and they adjust independently for varying illuminations. Each cone increases or decreases responsiveness based on illuminant spectral energy. Each cone cell gains (increase or decrease) some spectral sensitivity response to adapt for new illumination from the previous illumination to perceive object constant colors.\n\nBased on LMS cones gain, Von Kries adaptation transforms one illuminant to other to keep the **white color** constant in both systems. Von Kries transform based on LMS cone sensitivity is the base for chromatic adaptation transforms proposed later.\n\n---\n\n## Chromatic adaptation transform\n\nVon Kries chromatic adaptation transformation is a linear transformation of the source color to destination color based on LMS cones gain for adaptation to destination illuminant. That is, we can transform the color in one illuminant to other illuminants if we scale the color with a gain of LMS for adaptation to color constancy.\n\n\u003e If you're not familiar with concepts like **CIE RGB**, **CIE XYZ** tristimulus values, **chromaticity diagram**, and other color science topics, please refer to those topics at my previous blog about [color-science](https://santhalakshminarayana.github.io/blog/color-science).\n\nThe destination LMS cone responses can be calculated by multiplying the gain LMS with source LMS cone responses. In matrix form, the representation is\n\n$$\n\\begin{bmatrix}\nL_{D} \\\\\nM_{D} \\\\\nS_{D} \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\nL_{D}/L_{S} \u0026 0 \u0026 0 \\\\\n0 \u0026 M_{D}/M_{S} \u0026 0 \\\\\n0 \u0026 0 \u0026 S_{D}/S_{S} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nL_{S} \\\\\nM_{S} \\\\\nS_{S} \\\\\n\\end{bmatrix}\n$$\n\nwhere **L, M, and S** of source and destination are represented in **CIE LMS color space**, and $$L_{D}/L_{S}$$, $$M_{D}/M_{S}$$, and $$S_{D}/S_{S}$$ are gain factors for source to destination illumination adaptation.\n\nAs color representation in **LMS color space** is difficult and not practiced, transform the **LMS color space** to **CIE XYZ**. When a transformation is performed in **CIE XYZ** or any other space but not **LMS cone space** is called **Wrong Von Kries**.\n\n$$\n\\begin{bmatrix}\nL \\\\\nM \\\\\nS \\\\\n\\end{bmatrix} = M_{A}\n\\begin{bmatrix}\nX \\\\\nY \\\\\nZ \\\\\n\\end{bmatrix}\n$$\n\n$$\n\\begin{bmatrix}\nL \\\\\nM \\\\\nS \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\na \u0026 b \u0026 c \\\\\nd \u0026 e \u0026 f \\\\\ng \u0026 h \u0026 i \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nX \\\\\nY \\\\\nZ \\\\\n\\end{bmatrix}\n$$\n\nwhere the transformation matrix $$M_{A}$$ with dimensions $$3x3$$ and independent gain control factors $$(a, e, i)$$ is called chromatic adaptation transform (CAT) matrix. Each CAT model give different CAT matrices for transformation from **LMS cone space** to **CIE XYZ**.\n\nWrong Von Kries transform by **XYZ** gain scaling to **CIE XYZ** tristimulus values of the source is,\n\n$$\n\\begin{bmatrix}\nX_{D} \\\\\nY_{D} \\\\\nZ_{D} \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\nX_{D}/X_{S} \u0026 0 \u0026 0 \\\\\n0 \u0026 Y_{D}/Y_{S} \u0026 0 \\\\\n0 \u0026 0 \u0026 Z_{D}/Z_{S} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nX_{S} \\\\\nY_{S} \\\\\nZ_{S} \\\\\n\\end{bmatrix}\n$$\n\nWe can convert **CIE XYZ** to **LMS cone space** and then back to **CIE XYZ** with $$M_{A}$$, then we can change Wrong Von Kries to general Von Kries as,\n\n$$\n\\begin{bmatrix}\nX_{D} \\\\\nY_{D} \\\\\nZ_{D} \\\\\n\\end{bmatrix} = M\n\\begin{bmatrix}\nX_{S} \\\\\nY_{S} \\\\\nZ_{S} \\\\\n\\end{bmatrix}\n$$\n\n$$\nM = M_{A}^{-1}\n\\begin{bmatrix}\nL_{D}/D_{S} \u0026 0 \u0026 0 \\\\\n0 \u0026 M_{D}/M_{S} \u0026 0 \\\\\n0 \u0026 0 \u0026 S_{D}/S_{S} \\\\\n\\end{bmatrix} M\n$$\n\n$$\n\\begin{bmatrix}\nL_{S} \\\\\nM_{S} \\\\\nS_{S} \\\\\n\\end{bmatrix} = M_{A}\n\\begin{bmatrix}\nX_{S} \\\\\nY_{S} \\\\\nZ_{S} \\\\\n\\end{bmatrix}\n$$\n\n$$\n\\begin{bmatrix}\nL_{D} \\\\\nM_{D} \\\\\nS_{D} \\\\\n\\end{bmatrix} = M_{A}\n\\begin{bmatrix}\nX_{D} \\\\\nY_{D} \\\\\nZ_{D} \\\\\n\\end{bmatrix}\n$$\n\nThe generalized version of Von Kries for **CIE XYZ white point** reference is,\n\n$$\n\\begin{bmatrix}\nX_{D} \\\\\nY_{D} \\\\\nZ_{D} \\\\\n\\end{bmatrix} = M_{A}^{-1}\n\\begin{bmatrix}\nL_{D}/L_{S} \u0026 0 \u0026 0 \\\\\n0 \u0026 M_{D}/M_{S} \u0026 0 \\\\\n0 \u0026 0 \u0026 S_{D}/S_{S} \\\\\n\\end{bmatrix} M_{A}\n\\begin{bmatrix}\nX_{S} \\\\\nY_{S} \\\\\nZ_{S} \\\\\n\\end{bmatrix}\n$$\n\n$$(X, Y, Z)$$ values for source and destination are illuminant white point values in **CEI XYZ**.\n\n\u003e [White point](https://en.wikipedia.org/wiki/White_point) is the color that is formed when all cone sensitivity responses are maximum.\n\n---\n\n## Implementation of Chromatic adaptation transform\n\nWe will implement chromatic adaptation for converting source image to destination illuminant by applying CAT. Chromatic adaptation for an image is implemented in two steps\n\n1. Estimate scene illuminant of image\n2. Convert source image to destination illuminant with source illuminant obtained in step 1\n\n### Illumination estimation of image\n\nIllumination estimation is predicting tristimulus values (CIE XYZ) for the white point of the illuminant light source under which the reference image has been taken. These predicted white point tristimulus values were then later used for CAT. Illumination estimation is a very crucial step in chromatic adaptation. Several illumination estimation methods exist like\n\n- Gray world assumption\n- White patch retinex\n- Reflection model\n\nThe above methods are suitable for uniform illumination, for nonuniform illumination Retinex models are widely used.\n\nFor now, we use the Gray world assumption for illumination estimation\n\n### Gray world assumption illuminant estimation\n\n[Gray world assumption](https://en.wikipedia.org/wiki/Color_normalization) is a simple method that assumes image contains objects with different reflectance colors uniformly from minimum to maximum intensities and averaging all pixel colors gives gray color. Illumination estimation is calculated by averaging all pixel values for each channel. For an image with equal representation of colors, illumination estimation gives average color gray.\n\n$$\nIlluminant = (R_{avg}/255, G_{avg}/255, B_{avg}/255)\n$$\n\nwhere $R_{avg}$, $G_{avg}$, and $B_{avg}$ are means of each image channel (R, B, G). Scale the mean values to the maximum intensity value of an image which is generally 255 (8-bit).\n\n\u003e Images are generally stored in JPEG or PNG in [sRGB](https://en.wikipedia.org/wiki/SRGB) format. To mimic the human perception of non-linear luminance factor, while taking pictures, exposure values are [Gamma corrected](https://www.cambridgeincolour.com/tutorials/gamma-correction.htm) with a value close to 2.2. And any image processing operation should be applied on linear RGB values by converting sRGB gamma to linear RGB without gamma.\n\nRead source image to which we apply chromatic adaptation and transform to destination illuminant,\n\n```python\nimport numpy as np\nimport cv2\n\n# read image which generally in sRGB format\nimg = cv2.imread('input.jpg')\n# reverse channel order from BGR to RGB\nimg = img[:, :, ::-1]\n# scale image in range [0.0, 1.0]\nimg = img / 255.0\n```\n\nConvert sRGB image to linear RGB\n\n```python\ndef srgb_to_linear(srgb):\n    # 'sRGB' in [0.0, 1.0]\n\n    ln_rgb = srgb.copy()\n    mask = ln_rgb \u003e 0.04045\n    ln_rgb[mask] = np.power((ln_rgb[mask] + 0.055) / 1.055, 2.4)\n    ln_rgb[~mask] /= 12.92\n    return ln_rgb\n\ndef linear_to_srgb(linear):\n    # 'linear RGB' in [0.0, 1.0]\n\n    srgb = linear.copy()\n    mask = srgb \u003e 0.0031308\n    srgb[mask] = 1.055 * np.power(srgb[mask], 1 / 2.4) - 0.055\n    srgb[~mask] *= 12.92\n    return np.clip(srgb, 0.0, 1.0)\n```\n\n![sRGB to linear RGB](chromatic-adaptation/srgb-to-linear-rgb.jpg)\n\nApply gray world on linear RGB image and get illuminant\n\n```python\ndef get_gray_world_illuminant(img):\n    # image in sRGB with range [0.0, 1.0]\n    # convert sRGB to linear RGB\n    ln_img = srgb_to_linear(img)\n    # mean of each channel\n    avg_ch = ln_img.mean(axis=(0, 1))\n    # convert back RGB mean values to sRGB\n    return linear_to_srgb(avg_ch)\n```\n\nIlluminant for the image is\n\n```python\nilluminant = get_gray_world_illuminant(img)\n```\n\n```python\nprint(illuminant) =\u003e array([0.29078475, 0.39055316, 0.50858647])\n```\n\nIlluminant for the above image is **[0.29078475, 0.39055316, 0.50858647]**. For the output illuminant, we can say that the blue color influence is higher than other color wavelengths in the image.\n\nNow we have the illuminant of the source image, next apply chromatic adaptation for this image.\n\n### Chromtic adaptation\n\nWe have source illuminant, to transform to destination illuminant, we consider [D65 standard illuminant](https://www.waveformlighting.com/color-matching/what-is-d65-and-what-is-it-used-for) for the destination which is standard for digital images and displays. We assume that the source image was taken under a different illuminant and we now transform that image to D65 illuminant.\n\nIlluminant for D65 white is **[0.95047, 1., 1.08883]**, these are the **CIE XYZ** values for white point of sRGB D65. sRGB values for D65 white point (all values are 255) is **(1.0, 1.0, 1.0)** which is destination illuminant white point.\n\nWe implement chromatic adaptation transform step-by-step.\n\n\u003e Every operation should be applied on **linear RGB**\n\nFirst, convert illuminant sRGB values to **CIE XYZ**.\n\n```python\nRGB_TO_XYZ = np.array([[0.412453, 0.357580, 0.180423],\n                       [0.212671, 0.715160, 0.072169],\n                       [0.019334, 0.119193, 0.950227]])\n\nXYZ_TO_RGB = np.array([[3.240481, -1.537151, -0.498536],\n                       [-0.969256, 1.875990, 0.0415560],\n                       [0.055647, -0.204041, 1.057311]])\n\ndef srgb_to_xyz(srgb):\n    # convert 'sRGB' to 'linear RGB'\n    rgb = srgb_to_linear(srgb)\n    # convert 'linear RGB' to 'XYZ'\n    return rgb @ RGB_TO_XYZ.T\n\ndef xyz_to_srgb(xyz):\n    # convert 'XYZ' to 'linear RGB'\n    rgb = xyz @ XYZ_TO_RGB.T\n    # convert back 'linear RGB' to 'sRGB'\n    return linear_to_srgb(rgb)\n\ndef normalize_xyz(xyz):\n    # normalize xyz with 'y' so that 'y' represents luminance\n    return xyz / xyz[1]\n```\n\nConvert sRGB to XYZ passing source and destination white points\n\n```python\n# source illuminant white point obtained from previous step\nsrc_white_point = illuminant\n# destination illuminant white point scale to 1.0\ndst_white_point = (1.0, 1.0, 1.0)\n\n# convert white point in 'sRGB' to 'XYZ'\n# and normalize 'XYZ' that 'Y' as luminance\nxyz_src = srgb_to_xyz(src_white_point)\nn_xyz_src = normalize_xyz(xyz_src)\nxyz_dst = srgb_to_xyz(dst_white_point)\nn_xyz_dst = normalize_xyz(xyz_dst)\n```\n\nNext, convert XYZ values to LMS cone space by transforming XYZ values with CAT transform matrix. Multiple CAT matrices exist and the Bradford model is popular among all these types.\n\n```python\nBRADFORD = np.array([[0.8951, 0.2664, -0.1614],\n                     [-0.7502, 1.7135, 0.0367],\n                     [0.0389, -0.0685, 1.0296]])\n\nVON_KRIES = np.array([[0.40024, 0.70760, -0.08081],\n                      [-0.22630, 1.16532, 0.04570],\n                      [0.00000, 0.00000, 0.91822]])\n\nSHARP = np.array([[1.2694, -0.0988, -0.1706],\n                  [-0.8364, 1.8006, 0.0357],\n                  [0.0297, -0.0315, 1.0018]])\n\nCAT2000 = np.array([[0.7982, 0.3389, -0.1371],\n                    [-0.5918, 1.5512, 0.0406],\n                    [0.0008, 0.2390, 0.9753]])\n\nCAT02 = np.array([[0.7328, 0.4296, -0.1624],\n                  [-0.7036, 1.6975, 0.0061],\n                  [0.0030, 0.0136, 0.9834]])\n```\n\nConvert XYZ values to LMS and get gain scale factors by dividing destination LMS with source LMS.\n\n```python\ndef get_cat_matrix(cat_type = 'BRADFORD'):\n    if cat_type == 'BRADFORD':\n        return BRADFORD\n    elif cat_type == 'VON_KRIES':\n        return VON_KRIES\n    elif cat_type == 'SHARP':\n        return SHARP\n    elif cat_type == 'CAT2000':\n        return CAT2000\n    else:\n        return CAT02\n\ndef xyz_to_lms(xyz, M):\n    return xyz @ M.T\n\ndef get_gain(lms_src, lms_dst):\n    return lms_dst / lms_src\n\ndef transform_lms(M, gain):\n    return np.linalg.inv(M) @ np.diag(gain) @ M\n```\n\n```python\n# get CAT type matrix\ncat_m = get_cat_matrix('BRADFORD')\n\n# convert 'XYZ' to 'LMS'\nlms_src = xyz_to_lms(n_xyz_src, cat_m)\nlms_dst = xyz_to_lms(n_xyz_dst, cat_m)\n# LMS gain by scaling destination with source LMS\ngain = get_gain(lms_src, lms_dst)\n\n# multiply CAT matrix with LMS gain factors\nca_transform = transform_lms(cat_m, gain)\n```\n\n_ca_transform()_ applies chromatic adaptation transformation on LMS gain factors and then converts back the LMS values to XYZ by multiplying with the inverse transform of CAT matrix.\n\nTransform the source image to destination illuminant by multiplying XYZ values with chromatic adaptation transformation XYZ values.\n\n```python\n# convert 'sRGB' source image to 'XYZ'\nsrc_img_xyz = srgb_to_xyz(img)\n\n# apply CAT transform to image\ntransformed_xyz = src_img_xyz @ ca_transform.T\n\n# convert back 'XYZ' to 'sRGB' image\ntransformed_img = xyz_to_srgb(transformed_xyz)\n```\n\n**transformed_img** is the final image after transformation.\n\nThe final chromatic adaptation function that transforms source image to destination illuminant is\n\n```python\ndef chromatic_adaptation_image(src_white_point, dst_white_point, src_img, cat_type = 'BRADFORD'):\n    # convert white point in 'sRGB' to 'XYZ'\n    # and normalize 'XYZ' that 'Y' as luminance\n    xyz_src = srgb_to_xyz(src_white_point)\n    n_xyz_src = normalize_xyz(xyz_src)\n    xyz_dst = srgb_to_xyz(dst_white_point)\n    n_xyz_dst = normalize_xyz(xyz_dst)\n\n    # get CAT type matrix\n    cat_m = get_cat_matrix(cat_type)\n\n    # convert 'XYZ' to 'LMS'\n    lms_src = xyz_to_lms(n_xyz_src, cat_m)\n    lms_dst = xyz_to_lms(n_xyz_dst, cat_m)\n    # LMS gain by scaling destination with source LMS\n    gain = get_gain(lms_src, lms_dst)\n\n    # multiply CAT matrix with LMS gain factors\n    ca_transform = transform_lms(cat_m, gain)\n\n    # convert 'sRGB' source image to 'XYZ'\n    src_img_xyz = srgb_to_xyz(src_img)\n\n    # apply CAT transform to image\n    transformed_xyz = src_img_xyz @ ca_transform.T\n\n    # convert back 'XYZ' to 'sRGB' image\n    transformed_img = xyz_to_srgb(transformed_xyz)\n\n    return transformed_img\n```\n\n```python\n# read image which generally in sRGB format\nimg = cv2.imread('input.jpg')\n# reverse channel order from BGR to RGB and scale to 1.0\nr_img = img[:, :, ::-1] / 255\n# get source illuminant by illumination estimation\nsrc_white_point = get_gray_world_illuminant(r_img)\ndst_white_point = np.array([1.0, 1.0, 1.0])\n# apply chromatic apatation for source image\nca_img = chromatic_adaptation_image(src_white_point, dst_white_point, r_img, cat_type='BRADFORD')\n# reverse channel order from RGB to BGR, and rescale to 255\nca_img = (ca_img[:, :, ::-1] * 255).astype(np.uint8)\n```\n\n![Chromatic adaptation transform Batman](chromatic-adaptation/chromatic-adaptation-transform-batman.jpg)\n![Chromatic adaptation transform Pool](chromatic-adaptation/chromatic-adaptation-transform-pool.jpg)\n\nAs chromatic adaptation is done with reference white point, the transformed images contain improved white color ranges and other colors are transformed based on illumination transform obtained by white point transform.\n\nAs illumination estimation is a crucial step, we can improve results further by applying other illumination estimation methods and images with uniform color objects.\n\n---\n\n### ColorChecker\n\n[ColorChecker](https://en.wikipedia.org/wiki/ColorChecker) is a color checker chart containing multiple color squares or rectangles in a grid. ColorChecker chart contains a range of spectral reflectance color patches that represents natural object colors like human skin, leaves, flowers, sky, etc. These color patches on a whole represent all possible intensity ranges that are suitable for many uniform illumination conditions.\n\nColorChecker charts are used in photography while shooting images/videos under varying illumination conditions. Multiple scenes are shot under varying lighting conditions placing ColorChecker chart in the scene, and later based on the color patches illumination, images are adjusted to destination illuminant. This process is called color grading and it involves several steps including chromatic adaptation.\n\n![ColorChecker chart:=:40:=:Macbeth ColorChecker chart](chromatic-adaptation/color-checker.jpg)\n\nAs the color checker chart contains uniform color ranges, we can use this chart to estimate the illuminant of an image. We can detect ColorChecker chart in an image using OpenCV using OpenCV-Contrib module **mcc** (Macbeth ColorChecker).\n\n```python\ndef get_colorchecker_coord(img):\n    # initialize CCDetector object\n    checker_detector = cv2.mcc.CCheckerDetector_create()\n    # detect classic Macbeth 24 color grid chart\n    has_chart = checker_detector.process(img, cv2.mcc.MCC24, 1)\n    # if any chart present\n    if has_chart:\n        # ColorChecker chart coordinates\n        # order - (tl, tr, br, bl)\n        box = checker_detector.getListColorChecker()[0].getBox()\n        min_x = int(min(box[0][0], box[3][0]))\n        max_x = int(max(box[1][0], box[2][0]))\n        min_y = int(min(box[0][1], box[1][1]))\n        max_y = int(max(box[2][1], box[3][1]))\n        coord = [(min_x, min_y), (max_x, max_y)]\n        return [True, coord]\n    else:\n        return [False, []]\n```\n\nThe above function returns rectangle coordinates (top-left, bottom-right) for ColorChecker chart if present in an image.\n\nWe extract ColorChecker from an image and estimate illuminant for only that extracted color chart as it contains uniform color ranges that give a good estimation of illuminant than the whole image.\n\n```python\nsrc_white_point = np.array([1.0, 1.0, 1.0])\nhas_chart, coord = get_colorchecker_coord(img)\nif has_chart:\n    src_white_point = get_gray_world_illuminant(r_img[coord[0][1]: coord[1][1], coord[0][0]: coord[1][0]])\nelse:\n    src_white_point = get_gray_world_illuminant(r_img)\n```\n\n![Detect ColorChecker chart](chromatic-adaptation/detec-colorchecker-chart.jpg)\n\nChromatic adaptation of the above image with illuminant obtained from cropped ColorChecker chart is\n\n![Chromatic adptation ColorChecker Batman](chromatic-adaptation/chromatic-adaptation-batman-comparison.jpg)\n\nSome regions and color ranges of transformed images of **Img 1** and **Img 2** are close to each other because we have applied the same destination illuminant for both images and this gives the conclusion that chromatic adaptation works to transform images from one illuminant to another.\n\n---\n\nWe can improve transformation further with better illumination estimation models (both uniform and non-uniform) and compare various chromatic adaptation transform models.\n\n### References\n\n- https://www.cell.com/current-biology/pdf/S0960-9822(07)01839-8.pdf\n- http://www.marcelpatek.com/color.html\n- http://www.brucelindbloom.com/index.html?Eqn_ChromAdapt.html\n- https://web.stanford.edu/~sujason/ColorBalancing/adaptation.html\n- https://in.mathworks.com/help/images/color.html\n- https://in.mathworks.com/matlabcentral/fileexchange/66682-chromadapt-adjust-color-balance-of-rgb-image-with-chromatic\n"},{"metadata":{"title":"Whiteboard Image Enhancement using OpenCV","description":"Enhance whiteboard images taken from mobile using OpenCV.","imgName":"whiteboard-enhance/whiteboard-image-enhancement.jpg","date":"Oct 19, 2021","tags":["image-processing","opencv","python"],"keywords":["whiteboard","whiteboard-enhance","image-enhance","image-processing","opencv","python","difference-of-guassian","dog","contrast-stretch","color-balance"],"id":"whiteboard-image-enhancement-opencv-python"},"content":"\n![Whiteboard image enhancement in Python](whiteboard-enhance/whiteboard-image-enhancement.jpg)\n\n###### Published on: **Oct 19, 2021**\n\n# Whiteboard image enhancement using OpenCV\n\nWhiteboard images generally contain less contrast and low brightness as they would be captured in mobile under normal room light conditions. Enhancing whiteboard images makes text readable and gives an image with high contrast and brightness.\n\nWe will apply different image-processing techniques to enhance whiteboard images using OpenCV in Python. From this [whiteboard-cleaner](https://gist.github.com/lelandbatey/8677901) gist that enhances whiteboard images using [ImageMagick](https://imagemagick.org/), we will implement those ImageMagick methods in Python.\n\nIn that script, the following ImageMagick functions were used to enhance whiteboard images\n\n```shell\n-morphology Convolve DoG:15,100,0 -negate -normalize -blur 0x1 -channel RBG -level 60%,91%,0.1\n```\n\nAbove command applies image enhancing functions in order\n\n- **-morphology Convolve DoG:15, 100, 0**: Difference of Gaussian (DoG) with kernel_radius=15, sigma1=100, and sigma2=0\n- **-negate**: Negative of image\n- **-normalize**: Contrast stretch image with black=0.15% and white=0.05%\n- **-blur 0x1**: Gaussian blur with sigma=1\n- **-level 60%,91%,0.1**: Stretch image with black=60% and white=91%, and Gamma correction by gamma=0.1\n\nAs I found some difficulty for exactly converting the ImageMagick C code to Python, I have changed the order and parameters that would give close results.\n\n---\n\nWe will apply series of image-processing methods and effects to enhance whiteboard images in the following order\n\n- Difference of Gaussian (DoG)\n- Negative effect\n- Contrast Stretching\n- Gaussian blur\n- Gamma correction\n- Color balance\n\nYou can find the full code in my Github repository [whiteboard-image-enhance](https://github.com/santhalakshminarayana/whiteboard-image-enhance)\n\n### Import packages and read image\n\n```python\nimport cv2\nimport numpy as np\n\nimg = cv2.imread('input.jpg')\n```\n\n![Whiteboard image input:=:70:=:Input Whiteboard image](whiteboard-enhance/whiteboard-image.jpg)\n\n### Difference of Gaussian (DoG)\n\n[Difference of Gaussians (DoG)](https://en.wikipedia.org/wiki/Difference_of_Gaussians) is the difference of two Gaussian kernel convoluted images. DoG image is obtained by subtracting two Gaussian blurred images with different kernel radius and variance.\n\nNormally $I_{dog}$ (DoG of image) is calculated by subtracting $I_{g1}$ and $I_{g2}$ which are convoluted images with two different Gaussian kernels. But ImageMagick applies convolution after subtracting and scaling two gaussian kernels.\n\nSo, we will first subtract two different Gaussian kernels, scale and normalize the dog-kernel to the zero-summing kernel (sum of all elements ~ 0.0) and then apply convolution.\n\n```python\ndef normalize_kernel(kernel, k_width, k_height, scaling_factor = 1.0):\n    '''Zero-summing normalize kernel'''\n\n    K_EPS = 1.0e-12\n    # positive and negative sum of kernel values\n    pos_range, neg_range = 0, 0\n    for i in range(k_width * k_height):\n        if abs(kernel[i]) \u003c K_EPS:\n            kernel[i] = 0.0\n        if kernel[i] \u003c 0:\n            neg_range += kernel[i]\n        else:\n            pos_range += kernel[i]\n\n    # scaling factor for positive and negative range\n    pos_scale, neg_scale = pos_range, -neg_range\n    if abs(pos_range) \u003e= K_EPS:\n        pos_scale = pos_range\n    else:\n        pos_sacle = 1.0\n    if abs(neg_range) \u003e= K_EPS:\n        neg_scale = 1.0\n    else:\n        neg_scale = -neg_range\n\n    pos_scale = scaling_factor / pos_scale\n    neg_scale = scaling_factor / neg_scale\n\n    # scale kernel values for zero-summing kernel\n    for i in range(k_width * k_height):\n        if (not np.nan == kernel[i]):\n            kernel[i] *= pos_scale if kernel[i] \u003e= 0 else neg_scale\n\n    return kernel\n\ndef dog(img, k_size, sigma_1, sigma_2):\n    '''Difference of Gaussian by subtracting kernel 1 and kernel 2'''\n\n    k_width = k_height = k_size\n    x = y = (k_width - 1) // 2\n    kernel = np.zeros(k_width * k_height)\n\n    # first gaussian kernal\n    if sigma_1 \u003e 0:\n        co_1 = 1 / (2 * sigma_1 * sigma_1)\n        co_2 = 1 / (2 * np.pi * sigma_1 * sigma_1)\n        i = 0\n        for v in range(-y, y + 1):\n            for u in range(-x, x + 1):\n                kernel[i] = np.exp(-(u*u + v*v) * co_1) * co_2\n                i += 1\n    # unity kernel\n    else:\n        kernel[x + y * k_width] = 1.0\n\n    # subtract second gaussian from kernel\n    if sigma_2 \u003e 0:\n        co_1 = 1 / (2 * sigma_2 * sigma_2)\n        co_2 = 1 / (2 * np.pi * sigma_2 * sigma_2)\n        i = 0\n        for v in range(-y, y + 1):\n            for u in range(-x, x + 1):\n                kernel[i] -= np.exp(-(u*u + v*v) * co_1) * co_2\n                i += 1\n    # unity kernel\n    else:\n        kernel[x + y * k_width] -= 1.0\n\n    # zero-normalize scling kernel with scaling factor 1.0\n    norm_kernel = normalize_kernel(kernel, k_width, k_height, scaling_factor = 1.0)\n\n    # apply filter with norm_kernel\n    return cv2.filter2D(img, -1, norm_kernel.reshape(k_width, k_height))\n```\n\nGet Difference of Gaussian (DoG) for image by calling _dog()_ function with **radius = 15, sigma_1 = 100, sigma_2 = 0**. Here **radius = 15** for both kernels, for first kernel, **sigma = 100**, and for second kernel, **sigma = 0**. Kernel with **sigma = 0** creates unity kernel means convolution with this kernel gives same image.\n\n```python\ndog_img = dog(img, 15, 100, 0)\n```\n\nAfter applying DoG, the resultant image looks like\n\n![Difference of Gaussian (DoG) Image:=:70:=:Difference of Gaussian (DoG) image](whiteboard-enhance/difference-of-gaussian.jpg)\n\n---\n\n### Negative Image\n\nFor $I_{dog}$, get a negative image which is just an inversion of colors (255 - image).\n\n```python\ndef negate(img):\n    '''Negative of image'''\n\n    return cv2.bitwise_not(img)\n```\n\n```python\nnegative_img = negate(dog_img)\n```\n\nThe result of the inversion image is\n![Negative image:=:70:=:Negative image](whiteboard-enhance/negative.jpg)\n\nImage content is not much visible as we inverted an image whose most of the pixels are black. So, to improve the contrast, we apply contrast-stretch enhancement for the negative image.\n\n---\n\n### Contrast Stretching\n\nContrast stretching of an image is the same as histogram equalization but we cap some percentage of pixel values to black (0) and white (255).\n\n```python\ndef get_black_white_indices(hist, tot_count, black_count, white_count):\n    '''Blacking and Whiting out indices same as color balance'''\n\n    black_ind = 0\n    white_ind = 255\n    co = 0\n    for i in range(len(hist)):\n        co += hist[i]\n        if co \u003e black_count:\n            black_ind = i\n            break\n\n    co = 0\n    for i in range(len(hist) - 1, -1, -1):\n        co += hist[i]\n        if co \u003e (tot_count - white_count):\n            white_ind = i\n            break\n\n    return [black_ind, white_ind]\n\ndef contrast_stretch(img, black_point, white_point):\n    '''Contrast stretch image with black and white cap'''\n\n    tot_count = img.shape[0] * img.shape[1]\n    black_count = tot_count * black_point / 100\n    white_count= tot_count * white_point / 100\n    ch_hists = []\n    # calculate histogram for each channel\n    for ch in cv2.split(img):\n        ch_hists.append(cv2.calcHist([ch], [0], None, [256], (0, 256)).flatten().tolist())\n\n    # get black and white percentage indices\n    black_white_indices = []\n    for hist in ch_hists:\n        black_white_indices.append(get_black_white_indices(hist, tot_count, black_count, white_count))\n\n    stretch_map = np.zeros((3, 256), dtype = 'uint8')\n\n    # stretch histogram\n    for curr_ch in range(len(black_white_indices)):\n        black_ind, white_ind = black_white_indices[curr_ch]\n        for i in range(stretch_map.shape[1]):\n            if i \u003c black_ind:\n                stretch_map[curr_ch][i] = 0\n            else:\n                if i \u003e white_ind:\n                    stretch_map[curr_ch][i] = 255\n                else:\n                    if (white_ind - black_ind) \u003e 0:\n                        stretch_map[curr_ch][i] = round((i - black_ind) / (white_ind - black_ind)) * 255\n                    else:\n                        stretch_map[curr_ch][i] = 0\n\n    # stretch image\n    ch_stretch = []\n    for i, ch in enumerate(cv2.split(img)):\n        ch_stretch.append(cv2.LUT(ch, stretch_map[i]))\n\n    return cv2.merge(ch_stretch)\n```\n\nFor each image channel, calculate cummulative histogram sum, and then cap pixels based on **black_point = 2** and **white_point = 99.5** percentage.\n\n```python\ncontrast_stretch_img = contrast_stretch(negative_img, 2, 99.5)\n```\n\nNegative image after contrast stretching is\n\n![Contrast stretch image:=:70:=:Contrast strech image](whiteboard-enhance/contrast-stretch.jpg)\n\n---\n\n### Gaussin Blur \u0026 Gamma Correction\n\nContrast stretching image contains noise, so blur the image with Gaussian kernel. As Gaussian distribution kernel can be linearly separable, we apply convolution with the same 1D-kernel along the x-axis and y-axis for performance (negligible for small kernels and low-res images).\n\n```python\ndef fast_gaussian_blur(img, ksize, sigma):\n    '''Gussian blur using linear separable property of Gaussian distribution'''\n\n    kernel_1d = cv2.getGaussianKernel(ksize, sigma)\n    return cv2.sepFilter2D(img, -1, kernel_1d, kernel_1d)\n\ndef gamma(img, gamma_value):\n    '''Gamma correction of image'''\n\n    i_gamma = 1 / gamma_value\n    lut = np.array([((i / 255) ** i_gamma) * 255 for i in np.arange(0, 256)], dtype = 'uint8')\n    return cv2.LUT(img, lut)\n```\n\nApply Gaussian blur with **kernel_size = 3** and **sigma = 1**.\n\n```python\nblur_img = fast_gaussian_blur(contrast_stretch_img, 3, 1)\n```\n\nBlurred image after noise suppression is\n\n![Gaussian blur image:=:70:=:Blurred image](whiteboard-enhance/gaussian-blur.jpg)\n\nNow apply Gamma correction to enhance the blurred image with **gamma_value = 1.1**.\n\n```python\ngamma_img = gamma(blur_img, 1.1)\n```\n\nBlurred image Gamma corrected looks like\n\n![Gamma correction:=:70:=:Gamma corrected image](whiteboard-enhance/gamma-correction.jpg)\n\n---\n\n### Color Balance\n\nColor balance of an image is same as contrast-stretching method above but they are different in implementation. Above contrast-stretching is an implementation based on [ImageMagick-ContrastStretchImage()](https://imagemagick.org/api/MagickCore/enhance_8c.html), and [color balance](https://gist.github.com/DavidYKay/9dad6c4ab0d8d7dbf3dc) is based on [Simplest Color Balance](https://www.ipol.im/pub/art/2011/llmps-scb/article.pdf).\n\n```python\ndef color_balance(img, low_per, high_per):\n    '''Contrast stretch image by histogram equilization with black and white cap'''\n\n    tot_pix = img.shape[1] * img.shape[0]\n    # no.of pixels to black-out and white-out\n    low_count = tot_pix * low_per / 100\n    high_count = tot_pix * (100 - high_per) / 100\n\n    cs_img = []\n    # for each channel, apply contrast-stretch\n    for ch in cv2.split(img):\n        # cummulative histogram sum of channel\n        cum_hist_sum = np.cumsum(cv2.calcHist([ch], [0], None, [256], (0, 256)))\n\n        # find indices for blacking and whiting out pixels\n        li, hi = np.searchsorted(cum_hist_sum, (low_count, high_count))\n        if (li == hi):\n            cs_img.append(ch)\n            continue\n        # lut with min-max normalization for [0-255] bins\n        lut = np.array([0 if i \u003c li\n                        else (255 if i \u003e hi else round((i - li) / (hi - li) * 255))\n                        for i in np.arange(0, 256)], dtype = 'uint8')\n        # constrast-stretch channel\n        cs_ch = cv2.LUT(ch, lut)\n        cs_img.append(cs_ch)\n\n    return cv2.merge(cs_img)\n```\n\nEnhance image by passing Gamma corrected image to **color_balance()** with parameters **low_per = 2** and **high_per = 1**.\n\n```python\ncolor_balanced_img = color_balance(gamma_img, 2, 1)\n```\n\nThe final enhanced whiteboard image is\n![Whiteboard image enhance:=:70:=:Whiteboard image enhanced](whiteboard-enhance/whiteboard-image-enhanced.jpg)\n\nYou can find out full code at my Github repository file [whiteboard_image_enhance.py](https://github.com/santhalakshminarayana/whiteboard-image-enhance/blob/main/whiteboard_image_enhance.py)\n\n---\n\n## Results\n\n![img_2:=:100](whiteboard-enhance/img_2.jpg)\n\n![img_3:=:100](whiteboard-enhance/img_3.jpg)\n\n![img_4:=:100](whiteboard-enhance/img_4.jpg)\n"},{"metadata":{"title":"Color Theory","description":"Different color properties, color models, and color space that are useful in image processing, graphic design, and game design.","imgName":"color-theory/color-theory.jpg","date":"Sep 14, 2021","tags":["color-science"],"keywords":["color","color-theory","color-models","color-spaces","graphic-design","image-processing","photography"],"id":"color-theory"},"content":"\n![Color Properties, Models and Spaces](color-theory/color-theory.jpg)\n\n###### Published on: **Sep 14, 2021**\n\n# Color Theory\n\n\u003e Before reading color theory, it is recommended to know about color science that explains how color is defined, represented, and quantified in my previous article about [color science](https://santhalakshminarayana.github.io/blog/color-science)\n\nThe Color theory deals with multiple color terminology, models, and schemes that we use in our daily day-to-day life. The color theory describes relationships between different colors and color schemes. It acts like guidelines for better marketing and design of a product. Color theory is a set of rules that combines art and science. The color theory defines the logical structure of color to create color palettes, color schemes, aesthetically pleasing color designs, and color psychology.\n\nFrom an e-commerce website to an offline store, still photography to a motion picture, interior design to an art gallery, we can see the usage of color theory to attract, please, or astonish the customers.\n\nIn fields like image processing, digital photography, graphic design, and game design, color plays a vital role and one must understand basic color theory concepts like color properties, color models, and color spaces before learning other concepts.\n\n## Color Wheel\n\nA color wheel is a set of colors distributed over a circular disc with some rules that signify the meaning of colors. A color wheel can have numerous colors, and generally, a color wheel is a combination of 12 basic colors from primary, secondary, and teritary colors.\n\nHue, saturation and lightness are three elements used to define a color.\n\n## Hue\n\nHue is the dominant color among similar colors in a range of the visible spectrum. Hues are the basic colors to which adding white and black produce multiple colors. Hues are generally classified into major colors we see in nature. They are Red, Yellow, Green, Blue, Violet, and Orange.\n\n**Primary colors** - Primary hues are colors that cannot be produced by mixing other hues. In painting world **red**, **yellow**, and **blue** are primary hues. In digital world **red**, **green**, and **blue** are primary colors.\n\n**Secondary colors** - Mixing two primary hues creates secondary hues:\n\n- **Green** - (Blue + Yellow)\n- **Orange** - (Yellow + Red)\n- **Purple** - (Red + Blue)\n\n**Tertiary colors** - In the color wheel, tertiary hues are mixtures of adjacent primary and secondary hues. Thus, six territory hues are produced.\n\n- **Chartreuse** - (Yellow + Green)\n- **Teal** - (Green + Blue)\n- **Violet** - (Blue + Purple)\n- **Magenta** - (Purple + Red)\n- **Vermillion** - (Red + Orange)\n- **Amber** - (Orange + Yellow)\n\n![Color Wheel - Primary, Secondary, and Tertiary Hues:=:50:=:Color wheel](color-theory/color-wheel.jpg)\n\n## Saturation/Chroma\n\nSaturation/Chroma is the purity of a hue. It defines the color intensity. A pure color is 100% saturated or fully saturated. Mixing gray (white + black) color to a pure hue decreases the intensity and finally reaches a 0% saturation level. A color can be desaturated by mixing with its complementary color. A hue at 0% saturation is a gray color. Saturation defines a range of color intensities from pure color (100%) to gray (0%).\n\n![Saturation:=:70](color-theory/saturation.jpg)\n\n### Tint\n\nWhen white color is mixed with a color, the resultant is the Tint of that color. Adding white color makes color lighter than the original. The amount of white added defines the tint of color.\n\n![Tints:=:70](color-theory/tints.jpg)\n\n### Shade\n\nA shade of a color is the blackness of the color after being mixed with black color. The shade of a color is darker than the original color.\n\n![Shades:=:70](color-theory/shades.jpg)\n\n### Tone\n\nA Tone is mixing both white and black colors to a color. The resultant color is grayer than the original.\n\n![Tones:=:70](color-theory/tones.jpg)\n\nAlthough Chroma and Saturation speak the same, Chroma is an absolute term that defines color value a range from gray (0%) to pure hue (100%), and saturation defines the brilliance of a color relative to gray.\n\n## Lightness/Luminance\n\nLightness/Luminance define how light (100%) or dark (0%) a color is. All pure hues have a lightness value of 50%. Lightness is measured relative to the brightness of white color. In color perception, lightness tells how much light is reflected from a surface.\n\n![Lightness:=:70](color-theory/lightness.jpg)\n\n---\n\n## Color properties and color terms\n\n### Chromatic \u0026 Achromatic colors\n\nAchromatic colors are colors that have lightness but no hue or saturation. They are black, white and gray. Mixing complementary colors produces achromatic colors.\n\nChromatic colors are the ones that have any amount of hue and saturation. The presence of saturation defines chromatic colors. All other colors except achromatic colors are chromatic colors.\n\n### Chromatic value/Chromatic intensity/Chromaticity\n\nChromatic value/intensity is the measure of how light or dark a color is. Depending on how light or dark a hue is, Tints and Shades of color are created. Chromaticity is similar to saturation as they both speak about how much color is chromatic (a hue with some saturation). Chromaticity is also defined as color independent of lightness (only hue and saturation).\n\n### Luminance\n\nLuminance is the intensity of light emitted, reflected, or passes through per unit area in a given direction.\n\n### Brightness\n\nBrightness is a relative term that describes how much light is shining from something. It is the average lightness of an object. It is not a color term but a perception of our eyes created by color's lightness. It is not a quantitative term but can be scaled (%). It is quite similar to luminance in human color vision. [Luminance](http://www.workwithcolor.com/color-luminance-2233.htm) is the brightness of a color perceived by our eyes. A high-intensity color looks more bright. The green color looks more bright than the blue color. A hue with more saturation is brighter than a desaturated hue. A color with the same luminance appears in different brightness depending on the surrounding colors.\n\n### Contrast\n\nContrast is the difference between the luminance of two or more colors placed adjacent or overlapped. High contrast colors appear more bright than low contrast. Generally, contrast is used in the context of foreground and background colors. When equal luminance colors are placed on foreground and background, they look similar and there is no difference observed. Depending on the position of colors in the color wheel, contrast levels vary. Colors next to each other have low contrast and opposite colors have high contrast.\n\n### Color contrast ratio\n\nColor contrast ratio is the quantitative term to define the contrast between two colors. They are measured based on the relative luminance of different colors. The [Web Content Accessibility Guidelines (WCGA)](https://www.w3.org/WAI/WCAG22/Understanding/contrast-minimum.html) published guidelines for calculating contrast ratio of color and these are considered as standard rules in web design.\n\n### Luma\n\nLuma describes the presence of achromatic signal (white, gray, and black) intensity in a color. Luma describes how bright a color is. The difference between luminance and luma is, luminance is derived from linear RGB values, whereas luma is derived from non-linear (gamma-corrected) RGB values. Luminance is based on standard [relative luminance ratios of RGB](https://santhalakshminarayana.github.io/blog/color-science#relative-luminance), and luma is based on relative gamma-corrected RGB ratios.\n\n### Metamerism (Color metameres)\n\nMetamerism occurs when two colors look the same under one lighting condition but, different when light source/conditions change. This usually happens when we buy clothes in a store like a shirt appears light blue in the store but when we brought it in sunlight the shirt looks pale blue.\n\n### Color Temperature\n\nThe color temperature of the light source is the temperature of black-body radiation when black-body is heated at a constant temperature. As the black body observes all spectrum wavelengths, when heated up at a constant temperature, it emits certain wavelengths which are perceived as color. Color temperature is measured in Kelvin (\u0026degK). At different color temperatures, the body is seen as different colors.\n\n![Color Temperature:=:60:=:Colors at different temperatures emitted by a black-body](color-theory/color-temperature.jpg)\n\nThe lower the temperature, the lower the color appears warm, and the higher the temperature, the cooler the color appears.\n\n### Color Rendering Index (CRI)\n\nColor Rendering Index (CRI) is a measurement of how light affects the appearance of a color. It defines how accurately colors can be distinguished under a light source. Light sources with different color temperatures illuminate objects as different colors and sometimes we cannot distinguish object colors in extremely warm or cool light sources. CRI indicates how well a light source can reproduce colors with the same temperature for natural light such as the sun. It is a quantitative term measured on a scale of 0-100. A score of 90 or more is considered excellent and less than 80 is considered poor.\n\nColor temperature and Color Rendering Index (CRI) are two vital terms that affect the selection of light sources and conditions. Art galleries, museums, photographers, and product displays stores choose light sources based on these terms.\n\n---\n\n## Color Harmony or Color schemes\n\nColor harmony is the theory of selecting/combining colors to create a visually pleasing color scheme. Color harmony creates aesthetically rich color combinations that maintain harmony and engages users with the product. The color wheel is the basic logical structure for creating color schemes.\n\n### Monochromatic\n\nThe monochromatic color scheme contains single color with variations of tints, tones, and shades.\n\n![Monochromatic Harmony:=:25:=:Monochromatic Harmony](color-theory/monochromatic.jpg)\n\n### Analogous\n\nAnalogous color schemes are created by grouping the main color with adjacent colors to it in the color wheel.\n\n![Analogous Harmony:=:25:=:Analogous Harmony](color-theory/analogous.jpg)\n\n### Complementary\n\nComplementary colors are two colors selected in a way that colors are opposite to each other in the color wheel.\n\n![Complementary Harmony:=:25:=:Complementary Harmony](color-theory/complementary.jpg)\n\n### Split Complementary\n\nIn the split complementary scheme, the main color is selected and the other two colors are adjacent colors to the complementary color of the main color.\n\n![Split Complementary Harmony:=:25:=:Split Complementary Harmony](color-theory/split-complementary.jpg)\n\n### Triad\n\nTriad is a group of three colors that are equally distant from each other in the color wheel and form a triangle.\n\n![Triad Harmony:=:25:=:Triad Harmony](color-theory/triad.jpg)\n\n### Square\n\nThe square color scheme has four colors that are the same distance to each other forming a square or rhombus in the color.\n\n![Square Harmony:=:25:=:Square Harmony](color-theory/square.jpg)\n\nOther harmonies include diad, diad complementary, rectangle, and polygon schemes.\n\n## Color models\n\nColor models are the organization of colors and their mixing (additive or subtractive) based on human color perception. A color model defines color and its properties in a mathematical way.\n\n### RGB\n\nIn the RGB color model, Red, Green, and Blue are three primary colors that are mixed to produce all other colors by [additive color mixing](https://santhalakshminarayana.github.io/blog/color-science#additve-subtractive-color-mixing). Display systems and digital photography stores color values based on RGB model.\n\nBased on the depth (no. of bits allocated to store) of a color, RGB values are scaled in a certain range. For an 8-bit system, the range is 0-255, and when all colors are mixed at minimum level then it designates black color and at maximum level designates white color. If RGB colors are mapped to 3-dimensional space with unit vectors X, Y, and Z-axis as R, G, and B colors, then the vector space forms a cube with all possible colors in it formed by the combination of RGB at different values.\n\n![RGB color model:=:35](color-theory/rgb-cube.jpeg)\n\nWith the RGB model, one cannot easily identify important color terms like lightness, contrast, brightness, tones, tint, and shades.\n\n### HSV (HSB)\n\nHue, Saturation, and Value (or Brightness) (HSV or HSB) is a cylindrical color model that is based on hue, saturation, and brightness described above.\n\nHue is arranged with all hues taking up a certain range in a 0-360 degrees circle. Starting red at 0\u0026deg, green at 120\u0026deg, blue at 240\u0026deg, and again wrapping back to red at 360\u0026deg.\n\nSaturation has a range from 0%-100%, starting from 0% (gray) at the center of the base to 100% (pure color/hue) at the circumference. Saturation controls tints and shades\n\nValue/Brightness also has a range of 0%-100%, with 0 (black) at the bottom of the base to 100% (no black) at the top.\n\n![HSV:=:35](color-theory/hsv.jpeg)\n\nIf brightness is at 0% then the color is black irrespective of hue and saturation. But for white color, the brightness value is 100% and saturation is kept at 0% while hue can be anything.\n\n### HSL\n\nHue, Saturation, and Lightness (HSL) is a bi-cone (double) model while H and S are the same as HSB.\n\n![HSL Cone:=:35](color-theory/hsl-cone.jpeg)\n\nLightness has a range from 0%-100%, starting from 0% (black) at bottom of the cone to 100%(white) at top of the cone. All pure hues have a lightness value of 50%.\n\n![HSL:=:35](color-theory/hsl.jpeg)\n\nColor is white if lightness is 100% irrespective of hue and saturation. Same for black color with lightness at 0% irrespective of hue and saturation values.\n\n## Color spaces\n\nA color space is based on the color model that maps colors to a set of colors like sRGB and Adobe RGB and is reproduced by display systems.\n\n### L\\*a\\*b\\* (CIELAB) color space\n\nL\\*a\\*b\\* color space is a device-independent color space in which color is expressed in three components:\n\n- **L\\***- Lightness\n- **a\\***- Green-Red\n- **b\\***- Blue-Yellow\n\nIt was defined by CIE and also called CIELAB (Lab without \\* is color space defined by Hunter Lab).\n\nIn L\\*a\\*b\\* color space, L indicates lightness, and a\\* and b\\* indicates chromaticity coordinates. Unlike RGB and HSV, L\\*a\\*b\\* represents colors in the sphere where L\\*, a\\*, and b\\* are orthogonal axis to each other. CIELAB is a perceptually uniform color space i.e. a standard color system that reflects the color representation close to human vision color perception.\n\n![Lab color space:=:35:=:Lab sphere$$$https://sensing.konicaminolta.asia/what-is-cie-1976-lab-color-space/](color-theory/3d-lab.jpg)\n\nL\\*a\\*b\\* is based on [opponent process theory](https://santhalakshminarayana.github.io/blog/color-science#opponent-process-theory) where red-green and blue-yellow form opponent pairs.\n\nL\\*-axis is a positive axis with coordinates black at 0 to white at 100.\na\\*-axis is present along horizontal from left to right in the range of -128 to 127, +a\\* is the red axis, and -a\\* is the green axis.\nb\\*-axis runs along top to down in the range of -128 to 127, +b\\* axis is the yellow axis, and -b\\* axis is the blue axis.\n\n![Lab color space:=:40:=:Lab color space$$$https://www.xrite.com/blog/lab-color-space](color-theory/lab-color-space.jpg)\n\nAt the center, colors are achromatic colors, and saturation increases moving towards the circumference.\n\n### YUV and YCbCr\n\n[YUV](https://en.wikipedia.org/wiki/YUV) and [YCbCr](https://en.wikipedia.org/wiki/YCbCr) are a family of color spaces used to encode color data. In both models, the Y component refers to luminance but practically Y'UV and Y'CbCr are used. Y' component is luma which is scaled in the range of 0-100. The other two components are chrominance components derived by the difference of Y' on blue and red values.\n\nYUV, historically used for transmitting video data for analog devices. In the olden days, as there was a need to support both black and white display and color displays, a conversion has be to applied for RGB data to transmit signals without disturbing black-and-white channels. As the black-and-white image is the same as the luminance channel, YUV was developed to pass chrominance channels along with luminance. Later, YCbCr was developed to store, compress and encode digital photographs. YUV is for analog TVs and YCbCr is for digital TVs.\n\nUsing [chroma subsampling](https://www.matrox.com/en/video/media/guides-articles/introduction-color-spaces-video), image data can be transmitted and stored at a low-bit rate for chrominance channels UV and CbCr, because human eyes are tolerant for perceptually equal colors. This made transmitting or storing image data in [Y'UV or Y'CbCr](https://docs.microsoft.com/en-us/windows/win32/medfound/about-yuv-video) requires less space: a high bit-rate luma channel and low bit-rate chrominance channels.\n\nConversion from **RGB** to **Y'UV** is computed as follows:\n\n$$\nY' = W_{R}R + W_{G}G + W_{B}B\n$$\n\n$$\nY' = 0.299R + 0.587G + 0.114B\n$$\n\n$$\nU = U_{max}\\frac{B - Y'}{1 - W_B}\n$$\n\n$$\nV = V_{max}\\frac{R - Y'}{1 - W_R}\n$$\n\nWhere $W_R$, $W_G$, and $W_B$ are relative ratios of non-linear gamma-corrected RGB values. $U_{max}$ and $V_{max}$ are maximum values of the numerical range selected. For PAL and NTSC, these values were $U_{max} = 0.436$ and $V_{max} = 0.615$.\n\nSimilarly, **RGB** to **Y'CbCr** is computed as\n\n$$\nY' = K_{R}R + K_{G}G + K_{B}B\n$$\n\n$$\nY' = 0.299R + 0.587G + 0.114B\n$$\n\n$$\nC_b = \\frac{1}{2}\\frac{B - Y'}{1 - K_B}\n$$\n\n$$\nC_r = \\frac{1}{2}\\frac{R - Y'}{1 - K_R}\n$$\n\nWhere $K_R$, $K_G$, and $K_B$ are relative ratios of non-linear gamma-corrected RGB values and satisfies $K_R + K_G + K_R = 1$.\n\n---\n\n## Device-dependent and device-independent color spaces\n\n**Device-dependent** color models mean they depend on the display system subset of colors or reference color space. This model explains the physical device output rather than human color vision. Coordinates used to display colors in device-dependent models change when the display system changes its light source. RGB, CMY, HSV, and HSL are device-dependent color models.\n\n**Device-independent** color spaces are universal references and coordinates used to specify colors will produce the same color in all conditions because they define color output based on human color vision. These are used to convert device-dependent color spaces across display devices. CIELAB, CIELUV, YCbCr, and YUV are device-independent color spaces.\n"},{"metadata":{"title":"Color Science","description":"Understanding the concept of Color from human eye's perception to digital world representation.","imgName":"color-science/color-science.jpg","date":"Aug 28, 2021","tags":["color-science"],"keywords":["color-matching","color","chromaticity-diagram","cie","graphic-design"],"id":"color-science"},"content":"\n![Color Science](color-science/color-science.jpg)\n\n###### Published on: **Aug 28, 2021**\n\n# Color Science\n\nColor is an important element that exists in nature. Everything we see or feel through the eyes is all about colors. It is necessary to understand the concept of color in domains like image processing, film making, and digital photography, where color is the primary element.\n\nFrom Space exploration to Film making, Air force to Archeology, color science is applied everywhere that uses colors to gather data. For example Hubble telescope uses image processing techniques like **broad-band** and **narrow-band** filtering to color map planets, nebula, and galaxies based on the gases and their interactions like in \"The Pillars of Creation\".\n\n![Pillars of Creation:=:40:=:The Pillars of Creation (NASA)$$$https://www.nasa.gov/image-feature/the-pillars-of-creation](color-science/pillars-of-creation.jpg)\n\nIn this article, we discuss color, the evolution of color understanding, and numerical representation of color in the digital world.\n\n---\n\n## Color and Human Vision\n\nWhen we talk about colors, colors are represented in a way human eyes perceive. Because different species see things in different colors, and they can also see what humans cannot see like Snakes and Bats can detect infrared radiation which humans couldn't. So we generally restrict colors to human vision.\n\nColor is an illusion that our brain creates when a light beam reflects off an object/emits from an object and reaches our eyes. When light strikes the surface of an object, some light will be absorbed and some will be reflected. That reflected light with different wavelengths reaches our eyes and we perceive them as different colors.\n\nIssac Newton described the color as a quality of light. And the light which is an electromagnetic wave is classified into different segments with certain frequencies and wavelength ranges. The visible light spectrum is the segment of the electromagnetic spectrum that human eyes can only perceive. The visible light wavelength range is ~380 to ~780 nanometers. By deflection of light through a prism, Newton assigned different colors to different wavelengths in the visible spectrum which we generally see when a rainbow appears (VIBGYOR).\n\n![Visible Spectrum:=:55:=:Visible Light Spectrum$$$https://en.wikipedia.org/wiki/Electromagnetic_radiation](color-science/visible-spectrum.jpg)\n\nThe retina in our eye is responsible for vision. The retina contains photoreceptor cells that convert light signals to neural signals and send those signals back to the brain through nerves. And then, the brain intercepts these signals to colors. Rods and Cones in the retina are two types of photoreceptors that are responsible for our vision in dark and bright conditions. Rods work at a low level of light and our vision is in grayscale. Rods don't provide any color vision. At night as there is a low level of light we can see the objects in combinations of white and black (grayscale). Cones work in bright light and provides color vision.\n\n![Retina:=:70:=:Light capture by Rods and Cones in Retina$$$https://www.xrite.com/blog/color-perception-part-3](color-science/retina-rods-cones.jpg)\n\n### Triochromatic theory\n\nIn the 1800s, Thomas Young stated that the human eye consists of three different types of color receptors more likely red, green, and blue, and mixing these colors create other colors.\n\nThere are three types of cones called short (S), medium (M), and long (L) which are sensitive to different wavelength ranges of spectral distribution and detect Blue, Green, and Red respectively.\n\n![Normalized Cone Responsivity:=:45:=:Normalized Responsivity$$$https://en.wikipedia.org/wiki/Cone_cell^^^ of S, M and L cones](color-science/normalized-cone-responsivity.jpg)\n\nThese S, M, and L cones detect Blue, Green, and Red colors respectively and other colors are perceived by overlapping of different stimulations of these cones. The brain then integrates these cone signals and detects millions of colors. For example, Yellow color is a proportion of Green and Red cones and no or less effect of Blue cone. These three colors RGB (Red, Green, Blue) are primary colors and any color can be produced with the combination of these primitive colors.\n\n![Retinal Response:=:40:=:Retinal Response$$$https://askabiologist.asu.edu/rods-and-cones^^^ of rods and cones](color-science/retinal-response.jpg)\n\nIf we mix all RGB colors at a high-intensity level we get White color, and if RGB colors with 0 intensity level produce Black Color.\n\n### Opponent process theory:=:opponent-process-theory\n\nAfter cones converting physical (light) signals to neural signals, these neural signals reach the brain through nerves. Here, cells changes behavior and responds in the opponent manner for colors. The photoreceptor cells are in inter-connection with each other cell and give positive or negative responses for incoming color signals. Some cells fire positive signals when seeing red color, and activate negative signals for green color. These opposite responses don't happen at the same time in a cell. The opponent-process theory states that color perception is controlled by three opponent color systems: red-green, blue-yellow, and white-black (according to recent studies, these pairs are blue-yellow, red-cyan, and green-magenta). These opponent colors don't perceive as together because cells can only fire one of the colors in a pair i.e. there is no \"bluish-yellow\" or \"greenish-red\".\n\n![Opponent Process:=:60:=:Opponent Process$$$https://en.wikipedia.org/wiki/Opponent_process](color-science/opponent-process-theory.jpg)\n\nThese trichromatic and opponent-process theories give an idea about how our brain receives and processes light signals into colors.\n\n## The Color of an object\n\nWhen a light beam hits the surface of an object, it absorbs some wavelengths and reflects a particular wavelength of light. An object looking Pink absorbs all wavelengths of light except some portion of Red and Blue. And our eyes receive those Red and Blue colors by cones and intercept as pink by stimulating the Red and Blue cones. Paints on walls are made to absorb all of the wavelengths except the color of their appearance.\n\n### Secondary colors\n\nIf we mix two primary colors at equal intensities and another one is being kept at a 0 level, we get secondary colors.\n\n- Cyan (Blue + Green)\n- Magenta (Red + Blue)\n- Yellow (Green + Red).\n\nThese Cyan (C), Magenta (M), and Yellow (Y) colors are called secondary colors (CMY) as they are derived from primary ones (RGB).\n\nC, M, and Y are also complementary colors to R, G, and B as synthesize of C, M and Y don't contain R, G, and B colors respectively. This means when an object is illuminated with RGB colors, the object absorbs at least one primary color and reflects the other two primary colors. An object looking Cyan would absorb Red but reflects Green and Blue (G + B = C). Mixing secondary colors produces RGB colors.\n\n- M + Y = R\n- Y + C = G\n- C + M = B\n\n### Additive and Subtractive color mixing:=:additve-subtractive-color-mixing\n\nThe object appears in different colors if the various amount of RGB light emitted from it. It appears black when no percentage of RGB is emitted and if all are emitted at the highest intensity, it appears as white. Thus adding different RGB percentages produce different colors. This process of synthesizing colors by emitting RGB colors from black (when no light is produced) is called **Additive** color model. We start from black and reach white. Display monitors and screens we see around are based on an Additive color model. They produce colors by varying RGB color intensities.\n\nWhat if an object instead of emitting it reflects certain wavelengths of light by absorbing other wavelengths. This is how we naturally see an object and its appearance. An object absorbs some light and reflects other visible spectrum wavelengths that are perceived as color by our eyes. If an object absorbs cyan color, then it reflects red. Because\n\n$$\n\\small{White - Cyan = (R + G + B) - (G + B) = R}\n$$\n\nif we subtract cyan from white it produces a red color.\n\n![Additive and Subtractive Colors:=:40](color-science/additive-subtractive-colors.jpeg)\n\nHere subtracting means mixing colors as an object absorbs colors and then reflects. Thus mixing CMY colors with a white color produces different colors. This process of synthesizing colors from adding CMY mixtures to white is called **Subtractive** color model. We start from white to black color. Printing and painting involve this color synthesis to generate different colors on white paper or canvas. As mixing of CMY at the highest intensity produces brown color, printers also use black (K) color to generate shades of the black, and the resultant color group is called as CMYK color model.\n\n---\n\n## Color matching\n\nColor matching is representing and reproducing any color wavelength using primary monochromatic wavelengths. Using different intensities of primary colors, the target color wavelength is produced.\n\n### Grassmann's laws of additive color mixture\n\nIn 1953, Grassmann recognized that any color can be matched with a linear combination of three primary colors. Grassmann's laws describe the relations between primary colors to match any color by additive color mixture. The following Grassmann's laws are fundamental for color mixing,\n\n- If two colors (X and Y) are the same, then mixing X and Y with the third color Z would still look like same.\n\n  $$\n  \\footnotesize{If \\space X = Y, \\space then \\space X + Z = Y + Z}\n  $$\n\n- Any color C can be produced by a linear combination of three primary colors but no primary color is matched by a combination of the other two.\n\n  $$\n  \\footnotesize{C = xX + yY + zZ}\n  $$\n\n  where (x, y, z) are portions of primary colors (X, Y, Z) required to match color C.\n\n- Two colors C1 and C2, if mixed to form another color C3, then C3 can be matched by a linear combination of mixtures of primary colors that are used to produce C1 and color C3.\n\n  $$\n  \\footnotesize{C3 = C1 + C2 = (xX1 + yY1 + zZ1) + (xX2 + yY2 + zZ2)}\n  $$\n\n- Colors that are produced by mixing primary colors have constant luminance. This is not true at various lighting conditions like in photopic (daylight) vision, objects appear in different colors, but in scotopic (night) vision, objects appear in grayscale.\n\n### Human tristimulus response\n\nIn the 1860s, James Maxwell stated that using RGB primary colors, all other colors can be generated but it is not possible to generate all colors only by addition and requires subtraction also to match certain colors. This is called the human tristimulus response.\n\n### 2\u0026deg Standard Observer color-matching experiment\n\nIn the late 1920s, based on the works of Newton, Grassmann, and Maxwell, David Wright and John Guild independently conducted experiments to quantify the color reception ability of a normal human observer. They believed that human color receptors are located within the 2\u0026deg arc of the fovea back from the retina, and asked standard observers or human volunteers to look through a hole that provides a 2\u0026deg field of view. They asked volunteers to match the target color by adjusting combinations of red, blue, and green colors.\n\nAs not all colors could be matching using an additive color model of red, blue, and green, the target color is mixed with some portion of the primary color, and the other two primary colors are altered in portions to match the target colors.\n\nThe Commission International de l’Eclairage (CIE), based on Wright-Guild's 2\u0026deg standard observer data, published RGB color matching functions to represent colors as a combination of three primary colors. This is also called **CIE standard 2\u0026deg observer**.\n\n![2° and 10° Standard Observer:=:40:=:2° and 10° standard observer view$$$https://support.hunterlab.com/hc/en-us/articles/203420099-CIE-Standard-Observers-and-calculation-of-CIE-X-Y-Z-color-values-AN-1002b](color-science/standard-observer.jpg)\n\n### 10\u0026deg Supplementary Standard Observer color-matching experiment\n\nIn the 1960s, researchers observed that cones present in the retina cover a larger field than the standard 2\u0026deg view. Three researchers, Stiles, Burch, and Speranskaya again repeated color matching experiments with a 10\u0026deg field view. They believed 10\u0026deg color matching experiments would cover more spectral response than 2\u0026deg. And it was right. In 1964, the CIE published 10\u0026deg Standard Observer based on a 10\u0026deg color-matching experiment. The 10\u0026deg standard observer is recommended than 2\u0026deg as it covers larger color representatives.\n\n## CIE 1924 $\\small{V(\\lambda)}$ Spectral Luminous Efficiency function\n\nHuman eyes perceive different wavelengths of the visible spectrum at different brightness levels for the same radiance energy. Sensitivity is zero at either end of the spectrum thus we receive 0 brightness and no color.\n\nThe luminous efficiency function describes the human eye's sensitivity for different wavelengths. The more sensitivity observed the more brightness the color is. Eyes are most sensitive at **555nm** at daylight (photopic) and **507nm** at night (scotopic). CIE $\\small{V(\\lambda)}$ luminosity curve tells the relative sensitivity of the human eye for different color wavelengths. The luminous efficiency function distinguishes the brightness level of two colors at equal luminous. We can derive relative brightness for different colors with $\\small{V(\\lambda)}$ function.\n\n![Luminous efficiency function:=:50:=:Photopic (black) and Scotopic (green) luminosity function$$$https://en.wikipedia.org/wiki/Luminous_efficiencG_function](color-science/luminosity-curve.jpg)\n\nFrom $\\small{V(\\lambda)}$ luminous efficiency function, we can observe that, for equal radiance energy, brightness order of RGB is **green\u003ered\u003eblue**. For two colors at **500nm** and **570nm** to look as equal brightness, the color at **500nm** should be more luminous than **570nm**.\n$\\small{V(\\lambda)}$ function is an approximation function and accurate in some cases (like color blind people).\n\n### Relative luminance of RGB primaries:=:relative-luminance\n\nThe above CIE RGB color matching functions are scaled in assumtion that all colors have same brightness. The $\\bar{r}$, $\\bar{g}$ and $\\bar{b}$ are normalized to have equal area under curve to yield $\\bar{r}(\\lambda) = \\bar{g}(\\lambda) = \\bar{b}(\\lambda) = 1$. The integrated area is assumed to be same that\n\n$$\n\\int \\bar{r}(\\lambda)d\\lambda = \\int \\bar{g}(\\lambda)d\\lambda = \\int \\bar{b}(\\lambda)d\\lambda = \\int V(\\lambda)d\\lambda\n$$\n\nSolving the above expression gives the relative luminanace ratios for **r : g : b = 1 : 4.5907 : 0.0601**. If we compare $V(\\lambda)$ luminous curve for **R = 700nm**, **G = 546.1nm** and **G = 435.8nm**, (r, g, b) luminance ratios are approximately matches.\n\nIf ratios are normalized such that sum of ratios equals to 1, then **r : g : b = 0.2126 : 0.7152 : 0.0722**.\n\nTo match any target color, RGB colors are mixed in portions obtained from the CIE RGB color-matching function, and then those resultant RGB values should multiply by relative luminance ratios to get the target color with exact brightness.\n\n## CIE 1931 RGB color matching functions\n\nThe **Color matching functions** are mathematical estimation of color response of each primary color relative to human observer in 2\u0026deg view field. Color matching functions gives amount of primary color energy required to generate target color wavelength. CIE defined three color mathcing functions $\\small{\\bar{r}(\\lambda), \\space \\bar{g}(\\lambda) \\space and \\space \\bar{b}(\\lambda)}$ which are normalized weight factors.\n\n![CIE 1931 RGB color matching functions:=:50](color-science/cie-rgb-cmf.jpg)\n\nIn the above RGB cmf diagram, each color wave represents RGB spectral intensities at various wavelengths. The three primary colors have peak sensitivity at wavelengths **R = 700nm**, **G = 546.1nm** and **G = 435.8nm**. The distribution of spectral energy is normalized and adding all color matching functions equals 1. Negative values in the graph indicate primary colors have to be mixed with the target color before color matching. And $\\small{\\bar{r}(\\lambda)}$ slightly matches the $\\small{V(\\lambda)}$ luminous efficiency function.\n\n### RGB Tristimulus values\n\nThe **tristimulus values** are, in a color space, the amount of color proportions of primary colors are required in a trichromatic additive color model to produce a color.\n\nFor CIE 1931 RGB color space, the RGB tristimulus values for spectral distribution of color $\\small{S(\\lambda)}$ can be calculated as\n\n$$\n\\small{R = \\int S(\\lambda)\\bar{r}(\\lambda)d\\lambda \\hspace2ex\nG = \\int S(\\lambda)\\bar{g}(\\lambda)d\\lambda \\hspace2ex\nB = \\int S(\\lambda)\\bar{b}(\\lambda)d\\lambda}\n$$\n\nwhere $\\small{S(\\lambda)}$ is spectral intensity of a color.\n\nIt can also be expressed as, for a color $\\small{C}$,\n\n$$\n\\small{C = R\\bar{r}(\\lambda) + G\\bar{g}(\\lambda) + B\\bar{b}(\\lambda)}\n$$\n\n$$\n\\small{C = R\\bold{R} + G\\bold{G} + B\\bold{B}}\n$$\n\nwhere $\\small{R}$, $\\small{G}$ and $\\small{B}$ are tristimulus values, and scaled to unit length.\n\n## CIE XYZ color matching functions\n\nRGB color matching functions contain negative intensities in mathematical form but they are not practically suitable for the physical world. CIE then converted RGB color space to XYZ color space where XYZ are linear combinations of monochromatic colors RGB. The new XYZ color matching functions have only positive intensities. The derived XYZ parameters are imaginary and they roughly represent S, M, and L cones.\n\n![CIE XYZ color matching functions:=:60](color-science/cie-xyz-cmf.jpg)\n\n$\\small{\\bar{x}(\\lambda), \\bar{y}(\\lambda) \\space and \\space \\bar{z}(\\lambda)}$ are color matching functions. From above diagram, we can say Violet (around 450nm) color is a mixture of Red and Blue which is indeed true and this cannot be seen in RGB color matching function curves.\n\n### CIE XYZ tristimulus values calculation\n\nFor light source with spectral radiance $\\small{L(\\lambda)}$,\n\n$$\n\\small{X = \\int L(\\lambda)\\bar{x}(\\lambda)d\\lambda \\hspace2ex\nY = \\int L(\\lambda)\\bar{y}(\\lambda)d\\lambda \\hspace2ex\nZ = \\int L(\\lambda)\\bar{z}(\\lambda)d\\lambda}\n$$\n\nwhere $$\\lambda$$ is the equivalent monochromatic light (nm) for a range of wavelengths [380, 780]nm.\n\nFor reflective or transmissive object case when illuminated by light source, replace spectral radiance by spectral reflectance (or transmittance) $\\small{S(\\lambda)}$, multiplied by the spectral power distribution of the illuminant $\\small{I(\\lambda)}$\n\n$$\n\\small{X = \\frac{K}{N}\\int S(\\lambda)I(\\lambda)\\bar{x}(\\lambda)d\\lambda}\n$$\n\n$$\n\\small{Y = \\frac{K}{N}\\int S(\\lambda)I(\\lambda)\\bar{y}(\\lambda)d\\lambda}\n$$\n\n$$\n\\small{Z = \\frac{K}{N}\\int S(\\lambda)I(\\lambda)\\bar{z}(\\lambda)d\\lambda}\n$$\n\nwhere $$K$$ is scaling factor (1-100), and\n\n$$\n\\small {N = \\int I(\\lambda)\\bar{y}(\\lambda)d\\lambda}\n$$\n\nDue to absence of analytical expressions of color matching functions, the above integration is transformed to summation of equal intervals of **10nm** over spectrum wavelength range [380, 780]nm.\n\n### XYZ from RGB\n\nDistribution of $\\small{\\bar{y}(\\lambda)}$ is close to $\\small{V(\\lambda)}$ luminous efficiency function. So Y component roughly describes the luminance of the color. X is a combination of RGB colors and Z represents blue color. With Y, XZ represents all possible chromaticities (color without luminance).\n\nXYZ is a linear combination of RGB colors. If RGB colors are vectors with unit length and act as vector basis, they form a three-dimensional vector space. And XYZ vectors form a new basis vector by a linear transformation **M** of RGB vector basis.\n\n$$\n\\small{\\begin{bmatrix}\nX \\\\\nY \\\\\nZ \\\\\n\\end{bmatrix} = M\n\\begin{bmatrix}\nR \\\\\nG \\\\\nB \\\\\n\\end{bmatrix}}\n$$\n\n$$\n\\small{\\begin{bmatrix}\nX \\\\\nY \\\\\nZ \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\nR_x \u0026 R_y \u0026 R_z \\\\\nG_x \u0026 G_y \u0026 G_z \\\\\nB_x \u0026 B_y \u0026 B_z\n\\end{bmatrix}\n\\begin{bmatrix}\nR \\\\\nG \\\\\nB \\\\\n\\end{bmatrix}}\n$$\n\n$$\n\\small{\\begin{bmatrix}\nX \\\\\nY \\\\\nZ \\\\\n\\end{bmatrix} = \\frac{1}{0.17}\n\\begin{bmatrix}\n0.49 \u0026 0.31 \u0026 0.20 \\\\\n0.17 \u0026 0.81 \u0026 0.01 \\\\\n0.00 \u0026 0.01 \u0026 0.99\n\\end{bmatrix}\n\\begin{bmatrix}\nR \\\\\nG \\\\\nB \\\\\n\\end{bmatrix}}\n$$\n\nFrom the above transformation matrix **M**, we can say Red and Green colors have zero or no impact on Z which can be confirmed from RGB color matching functions. Also, Y is a combination of weighted RGB values of relative luminance.\n\n---\n\n## CIE xy chromaticity diagram \u0026 CIE xyY color space\n\nImagine if XYZ tristimulus values are represented as vectors, they form a 3D vector space and contains all possible colors with RGB combinations in a cube. Now trace out the coordinates of XYZ for different wavelengths in the visible spectrum, the closed curve that is formed by joining all coordinates is called spectral locus.\n\n![CIE XYZ color space:=:60:=:CIE XYZ spectral locus$$$https://commons.wikimedia.org/wiki/File:3D_Graph_of_CIE_XYZ_Colorspace.png](color-science/cie-xyz-colorspace.jpg)\n\nThe above 3D XYZ color space contains colors with Hue, Saturation, and Intensity/Brightness. It requires three parameters (H, S, and V) to describe a color (each for hue, saturation, and intensity).\n\nIf (X, Y, Z) are projected to $\\small{X + Y + Z = 1}$ plane in XYZ vector space, then normalized representation of (X, Y, Z) is\n\n$$\n\\small{x = \\frac{X}{X + Y + Z} \\hspace2ex\ny = \\frac{Y}{X + Y + Z} \\hspace2ex\nz = \\frac{Z}{X + Y + Z}}\n$$\n\nSumming up the normalized x, y and z components equals to 1 i.e, $\\small{x + y + z = 1}$ and $\\small{x,y,z\\ge0}$\n\nThe planar projection of the above spectral locus to the plane $\\small{X + Y + Z = 1}$ forms a planar triangle.\n\n![CIE XYZ planar projection:=:35](color-science/spectral-locus-planar-projection.jpg)\n\n**Chromaticity** is a color property without intensity i.e color independent component of a color. Chromaticity is the quality of a color determined by its dominant wavelength and its purity (chroma/saturation). In chromaticity, only hue and saturation are used to describe the colors. That means light blue and dark blue have the same hue and saturation values, and same color matching functions.\n\nA chromaticity diagram is a representation of 3D XYZ color space in 2D space with only chromaticity values (hue and saturation). As $x + y + z =1$, any component can be derived from two other components. So z can be derived from x and y as\n\n$$\n\\small{x + y + z = 1}\n$$\n\n$$\n\\small{z = 1 - x - y}\n$$\n\nAny tristimulus value can be derived from other tristimulus values. With Y, X and Z are derived as\n\n$$\n\\small{X = \\frac{Y}{y}x}\n$$\n\n$$\n\\small{Z = \\frac{Y}{y}(1 - x - y)}\n$$\n\nIf the above spectral locus triangular plane in xyz-plane is projected onto xy-plane, a chromaticity diagram is obtained. And to construct the actual XYZ values, Y is stored along with xy. Together xyY forms the **CIE xyY** color space with xy as chromaticity coordinates and Y as a luminance value because we discussed earlier that the Y component roughly matches $\\small{V(\\lambda)}$.\n\nThe [RGB chromaticity diagram](https://en.wikipedia.org/wiki/Rg_chromaticity) could be constructed by following the same procedure as XYZ. Here **CIE rgG** color space is constructed by keeping rg chromaticity coordinates and G value for luminance.\n\n![CIE chromaticity diagram:=:50:=:CIE xyY chromaticity diagram](color-science/cie-chromaticity-diagram.jpg)\n\nThe horseshoe shape in the above diagram contains all colors of the projected planar triangle. The chromaticity diagram is a mathematical representation of the human eye's color perception.\n\n- The outer line of the spectral locus has colors with wavelengths distributed across in visible spectrum. Starting from violet to red and magenta line as a bridge between. These colors are hues or dominant wavelength colors which are seen in the rainbow.\n- The region inside the spectral locus contains all colors possible in XYZ space that are visible to our human eyes. These colors are saturated colors of hues or a mixture of monochromatic wavelengths.\n- A point (x, y) on the above graph matches a color in the xyY color space.\n- It can be observed that CMY (cyan, magenta, yellow) colors lie between (green and blue), (blue and red), and (red and green). CMY colors are present in complement/opposite to RGB.\n\n### Color gamut\n\n**Color gamut** describes a range of colors in the visible spectrum that are visible for human eyes. The color gamut of a device is that subset of color space that can be represented. color gamuts are generally a subset of the CIE chromaticity diagram with the center as a white point.\n\n![Color gamut:=:50:=:Different color gamuts in CIE xy chromaticity diagram$$$https://en.wikipedia.org/wiki/Gamut](color-science/color-gamut.jpg)\n\nColor gamuts are color spaces enclosed by a triangle with color coordinates of the monitor as red, green, and blue colors. A gamut is a three-dimensional color space with lightness being the third dimension perpendicular to the chromaticity diagram.\n\nCertain color gamuts are standardized by different institutions to reproduce the colors across different environments. The **sRGB** color gamut is the standard color subset used to display colors around the web.\n\nThe colors outside of the chromaticity diagram but inside a color gamut (like ProPhoto RGB) are called imaginary colors as they are not visible for human eyes and they look like normal visible colors. These colors can be detected using color measuring techniques but our eyes cannot differentiate those colors with normal visible colors.\n\n---\n\nOver time, different color spaces and techniques were developed to describe human color perception. **Colorimetry** and **Spectrophotometry** are two different methods to quantify the colors. CIE XYZ color spaces are also defined for the 10\u0026deg standard observer, and it is the standard color space being used in the modern digital world. In the fields like image processing and digital photography, it is necessary to have a good grasp of concepts like chromaticity diagram and color gamuts that are useful for methods like color constancy, color correction, and color grading.\n\nFor further study about colors, check out the topics like color tolerance, color difference, conversion of different color gamuts between display systems, and additional color spaces like CIE LAB.\n\n---\n\n### References\n\n- [How the CIE 1931 color matching functions were derived](https://silo.tips/download/how-the-cie-1931-color-matching-functions-were-derived-from-wright-guild-data)\n- https://scholar.harvard.edu/files/schwartz/files/lecture17-color.pdf\n- [CIE 1931 color space - Wikipedia](https://en.wikipedia.org/wiki/CIE_1931_color_space)\n- [CIE color space - Gernot Hoffmann](http://docs-hoffmann.de/ciexyz29082000.pdf)\n- https://engineering.purdue.edu/~bouman/ece637/notes/pdf/Tristimulus.pdf\n- http://graphics.stanford.edu/courses/cs148-10-summer/docs/02_light_color.pdf\n- http://cs.haifa.ac.il/hagit/courses/ist/Lectures/IST03_ColorXYZx4.pdf\n- http://www.cs.cmu.edu/afs/cs/academic/class/15462-s16/www/lec_slides/23_color.pdf\n- https://web.eecs.umich.edu/~sugih/courses/eecs487/lectures/22-Light+Color.pdf\n- [Light, Color and Color Space - Scratch Pixel](https://www.scratchapixel.com/lessons/digital-imaging/colors/color-space)\n- [Color Matching - Craig Blackwell](https://www.youtube.com/watch?v=82ItpxqPP4I)\n- [Visualizing the XYZ Color space](https://www.youtube.com/watch?v=x0-qoXOCOow)\n- [A Beginner's Guide to Colorimetry](https://medium.com/hipster-color-science/a-beginners-guide-to-colorimetry-401f1830b65a)\n- [Precise Color Communincation - Konica Minolta](https://www.konicaminolta.com/instruments/knowledge/color/index.html)\n- [Color Gamut - Epxx](https://epxx.co/artigos/gamut_en.html)\n- http://www.marcelpatek.com/color.html\n- https://www.oceanopticsbook.info/view/photometry-and-visibility/chromaticity\n- https://www.thebroadcastbridge.com/content/entry/14426/color-and-colorimetry-part-7-cie-xyz\n"},{"metadata":{"title":"Create a Notes App with Flutter","description":"Create a color-rich Note-taking app with Flutter.","imgName":"note-app-flutter/note-app-in-flutter.jpg","date":"Jun 15, 2021","tags":["flutter"],"keywords":["flutter","dart","android","note-app","note-taking","sqflite","sqlite"],"id":"create-a-notes-app-with-flutter"},"content":"\n![Create a Notes App with flutter](note-app-flutter/note-app-in-flutter.jpg)\n\n###### Published on: **Jun 15, 2021**\n\n# Create a Note-taking App in Flutter\n\nFlutter is a declarative framework that requires programming in Dart. Flutter is suitable for creating simple apps like Note-taking, Event-registration, etc.\n\n\u003e Pre-requisites: Knowledge of basic Dart and Flutter. Read about [Flutter](https://flutter.dev/docs) and [Dart](https://dart.dev/guides).\n\nWith basic concepts like Widgets and Material design we can create simple apps very fast and easily in Flutter as in Flutter we can use tons of pre-designed widgets to create almost every popular design using in the modern design world.\n\nIn this tutorial, we discuss creating a simple note-taking app. The note-taking app we are going to create provides options like create, save, update and delete notes.\n\nOur note-taking app contains two screens\n\n- Home screen to display all saved notes\n- Notes edit screen to create new notes or edit saved notes\n\n## Create Flutter App\n\nBefore creating a Flutter app please make sure you have installed flutter-sdk and dart-sdk. If not follow the instructions to [install flutter](https://flutter.dev/docs/get-started/install).\n\nCreate a raw flutter app from the terminal. Run the following command and pass any name (to join more than single sting use only underscore)\n\n```bash\nflutter create notes_app\n```\n\nGo to root directory of **notes_app** and locate **main.dart** in **lib** folder. This is where our app starts execution by calling the main() function. You can find some code here which displays the welcome screen.\n\nNow to see the app in an emulator or on a physical device run the below command.\n\n```bash\ncd notes_app/\nflutter run\n```\n\nFor the initial run, it takes some time to install the app on the device, and later builds will be fast. If you encounter any error run _flutter doctor -v_ for additional information and make sure all necessary items are checked.\n\n---\n\n## What a Note should like and contain?\n\nA simple note must have a title and the content which can be edited as many times as possible. We can also add color to note for look and feel.\n\nTo store notes we use [Sqflite](https://pub.dev/packages/sqflite) (a plugin to mimic SQL database in Flutter). Each note can be stored as a single row in the database with fields id, title, content, color.\n\nCreate a file **note.dart** inside **lib/models**. Add a class **Note** to store note as an object which can be converted later as a Map object to store in the database.\n\n```dart:models/note.dart\nclass Note {\n\tint id;\n\tString title;\n\tString content;\n\tString noteColor;\n\n\tNote({\n\t\tthis.id = null,\n\t\tthis.title = \"Note\",\n\t\tthis.content = \"Text\",\n\t\tthis.noteColor = 'red'\n\t});\n\n\tMap\u003cString, dynamic\u003e toMap() {\n\t\tMap\u003cString, dynamic\u003e data = Map\u003cString, dynamic\u003e();\n\t\tif (id != null) {\n\t\t\tdata['id'] = id;\n\t\t}\n\t\tdata['title'] = title;\n\t\tdata['content'] = content;\n\t\tdata['noteColor'] = noteColor;\n\t\treturn data;\n\t}\n\n\t@override toString() {\n\t\treturn {\n\t\t\t'id': id,\n\t\t\t'title': title,\n\t\t\t'content': content,\n\t\t\t'noteColor': noteColor,\n\t\t}.toString();\n\t}\n}\n```\n\nThis Note class has attributes\n\n- id (primary key) - an identifier to store unique note objects in the database\n- title - the title of the note\n- content - content of the note\n- noteColor - the color of the note\n\n**toMap()** returns note as an object to store in the database.\n\nFor note colors, add another file called **theme/note_colors.dart** inside **lib/theme**.\n\n```dart:theme/note_colors.dart\nconst NoteColors = {\n\t'red': {'l': 0xFFFFCDD2,'b': 0xFFE57373},\n\t'pink': {'l': 0xFFF8BBD0, 'b': 0xFFF06292},\n\t'purple': {'l': 0xFFE1BEE7, 'b': 0xFFBA68C8},\n\t'deepPurple': {'l': 0xFFD1C4E9, 'b': 0xFF9575CD},\n\t'indigo': {'l': 0xFFC5CAE9, 'b': 0xFF7986CB},\n\t'blue': {'l': 0xFFBBDEFB, 'b': 0xFF64B5F6},\n\t'lightBlue': {'l': 0xFFB3E5FC, 'b': 0xFF4FC3F7},\n\t'cyan': {'l': 0xFFB2EBF2, 'b': 0xFF4DD0E1},\n\t'teal': {'l': 0xFFB2DFDB, 'b': 0xFF4DB6AC},\n\t'green': {'l': 0xFFC8E6C9, 'b': 0xFF81C784},\n\t'lightGreen': {'l': 0xFFDCEDC8, 'b': 0xFFAED581},\n\t'lime': {'l': 0xFFF0F4C3, 'b': 0xFFDCE775},\n\t'yellow': {'l': 0xFFFFF9C4, 'b': 0xFFFFF176},\n\t'amber': {'l': 0xFFFFECB3, 'b': 0xFFFFD54F},\n\t'orange': {'l': 0xFFFFE0B2, 'b': 0xFFFFB74D},\n\t'deepOrange': {'l': 0xFFFFCCBC, 'b': 0xFFFF8A65},\n\t'brown': {'l': 0xFFD7CCCB, 'b': 0xFFA1887F},\n\t'blueGray': {'l': 0xFFCFD8DC, 'b': 0xFF90A4AE},\n};\n```\n\nEach color name ('k') is a key and each key ('k') has two colors 'l' and 'b', where 'l' is a light color and 'b' is the bright color of this 'k' color. The light and bright colors are used to display a note in the UI which we discuss later. 'k' is the color name we store in the database.\n\n---\n\n## Store notes in the database\n\nNow to store notes on the database we use **sqflite** plugin. Install **sqflite** by adding dependency in _pubspec.yaml_.\n\n```bash\ndependencies:\n  flutter:\n    sdk: flutter\n  sqflite: ^1.3.0\n```\n\nNow in terminal run _flutter pub get_ to install or update dependencies in _pubspec.yaml_.\n\nTo handle database operations we write different functions for different operations like read, write, update and delete. Create **notes_database.dart** inside **models** and add a class to handle different operations\n\n```dart:models/notes_database.dart\nimport 'package:sqflite/sqflite.dart';\n\nimport 'note.dart';\n\nclass NotesDatabase {\n\tstatic final _name = \"NotesDatabase.db\";\n\tstatic final _version = 1;\n\n\tDatabase database;\n\tstatic final tableName = 'notes';\n\n\tinitDatabase() async {\n\t\tdatabase = await openDatabase(\n\t\t\t_name,\n\t\t\tversion: _version,\n\t\t\tonCreate: (Database db, int version) async {\n\t\t\t\tawait db.execute(\n\t\t\t\t\t'''CREATE TABLE $tableName (\n\t\t\t\t\tid INTEGER PRIMARY KEY AUTOINCREMENT,\n\t\t\t\t\ttitle TEXT,\n\t\t\t\t\tcontent TEXT,\n\t\t\t\t\tnoteColor TEXT\n\t\t\t\t\t)'''\n\t\t\t\t);\n\t\t\t}\n\t\t);\n\t}\n\n\tFuture\u003cint\u003e insertNote(Note note) async {\n\t\treturn await database.insert(tableName,\n\t\t\tnote.toMap(),\n\t\t\tconflictAlgorithm: ConflictAlgorithm.replace\n\t\t);\n\t}\n\n\tFuture\u003cint\u003e updateNote(Note note) async {\n\t\treturn await database.update(tableName, note.toMap(),\n\t\t\twhere: 'id = ?',\n\t\t\twhereArgs: [note.id],\n\t\t\tconflictAlgorithm: ConflictAlgorithm.replace\n\t\t);\n\t}\n\n\tFuture\u003cList\u003cMap\u003cString, dynamic\u003e\u003e\u003e getAllNotes() async {\n\t\treturn await database.query(tableName);\n\t}\n\n\tFuture\u003cMap\u003cString, dynamic\u003e\u003e getNotes(int id) async {\n\t\tvar result = await database.query(tableName,\n\t\t\twhere: 'id = ?',\n\t\t\twhereArgs: [id]\n\t\t);\n\n\t\tif (result.length \u003e 0) {\n\t\t\treturn result.first;\n\t\t}\n\n\t\treturn null;\n\t}\n\n\tFuture\u003cint\u003e deleteNote(int id) async {\n\t\treturn await database.delete(tableName,\n\t\t\twhere: 'id = ?',\n\t\t\twhereArgs: [id]\n\t\t);\n\t}\n\n\tcloseDatabase() async {\n\t\tawait database.close();\n\t}\n}\n```\n\nFirst, we need to create a table in the database with some schema. Inside **initDatabase()**, we are calling **openDatabase()** to create database and table or open existing database and table by passing parameters **\\_name** (name of the database) and **\\_version** where **\\_name = NotesDatabse.db** is the name of the database and we can maintain different versions of the database through **\\_version**.\n\nIf there is no database with a specified **name**, **onCreate** callback is called to create a database with table and schema. Above we create a table with **tableName = notes** and initial schema with required fields like id, title, content, and noteColor to store a note object.\n\n**openDatabase()** is an async operation and returns **Database** object reference which points to the created/existed database. We store this reference as **database** of type class **Database**.\n\nOther functions **insertNote**, **updateNote**, **getNotes** and **deleteNotes** handles different database operations. Read more about [how to perform different operations in sqflite](https://github.com/tekartik/sqflite/blob/master/sqflite/doc/how_to.md).\n\nAs we cannot store Note as a class object we convert Note object members to a Map object by calling Note.toMap() which returns a Map object which sqflite map fields and values to store in the database. And Sqflite returns data as Map objects the way we pass it to insert rows in the database.\n\nWe have added logic to maintain notes in the database. But we have not done anything in UI to interact for maintaining notes.\n\n## Add Home Screen\n\nNow create a file called **home.dart** in **lib/screens**. This **home.dart** serves as the Home screen of our app. Add following code to **home.dart**\n\n```dart:screens/home.dart\nimport 'package:flutter/material.dart';\n\nconst c1 = 0xFFFDFFFC, c2 = 0xFFFF595E, c3 = 0xFF374B4A, c4 = 0xFF00B1CC, c5 = 0xFFFFD65C, c6 = 0xFFB9CACA,\n\tc7 = 0x80374B4A, c8 = 0x3300B1CC, c9 = 0xCCFF595E;\n\n// Home Screen\nclass Home extends StatefulWidget{\n\t@override\n\t_Home createState() =\u003e _Home();\n}\n\nclass _Home extends State\u003cHome\u003e {\n\t@override\n\tWidget build(BuildContext context) {\n\t\treturn MaterialApp(\n\t\t\ttitle: 'Super Note',\n\t\t\thome: Scaffold(\n\t\t\t\tbackgroundColor: Color(c6),\n\t\t\t\tappBar: AppBar(\n\t\t\t\t\tautomaticallyImplyLeading: false,\n\t\t\t\t\tbackgroundColor: const Color(c2),\n\t\t\t\t\tbrightness: Brightness.dark,\n\n\t\t\t\t\ttitle: Text(\n\t\t\t\t\t\t'Super Note',\n\t\t\t\t\t\tstyle: TextStyle(\n\t\t\t\t\t\t\tcolor: const Color(c5),\n\t\t\t\t\t\t),\n\t\t\t\t\t),\n\t\t\t\t),\n\n\t\t\t\t//Floating Button\n\t\t\t\tfloatingActionButton: FloatingActionButton(\n\t\t\t\t\tchild: const Icon(\n\t\t\t\t\t\tIcons.add,\n\t\t\t\t\t\tcolor: const Color(c5),\n\t\t\t\t\t),\n\t\t\t\t\ttooltip: 'New Notes',\n\t\t\t\t\tbackgroundColor: const Color(c4),\n\t\t\t\t\tonPressed: () =\u003e {},\n\t\t\t\t),\n\t\t\t),\n\t\t);\n\t}\n}\n```\n\nThere are some color constants defined at the top which will be used across the app. The color format in Flutter is different from normal Hex. In normal Hex format, we provide opacity at last but in Flutter we have to provide opacity at first.\n\nHere we are creating a Home widget as **StatefulWidget** keeping in mind that we need to maintain the state. Every custom widget must override **build** method and return a widget. **MaterialApp** widget gives child widgets material look and we must declare required attributes. **Scaffold** widget is a common material design concept that provides appbar, floating button, drawer, body, etc.\n\nThe Home screen displays all notes stored in the database. We discuss later displaying notes in the Home screen after creating notes in the Edit screen.\n\nTo display our Home screen as default screen in our app call **Home()** widget inside **MyApp** in **main.dart**\n\n```dart:main.dart\nimport 'package:flutter/material.dart';\n\nimport './screens/home.dart';\n\nvoid main() =\u003e runApp(MyApp());\n\nclass MyApp extends StatelessWidget {\n  @override\n  Widget build(BuildContext context) {\n    return MaterialApp(\n      home: Home(),\n    );\n  }\n}\n```\n\nTo see the changes in the app, in the flutter running environment press **r** to hot reload or **R** restart of the app.\n\n![Home Screen:=:30](note-app-flutter/notes-app-initial-home-screen.jpg)\n\nThe Floating action button at the bottom-right will take us to the Edit screen to create a new note. To add navigation from Home to Edit, first create a Edit Screen Widget in **notes_edit.dart** inside **lib/screens**. For now, add a simple UI for the Edit screen like below because we just need a widget to route from Home to Edit screen.\n\n```dart:screens/notes_edit.dart\nimport 'package:flutter/material.dart';\n\nconst c1 = 0xFFFDFFFC, c2 = 0xFFFF595E, c3 = 0xFF374B4A, c4 = 0xFF00B1CC, c5 = 0xFFFFD65C, c6 = 0xFFB9CACA,\n\t\t\tc7 = 0x80374B4A;\n\nclass NotesEdit extends StatefulWidget {\n\t_NotesEdit createState() =\u003e _NotesEdit();\n}\n\nclass _NotesEdit extends State\u003cNotesEdit\u003e {\n\t@override\n\tWidget build(BuildContext context) {\n\t\treturn MaterialApp(\n\t\t\ttitle: 'Edit Screen',\n\t\t\thome: Text(\n\t\t\t\t'Edit'\n\t\t\t),\n\t\t);\n\t}\n}\n```\n\n**NotesEdit** widget is the main widget for the Edit screen. We call this widget in navigation.\n\n### Navigation from Home to Edit\n\nAdd navigation from Home to Edit when pressed floating-action-buttton. Call **Navigation.push()** for the **EditNotes** widget. In **home.dart** add navigation in **onPressed()** event of floating-action-button.\n\n```dart\nimport './notes_edit.dart';\n```\n\n```dart\n//Floating Button\nfloatingActionButton: FloatingActionButton(\n\tchild: const Icon(\n\t\tIcons.add,\n\t\tcolor: const Color(c5),\n\t),\n\ttooltip: 'New Notes',\n\tbackgroundColor: const Color(c4),\n\t// Go to Edit screen\n\tonPressed: () {\n\t  Navigator.push(\n\t  \tcontext,\n\t    MaterialPageRoute(builder: (context) =\u003e NotesEdit()),\n\t  );\n\t}\n),\n```\n\n---\n\n## Change Edit Screen\n\nChange Edit screen UI for creating a new note.\n\n```dart:screens/notes_edit.dart\nimport 'package:flutter/material.dart';\n\nimport '../models/note.dart';\nimport '../models/notes_database.dart';\nimport '../theme/note_colors.dart';\n\nconst c1 = 0xFFFDFFFC, c2 = 0xFFFF595E, c3 = 0xFF374B4A, c4 = 0xFF00B1CC, c5 = 0xFFFFD65C, c6 = 0xFFB9CACA,\n\tc7 = 0x80374B4A;\n\nclass NotesEdit extends StatefulWidget {\n\t_NotesEdit createState() =\u003e _NotesEdit();\n}\n\nclass _NotesEdit extends State\u003cNotesEdit\u003e {\n\tString noteTitle = '';\n\tString noteContent = '';\n\tString noteColor = 'red';\n\n\tTextEditingController _titleTextController = TextEditingController();\n\tTextEditingController _contentTextController = TextEditingController();\n\n\tvoid handleTitleTextChange() {\n\t\tsetState(() {\n\t\t\tnoteTitle = _titleTextController.text.trim();\n\t\t});\n\t}\n\n\tvoid handleNoteTextChange() {\n\t\tsetState(() {\n\t\t\tnoteContent = _contentTextController.text.trim();\n\t\t});\n\t}\n\n\t@override\n\tvoid initState() {\n\t\tsuper.initState();\n\t\t_titleTextController.addListener(handleTitleTextChange);\n\t\t_contentTextController.addListener(handleNoteTextChange);\n\t}\n\n\t@override\n\tvoid dispose() {\n\t\t_titleTextController.dispose();\n\t\t_contentTextController.dispose();\n\t\tsuper.dispose();\n\t}\n\n\t@override\n\tWidget build(BuildContext context) {\n\t\treturn Scaffold(\n\t\t\tbackgroundColor: Color(NoteColors[this.noteColor]['l']),\n\t\t\tappBar: AppBar(\n\t\t\t\tbackgroundColor: Color(NoteColors[this.noteColor]['b']),\n\n\t\t\t\tleading: IconButton(\n\t\t\t\t\ticon: const Icon(\n\t\t\t\t\t\tIcons.arrow_back,\n\t\t\t\t\t\tcolor: const Color(c1),\n\t\t\t\t\t),\n\t\t\t\t\ttooltip: 'Back',\n\t\t\t\t\tonPressed: () =\u003e {},\n\t\t\t\t),\n\n\t\t\t\ttitle: NoteTitleEntry(_titleTextController),\n\t\t\t),\n\n\t\t\tbody: NoteEntry(_contentTextController),\n\t\t);\n\t}\n}\n```\n\nIn the above **NotesEdit** widget, the state variables **noteTitle**, **noteContent** and **noteColor** are initialized to default values for now. **noteTitel** is to store title of the note, **noteContent** is to store note content and**noteColor** is color of the color, light and bright colors of the **noteColor** are used as **backgroundColor** for **appBar** and **Scaffold** respectively.\n\nAlso there are two **TextEditingController** defined which are used to controll **TextField** values for **noteTitle** and **noteContent**. These two text controller are attached with listeners in **iniitState()**. These listeneres listen to changes and updates text values in state. **\\_titleTextController** handles and updates text value for **noteTitle** and **\\_contentTextController** handles **noteContent**.\n\nThe **title** of the **appBar** is set to a widget **NoteTitleEntry** which handles displaying and editing of the title.\n\n```dart\nclass NoteTitleEntry extends StatelessWidget {\n\tfinal _textFieldController;\n\n\tNoteTitleEntry(this._textFieldController);\n\n\t@override\n\tWidget build(BuildContext context) {\n\t\treturn TextField(\n\t\t\tcontroller: _textFieldController,\n\t\t\tdecoration: InputDecoration(\n\t\t\t\tborder: InputBorder.none,\n\t\t\t\tfocusedBorder: InputBorder.none,\n\t\t\t\tenabledBorder: InputBorder.none,\n\t\t\t\terrorBorder: InputBorder.none,\n\t\t\t\tdisabledBorder: InputBorder.none,\n\t\t\t\tcontentPadding: EdgeInsets.all(0),\n\t\t\t\tcounter: null,\n\t\t\t\tcounterText: \"\",\n\t\t\t\thintText: 'Title',\n\t\t\t\thintStyle: TextStyle(\n\t\t\t\t\tfontSize: 21,\n\t\t\t\t\tfontWeight: FontWeight.bold,\n\t\t\t\t\theight: 1.5,\n\t\t\t\t),\n\t\t\t),\n\t\t\tmaxLength: 31,\n\t\t\tmaxLines: 1,\n\t\t\tstyle: TextStyle(\n\t\t\t\tfontSize: 21,\n\t\t\t\tfontWeight: FontWeight.bold,\n\t\t\t\theight: 1.5,\n\t\t\t\tcolor: Color(c1),\n\t\t\t),\n\t\t\ttextCapitalization: TextCapitalization.words,\n\t\t);\n\t}\n}\n```\n\nIn the **TextField**, the controller is set to **\\_textFieldController** which is passed from parent widget ** as **\\_titleTextController\\*\\*.\n\nSimilarly, content of the notes is handled by another widget **NoteEntry**.\n\n```dart\nclass NoteEntry extends StatelessWidget {\n\tfinal _textFieldController;\n\n\tNoteEntry(this._textFieldController);\n\n\t@override\n\tWidget build(BuildContext context) {\n\t\treturn Container(\n\t\t\theight: MediaQuery.of(context).size.height,\n\t\t\tpadding: EdgeInsets.symmetric(horizontal: 12, vertical: 8),\n\t\t\tchild: TextField(\n\t\t\t\tcontroller: _textFieldController,\n\t\t\t\tmaxLines: null,\n\t\t\t\ttextCapitalization: TextCapitalization.sentences,\n\t\t\t\tdecoration: null,\n\t\t\t\tstyle: TextStyle(\n\t\t\t\t\tfontSize: 19,\n\t\t\t\t\theight: 1.5,\n\t\t\t\t),\n\t\t\t),\n\t\t);\n\t}\n}\n```\n\nHere also controller of **TextField** is set to **\\_textFieldController** which is passed from parent wdiget as **\\_contentTextController**.\n\nAfter adding all these widgets, the Edit screen would look like\n\n![Edit screen:=:30](note-app-flutter/notes-app-initial-edit-screen.jpg)\n\n### Add a Color palette to select Note color\n\nWe will add a color palette to select note color and store the value in **noteColor**. For color palette, add an icon in **appBar** **actions** which on press shows a **Dialog** box with different colors.\n\nIn **NotesEdit** add color palette button\n\n```dart\nactions: [\n\tIconButton(\n\t\ticon: const Icon(\n\t\t\tIcons.color_lens,\n\t\t\tcolor: const Color(c1),\n\t\t),\n\t\ttooltip: 'Color Palette',\n\t\tonPressed: () =\u003e handleColor(context),\n\t),\n],\n```\n\nFor this button, **onPressed** event calls **handleColor()** function which shows a color palette and store selected value in **noteColor** variable. Define **handleColor()** inside **\\_NotesEdit**\n\n```dart\nvoid handleColor(currentContext) {\n\tshowDialog(\n\t\tcontext: currentContext,\n\t\tbuilder: (context) =\u003e ColorPalette(\n\t\t\tparentContext: currentContext,\n\t\t),\n\t).then((colorName) {\n\t\tif (colorName != null) {\n\t\t\tsetState(() {\n\t\t\t\tnoteColor = colorName;\n\t\t\t});\n\t\t}\n\t});\n}\n```\n\nThis **handleColor()** calls widget **ColorPalette** which is a **Dialog** box and returns selected color value. Add **ColorPalette** widget to show different colors and return selected color\n\n```dart\nclass ColorPalette extends StatelessWidget {\n\tfinal parentContext;\n\n\tconst ColorPalette({\n\t\t@required this.parentContext,\n\t});\n\n\t@override\n\tWidget build(BuildContext context) {\n\t\treturn Dialog(\n\t\t\tbackgroundColor: Color(c1),\n\t\t\tclipBehavior: Clip.hardEdge,\n\t\t\tinsetPadding: EdgeInsets.all(MediaQuery.of(context).size.width * 0.03),\n\t\t\tshape: RoundedRectangleBorder(\n\t\t\t\tborderRadius: BorderRadius.circular(2),\n\t\t\t),\n\t\t\tchild: Container(\n\t\t\t\tpadding: EdgeInsets.all(8),\n\t\t\t\tchild: Wrap(\n\t\t\t\t\talignment: WrapAlignment.start,\n\t\t\t\t\tspacing: MediaQuery.of(context).size.width * 0.02,\n\t\t\t\t\trunSpacing: MediaQuery.of(context).size.width * 0.02,\n\t\t\t\t\tchildren: NoteColors.entries.map((entry) {\n\t\t\t\t\t\treturn GestureDetector(\n\t\t\t\t\t\t\tonTap: () =\u003e Navigator.of(context).pop(entry.key),\n\t\t\t\t\t\t\tchild: Container(\n\t\t\t\t\t\t\t\twidth: MediaQuery.of(context).size.width * 0.12,\n\t\t\t\t\t\t\t\theight: MediaQuery.of(context).size.width * 0.12,\n\t\t\t\t\t\t\t\tdecoration: BoxDecoration(\n\t\t\t\t\t\t\t\t\tborderRadius: BorderRadius.circular(MediaQuery.of(context).size.width * 0.06),\n\t\t\t\t\t\t\t\t\tcolor: Color(entry.value['b']),\n\t\t\t\t\t\t\t\t),\n\t\t\t\t\t\t\t),\n\t\t\t\t\t\t);\n\t\t\t\t\t}).toList(),\n\t\t\t\t),\n\t\t\t),\n\t\t);\n\t}\n}\n```\n\nAs we already stored different colors in **NoteColors** Map object, we iterate this object and fill the color palette with bright colors.\n\n![Color Palette:=:60](note-app-flutter/notes-app-color-palette.jpg)\n\n### Save notes\n\nWe have everything to save notes in the database. We save a note in the database if **backButton** is pressed at the top. Now add a function to handle **backButton** **onPressed**.\n\n```dart\nvoid handleBackButton() async {\n\tif (noteTitle.length == 0) {\n\t\t// Go Back without saving\n\t\tif (noteContent.length == 0) {\n\t\t\tNavigator.pop(context);\n\t\t\treturn;\n\t\t}\n\t\telse {\n\t\t\tString title = noteContent.split('\\n')[0];\n\t\t\tif (title.length \u003e 31) {\n\t\t\t\ttitle = title.substring(0, 31);\n\t\t\t}\n\t\t\tsetState(() {\n\t\t\t\tnoteTitle = title;\n\t\t\t});\n\t\t}\n\t}\n\n\t// Save New note\n\tNote noteObj = Note(\n\t\ttitle: noteTitle,\n\t\tcontent: noteContent,\n\t\tnoteColor: noteColor\n\t);\n\ttry {\n\t\tawait _insertNote(noteObj);\n\t} catch (e) {\n\t\tprint('Error inserting row');\n\t} finally {\n\t\tNavigator.pop(context);\n\t\treturn;\n\t}\n}\n```\n\nThis function calls **\\_insertNote()** which saves the note object in the database.\n\n```dart\nFuture\u003cvoid\u003e _insertNote(Note note) async {\n  NotesDatabase notesDb = NotesDatabase();\n  await notesDb.initDatabase();\n  int result = await notesDb.insertNote(note);\n  await notesDb.closeDatabase();\n}\n```\n\nWe have saved notes in the database, now in the Home screen, display the saved notes in list view.\n\n---\n\n## Show saved notes on the Home screen\n\nWe can retrieve saved notes from the database and we use that retrieved data to show a note as a list on the Home screen. As retrieving data from the database is an async task and we need to have data before building the Home widget, we use [FutureBuilder](https://api.flutter.dev/flutter/widgets/FutureBuilder-class.html).\n\n```dart\nFuture\u003cList\u003cMap\u003cString, dynamic\u003e\u003e\u003e readDatabase() async {\n\ttry {\n\t  NotesDatabase notesDb = NotesDatabase();\n\t  await notesDb.initDatabase();\n\t  List\u003cMap\u003e notesList = await notesDb.getAllNotes();\n\t  await notesDb.closeDatabase();\n\t  List\u003cMap\u003cString, dynamic\u003e\u003e notesData = List\u003cMap\u003cString, dynamic\u003e\u003e.from(notesList);\n\t \tnotesData.sort((a, b) =\u003e (a['title']).compareTo(b['title']));\n\t  return notesData;\n\t} catch(e) {\n\t\tprint('Error retrieving notes');\n\t\treturn [{}];\n\t}\n}\n```\n\nThis function reads all saved notes in the database and returns them as **Future** objects. We call this function in FutureBuilder and it builds the note list which displays each notes as a list.\n\nBefore that add necessary imports in **home.dart** to handle the database, to store note object and colors.\n\n```dart\nimport '../models/note.dart';\nimport '../models/notes_database.dart';\nimport '../theme/note_colors.dart';\n```\n\nStore read notes from database in state and define other state variables\n\n```dart\nList\u003cMap\u003cString, dynamic\u003e\u003e notesData;\nList\u003cint\u003e selectedNoteIds = [];\n```\n\n**notesData** stores all notes data read from database and **selectedNoteIds** will have a list of selected notes when a note is selected in Home.\n\n```dart\nbody: FutureBuilder(\n\tfuture: readDatabase(),\n\tbuilder: (context, snapshot) {\n\t\tif (snapshot.hasData) {\n\t\t\tnotesData = snapshot.data;\n\t\t\treturn Stack(\n\t\t\t\tchildren: \u003cWidget\u003e[\n\t\t\t\t\t// Display Notes\n\t\t\t\t\tAllNoteLists(\n\t\t\t\t\t\tsnapshot.data,\n\t\t\t\t\t\tthis.selectedNoteIds,\n\t\t\t\t\t\tafterNavigatorPop,\n\t\t\t\t\t\thandleNoteListLongPress,\n\t\t\t\t\t\thandleNoteListTapAfterSelect,\n\t\t\t\t\t),\n\t\t\t\t],\n\t\t\t);\n\t\t} else if (snapshot.hasError) {\n\t\t\tprint('Error reading database');\n\t\t} else {\n\t\t\treturn Center(\n\t\t\t\tchild: CircularProgressIndicator(\n\t\t\t\t\tbackgroundColor: Color(c3),\n\t\t\t\t),\n\t\t\t);\n\t\t}\n\t}\n),\n```\n\nHere before building the widget we read the data from the database and builds a list of note widgets to display on the Home screen by calling **AllNoteLists** widget. We also pass different callback functions to **AllNoteLists** to handles cases like the long selection of note, deselect a note, etc.\n\nDefine all these functions inside **\\_Home**\n\n```dart\n// Render the screen and update changes\nvoid afterNavigatorPop() {\n\tsetState(() {});\n}\n\n// Long Press handler to display bottom bar\nvoid handleNoteListLongPress(int id) {\n\tsetState(() {\n\t\tif (selectedNoteIds.contains(id) == false) {\n\t\t\tselectedNoteIds.add(id);\n\t\t}\n\t});\n}\n\n// Remove selection after long press\nvoid handleNoteListTapAfterSelect(int id) {\n\tsetState(() {\n\t\tif (selectedNoteIds.contains(id) == true) {\n\t\t\tselectedNoteIds.remove(id);\n\t\t}\n\t});\n}\n\n// Delete Note/Notes\nvoid handleDelete() async {\n\ttry {\n\t\tNotesDatabase notesDb = NotesDatabase();\n\t\tawait notesDb.initDatabase();\n\t\tfor (int id in selectedNoteIds) {\n\t\t\tint result = await notesDb.deleteNote(id);\n\t\t}\n\t\tawait notesDb.closeDatabase();\n\t} catch (e) {\n\n\t} finally {\n\t\tsetState(() {\n\t\t\tselectedNoteIds = [];\n\t\t});\n\t}\n}\n```\n\nDefine **AllNoteLists** widget which gets arguments from parent widget including note data and callback functions to handle\n\n```dart\n// Display all notes\nclass AllNoteLists extends StatelessWidget {\n\tfinal data;\n\tfinal selectedNoteIds;\n\tfinal afterNavigatorPop;\n\tfinal handleNoteListLongPress;\n\tfinal handleNoteListTapAfterSelect;\n\n\tAllNoteLists(\n\t\tthis.data,\n\t\tthis.selectedNoteIds,\n\t\tthis.afterNavigatorPop,\n\t\tthis.handleNoteListLongPress,\n\t\tthis.handleNoteListTapAfterSelect,\n\t);\n\n\t@override\n\tWidget build(BuildContext context) {\n\t\treturn ListView.builder(\n\t\t\titemCount: data.length,\n\t\t\titemBuilder: (context, index) {\n\t\t\t\tdynamic item = data[index];\n\t\t\t\treturn DisplayNotes(\n\t\t\t\t\titem,\n\t\t\t\t\tselectedNoteIds,\n\t\t\t\t\t(selectedNoteIds.contains(item['id']) == false? false: true),\n\t\t\t\t\tafterNavigatorPop,\n\t\t\t\t\thandleNoteListLongPress,\n\t\t\t\t\thandleNoteListTapAfterSelect,\n\t\t\t\t);\n\t\t\t}\n\t\t);\n\t}\n}\n\n\n// A Note view showing title, first line of note and color\nclass DisplayNotes extends StatelessWidget {\n\tfinal notesData;\n\tfinal selectedNoteIds;\n\tfinal selectedNote;\n\tfinal callAfterNavigatorPop;\n\tfinal handleNoteListLongPress;\n\tfinal handleNoteListTapAfterSelect;\n\n\tDisplayNotes(\n\t\tthis.notesData,\n\t\tthis.selectedNoteIds,\n\t\tthis.selectedNote,\n\t\tthis.callAfterNavigatorPop,\n\t\tthis.handleNoteListLongPress,\n\t\tthis.handleNoteListTapAfterSelect,\n\t);\n\n\t@override\n\tWidget build(BuildContext context) {\n\t\treturn Padding(\n\t\t\tpadding: const EdgeInsets.symmetric(horizontal: 8.0, vertical: 2.0),\n\t\t\tchild: Material(\n\t\t\t\televation: 1,\n\t\t\t\tcolor: (selectedNote == false? Color(c1): Color(c8)),\n\t\t\t\tclipBehavior: Clip.hardEdge,\n\t\t\t\tborderRadius: BorderRadius.circular(5.0),\n\t\t\t\tchild: InkWell(\n\t\t\t\t\tonTap: () {\n\t\t\t\t\t\tif (selectedNote == false) {\n\t\t\t\t\t\t\tif (selectedNoteIds.length == 0) {\n\t\t\t\t\t\t\t\t// Go to edit screen to update notes\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\telse {\n\t\t\t\t\t\t\t\thandleNoteListLongPress(notesData['id']);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\telse {\n\t\t\t\t\t\t\thandleNoteListTapAfterSelect(notesData['id']);\n\t\t\t\t\t\t}\n\t\t\t\t\t},\n\n\t\t\t\t\tonLongPress: () {\n\t\t\t\t\t\thandleNoteListLongPress(notesData['id']);\n\t\t\t\t\t},\n\t\t\t\t\tchild: Container(\n\t\t\t\t\t\twidth: MediaQuery.of(context).size.width,\n\t\t\t\t\t\tpadding: const EdgeInsets.symmetric(horizontal: 8.0, vertical: 8.0),\n\t\t\t\t\t\tchild: Row(\n\t\t\t\t\t\t\tchildren: \u003cWidget\u003e[\n\t\t\t\t\t\t\t\tExpanded(\n\t\t\t\t\t\t\t\t\tflex: 1,\n\t\t\t\t\t\t\t\t\tchild: Column(\n\t\t\t\t\t\t\t\t\t\tmainAxisAlignment: MainAxisAlignment.center,\n\t\t\t\t\t\t\t\t\t\tcrossAxisAlignment: CrossAxisAlignment.center,\n\t\t\t\t\t\t\t\t\t\tmainAxisSize: MainAxisSize.min,\n\t\t\t\t\t\t\t\t\t\tchildren: \u003cWidget\u003e[\n\t\t\t\t\t\t\t\t\t\t\tContainer(\n\t\t\t\t\t\t\t\t\t\t\t\talignment: Alignment.center,\n\t\t\t\t\t\t\t\t\t\t\t\tdecoration: BoxDecoration(\n\t\t\t\t\t\t\t\t\t\t\t\t\tcolor: (selectedNote == false?\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tColor(NoteColors[notesData['noteColor']]['b']):\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tColor(c9)\n\t\t\t\t\t\t\t\t\t\t\t\t\t),\n\t\t\t\t\t\t\t\t\t\t\t\t\tshape: BoxShape.circle,\n\t\t\t\t\t\t\t\t\t\t\t\t),\n\t\t\t\t\t\t\t\t\t\t\t\tchild: Padding(\n\t\t\t\t\t\t\t\t\t\t\t\t\tpadding: EdgeInsets.all(10),\n\t\t\t\t\t\t\t\t\t\t\t\t\tchild: (\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tselectedNote == false?\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tText(\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnotesData['title'][0],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstyle: TextStyle(\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcolor: Color(c1),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tfontSize: 21,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t):\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tIcon(\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tIcons.check,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcolor: Color(c1),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tsize: 21,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t\t\t\t\t\t),\n\t\t\t\t\t\t\t\t\t\t\t\t),\n\t\t\t\t\t\t\t\t\t\t\t),\n\t\t\t\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\t\t),\n\t\t\t\t\t\t\t\t),\n\n\t\t\t\t\t\t\t\tExpanded(\n\t\t\t\t\t\t\t\t\tflex: 5,\n\t\t\t\t\t\t\t\t\tchild: Column(\n\t\t\t\t\t\t\t\t\t\tmainAxisAlignment: MainAxisAlignment.spaceAround,\n\t\t\t\t\t\t\t\t\t\tcrossAxisAlignment: CrossAxisAlignment.start,\n\t\t\t\t\t\t\t\t\t\tmainAxisSize: MainAxisSize.min,\n\t\t\t\t\t\t\t\t\t\tchildren:\u003cWidget\u003e[\n\t\t\t\t\t\t\t\t\t\t\tText(\n\t\t\t\t\t\t\t\t\t\t\t\tnotesData['title'] != null? notesData['title']: \"\",\n\t\t\t\t\t\t\t\t\t\t\t\tstyle: TextStyle(\n\t\t\t\t\t\t\t\t\t\t\t\t\tcolor: Color(c3),\n\t\t\t\t\t\t\t\t\t\t\t\t\tfontSize: 18,\n\t\t\t\t\t\t\t\t\t\t\t\t\tfontWeight: FontWeight.bold,\n\t\t\t\t\t\t\t\t\t\t\t\t),\n\t\t\t\t\t\t\t\t\t\t\t),\n\n\t\t\t\t\t\t\t\t\t\t\tContainer(\n\t\t\t\t\t\t\t\t\t\t\t\theight: 3,\n\t\t\t\t\t\t\t\t\t\t\t),\n\n\t\t\t\t\t\t\t\t\t\t\tText(\n\t\t\t\t\t\t\t\t\t\t\t\tnotesData['content'] != null? notesData['content'].split('\\n')[0]: \"\",\n\t\t\t\t\t\t\t\t\t\t\t\tstyle: TextStyle(\n\t\t\t\t\t\t\t\t\t\t\t\t\tcolor: Color(c7),\n\t\t\t\t\t\t\t\t\t\t\t\t\tfontSize: 16,\n\t\t\t\t\t\t\t\t\t\t\t\t\tfontWeight: FontWeight.w300,\n\t\t\t\t\t\t\t\t\t\t\t\t),\n\t\t\t\t\t\t\t\t\t\t\t),\n\t\t\t\t\t\t\t\t\t\t],\n\t\t\t\t\t\t\t\t\t),\n\t\t\t\t\t\t\t\t),\n\t\t\t\t\t\t\t],\n\t\t\t\t\t\t),\n\t\t\t\t\t),\n\t\t\t\t),\n\t\t\t),\n\t\t);\n\t}\n}\n```\n\n**AllNoteLists** builds a list of notes from the Map of a list of notes. In **ListView** builder it passes each note extracted data to another widget **DisplayNotes** which represents each note.\n\nNow Home screen displays all notes stored as\n\n![Home screen:=:30](note-app-flutter/notes-app-home-notes-list.jpg)\n\nLong press on the note to select the note. And if a note is selected we can add a delete action to delete the selected notes. Add **Delete** button at **appBar** **actions** which shows delete icon only if any note is selected.\n\n```dart\nactions: [\n\t(selectedNoteIds.length \u003e 0?\n\t\tIconButton(\n\t\t\ticon: const Icon(\n\t\t\t\tIcons.delete,\n\t\t\t\tcolor: const Color(c1),\n\t\t\t),\n\t\t\ttooltip: 'Delete',\n\t\t\tonPressed: () =\u003e handleDelete(),\n\t\t):\n\t\tContainer()\n\t),\n],\n```\n\nDefine **hanldeDelete()** which deletes all selected notes from database.\n\n```dart\n// Delete Notes\nvoid handleDelete() async {\n\ttry {\n\t\tNotesDatabase notesDb = NotesDatabase();\n\t\tawait notesDb.initDatabase();\n\t\tfor (int id in selectedNoteIds) {\n\t\t\tint result = await notesDb.deleteNote(id);\n\t\t}\n\t\tawait notesDb.closeDatabase();\n\t} catch (e) {\n\t\tprint('Cannot delete notes');\n\t} finally {\n\t\tsetState(() {\n\t\t\tselectedNoteIds = [];\n\t\t});\n\t}\n}\n```\n\nFor notes, we have added create, read and delete functions. Now we will add an update function to edit already stored notes.\n\n### Update notes\n\nFor this, we can use the Edit screen to update the notes as it has all features to create notes which are similar for update notes also. We have to tell the Edit screen which type of notes operations we doing either create or update notes. To inform the Edit screen we can pass arguments to **NotesEdit** widget while routing about the type of action and notes data if the action is to update. Change **NotesEdit** widget to accept arguments telling the type of action and necessary data.\n\n```dart\nclass NotesEdit extends StatefulWidget {\n\tfinal args;\n\n\tconst NotesEdit(this.args);\n\t_NotesEdit createState() =\u003e _NotesEdit();\n}\n```\n\n**args** stores parameters passed from parent widget.\n\nChange navigation arguments for **NotesEdit** in the floating-action-button in **\\_Home**.\n\n```dart\n//Floating Button\nfloatingActionButton: FloatingActionButton(\n\tchild: const Icon(\n\t\tIcons.add,\n\t\tcolor: const Color(c5),\n\t),\n\ttooltip: 'New Notes',\n\tbackgroundColor: const Color(c4),\n\tonPressed: () {\n\t  Navigator.push(\n\t  \tcontext,\n\t    MaterialPageRoute(builder: (context) =\u003e NotesEdit(['new', {}])),\n\t  );\n\t}\n),\n```\n\nAs floating-button triggers the creation of a new note, we pass argument **new** to inform **NotesEdit** that operation in creation of note.\n\nWhen tapped on a note on the Home screen we navigate to the Edit screen to update the note. For this add navigation from Home to Edit when tapped on the note in **DisplayNotes**.\n\n```dart\nchild: InkWell(\nonTap: () {\n\tif (selectedNote == false) {\n\t\tif (selectedNoteIds.length == 0) {\n\t\t\tNavigator.push(\n\t\t\t\tcontext,\n        MaterialPageRoute(\n          builder: (context) =\u003e NotesEdit(['update', notesData]),\n        ),\n\t\t\t).then((dynamic value) {\n\t\t\t\t\tcallAfterNavigatorPop();\n\t\t\t\t}\n\t\t\t);\n\t\t\treturn;\n\t\t}\n\t\telse {\n\t\t\thandleNoteListLongPress(notesData['id']);\n\t\t}\n\t}\n\telse {\n\t\thandleNoteListTapAfterSelect(notesData['id']);\n\t}\n},\n```\n\nWe pass **update** and **notesData** to the Edit screen stating the operation is updating notes and note data to fill in the Edit screen.\n\nChange **NotesEdit** widget in **notes_edit.dart** for handling update note operation.\n\n```dart\n@override\nvoid initState() {\n\tsuper.initState();\n\tnoteTitle = (widget.args[0] == 'new'? '': widget.args[1]['title']);\n\tnoteContent = (widget.args[0] == 'new'? '': widget.args[1]['content']);\n\tnoteColor = (widget.args[0] == 'new'? 'red': widget.args[1]['noteColor']);\n\n\t_titleTextController.text = (widget.args[0] == 'new'? '': widget.args[1]['title']);\n\t_contentTextController.text = (widget.args[0] == 'new'? '': widget.args[1]['content']);\n\t_titleTextController.addListener(handleTitleTextChange);\n\t_contentTextController.addListener(handleNoteTextChange);\n}\n\nvoid handleBackButton() async {\n\tif (noteTitle.length == 0) {\n\t\t// Go Back without saving\n\t\tif (noteContent.length == 0) {\n\t\t\tNavigator.pop(context);\n\t\t\treturn;\n\t\t}\n\t\telse {\n\t\t\tString title = noteContent.split('\\n')[0];\n\t\t\tif (title.length \u003e 31) {\n\t\t\t\ttitle = title.substring(0, 31);\n\t\t\t}\n\t\t\tsetState(() {\n\t\t\t\tnoteTitle = title;\n\t\t\t});\n\t\t}\n\t}\n\n\t// Save New note\n\tif (widget.args[0] == 'new') {\n\t\tNote noteObj = Note(\n\t\t\ttitle: noteTitle,\n\t\t\tcontent: noteContent,\n\t\t\tnoteColor: noteColor\n\t\t);\n\t\ttry {\n\t\t\tawait _insertNote(noteObj);\n\t\t} catch (e) {\n\n\t\t} finally {\n\t\t\tNavigator.pop(context);\n\t\t\treturn;\n\t\t}\n\t}\n\n\t// Update Note\n\telse if (widget.args[0] == 'update') {\n\t\tNote noteObj = Note(\n\t\t\tid: widget.args[1]['id'],\n\t\t\ttitle: noteTitle,\n\t\t\tcontent: noteContent,\n\t\t\tnoteColor: noteColor\n\t\t);\n\t\ttry {\n\t\t\tawait _updateNote(noteObj);\n\t\t} catch (e) {\n\n\t\t} finally {\n\t\t\tNavigator.pop(context);\n\t\t\treturn;\n\t\t}\n\t}\n}\n```\n\nTapping on the note in the Home screen will take us to the Edit screen to update notes.\n\n---\n\nThis tutorial addressed how to create a simple note-taking app in Flutter with common operations like create, read, update and delete. We can extend the app to have multiple day-to-day useful features. I hope you will do that to create your own notes app according to your interests and needs.\n\nI have created a full Android working application with additional features like Notes sharing, multi-select notes, deleting notes in the edit screen, sort text in notes, etc. Check out the full code at [github.com/santhalakshminarayana/zehero-note](https://github.com/santhalakshminarayana/zehero-note).\n"},{"metadata":{"title":"Build Blog with Next.js and MDX \u0026 Deploy to Github Pages","description":"Create a blog with Next.js as Static Site Generator, MDX for writing content, Github Pages for deploying the static website. Also add SEO and Image optimization.","imgName":"blog-nextjs-mdx/nextjs.jpeg","date":"Dec 31, 2020","tags":["react","next-js"],"keywords":["react'","next.js","blog","mdx","markdown","gh-pages","github-pages"],"id":"build-blog-with-nextjs-mdx-and-deploy-to-github-pages"},"content":"\n![Build Blog with Next.js \u0026 MDX and Deploy to Github Pages](blog-nextjs-mdx/nextjs.jpeg)\n\n###### Published on: **Dec 31, 2020**\n\n# Build Blog with Next.js \u0026 MDX and Deploy to Github Pages\n\nIn this post, we will discuss how to create and publish a blog with [Next.js](https://nextjs.org/), write content with [MDX](https://mdxjs.com/), deploy static site to Github Pages, Image optimization to reduce the page load time and SEO for better page ranking.\n\n\u003e Pre-requisites: Basic understanding of React and Markdown.\n\nBuilding a blog with Next.js is very easy and it is simple to understand, develop, and maintain the dynamic websites and credit goes to Next.js dynamic paging which lets the creation of dynamic URLs and routing. When I decided to start my blog then I searched and read about many frameworks like Vanilla React, Gatsby, Hugo, etc., After reading many blogs, comments, and reviews I felt Next.js would be the option I was looking for.\n\nNext.js offers everything need to create a blog:\n\n- Static site export support\n- Dynamic routing\n- MDX (markdown with JSX) support\n- Image optimization\n- SEO\n\nNow dive in to create a simple blog\n\n## Setup Next.js and MDX\n\nNext.js is a React framework to create SPA (single page applications) and enables both static websites and server-side rendering. Here we'll focus only on static website generation.\n\n### Install Next.js\n\n[Install Next.js](https://nextjs.org/docs/getting-started#setup) by typing any of the following commands\n\n```bash\nnpx create-next-app\n# or\nyarn create next-app\n```\n\n_create-next-app_ installs everything needed to start with.\n\nThe most important thing in Next.js is the **pages** directory. Every component exported from **.js**, **.jsx**, **.ts**, or **.tsx** in the pages folder is treated as a page and each page associates with a route based on its file name. In the pages folder, the **App** component from **\\_app.js** serves as the initialization of pages that can be edited for custom use like global style declaration, CDN's, etc., **index.js** is the starting point for adding content.\n\nTo write content in articles we use MDX, which lets us write JSX in Markdown (**.mdx** file). Writing in markdown is as beautiful as it is like writing in a text file and can render as HTML tags. Besides easy export and maintenance of articles, we can also reuse these files in another framework/platform which supports MDX without rewriting.\n\n### Install MDX\n\nInstall necessary plugins for MDX\n\n```bash\nyarn add @next/mdx gray-matter next-mdx-remote\n```\n\nWe installed _@next/mdx_ to handle **.mdx** files in pages directory, _gray-matter_ is to parse content from markdown and _next-mdx-remote_ for rendering markdown as HTML.\n\nNow create / open **next.config.js** (configuration file for Next.js) at the project root level and add the following to configure MDX and handle **.mdx** page extensions in the pages folder.\n\n```js\nconst withMDX = require(\"@next/mdx\")({\n  extension: /\\.mdx?$/,\n});\nmodule.exports = withMDX({\n  pageExtensions: [\"js\", \"jsx\", \"mdx\"],\n  target: \"serverless\",\n});\n```\n\nAs Next.js only looks for **.js** or **.jsx** files and gives routing to these pages, the above configuration tells Next.js to treat **.md** or **.mdx** files as pages and provide routing.\n\nAs we are creating a static site, **target: 'serverless'** notifies Next.js to generate static files for us.\n\nOpen **package.json** file in the root directory and add **deploy** command to export all static files as a folder as **out** (can have a different name) at the root level. After installing plugins and adding values **package.json** might look like this\n\n```json\n{\n  \"name\": \"blog\",\n  \"version\": \"0.1.0\",\n  \"private\": true,\n  \"scripts\": {\n    \"dev\": \"next dev\",\n    \"build\": \"next build\",\n    \"start\": \"next start\",\n    \"export\": \"next export\",\n    \"deploy\": \"npm run build \u0026\u0026 npm run export\"\n  },\n  \"dependencies\": {\n    ---\n  }\n}\n\n```\n\nLet's give some break to configuration and take a turn to add some content.\n\n## Home Page\n\nAs said earlier **index.js** is the pivot file and also the Home page for our website. So let's edit our Home page and customize it like below.\n\n```js:pages/index.js\nexport default function Home() {\n  return (\n    \u003cdiv className = 'info-container'\u003e\n      \u003cimg src = 'batman.png' alt = 'Batman Logo'/\u003e\n      \u003cp className = 'info-description'\u003eHi I'm Batman, the saviour of Gotham City and I like to roam in nights to bash the bad guys.\u003c/p\u003e\n      \u003cp className = 'info-description'\u003eBut please don't call me as a source for \u003cb\u003eCorona Virus\u003c/b\u003e and it could be the \u003cb\u003eJoker\u003c/b\u003e who\n      might have started this mess.\u003c/p\u003e\n\n      \u003cstyle jsx\u003e{`\n        .info-container {\n          margin: 0 5% 0 5%;\n        }\n\n        img {\n          width: 20%;\n          max-width: 20%;\n          height: auto;\n          margin-left: 40%;\n        }\n\n        .info-description {\n          font-size: 20px;\n        }\n      `}\u003c/style\u003e\n    \u003c/div\u003e\n  )\n}\n```\n\nIn the above snippet, **CSS** styles are provided inside the component. This is called **CSS-In-JS**, which is possible as Next.js bundles **styled-jsx**. There are many ways to add [CSS in Next.js](https://nextjs.org/docs/basic-features/built-in-css-support).\n\nIn the **img** tag above, **src** is provided with the name of the image only but not relative/absolute path. In Next.js we don't need to provide complete paths if we place any assets (like images, videos) in the **public** folder. Next.js automatically prepends the path at build time for assets in the public folder.\n\nNow to see changes, start localhost (default port is 3000) as a dev server\n\n```bash\n# starts localhost at port 8000\nyarn dev -p 8000\n```\n\nOpen any desktop browser and type URL http://localhost:port/ to see changes.\nFor the above code, the display is like below\n\n![First preview](blog-nextjs-mdx/nextjs-blog-display.jpg)\n\nWe have a home page with a welcome message. Now some create blog posts with MDX.\n\n## Write Blog Content with MDX\n\nCreate a directory to store our markdown posts at the root level or any accessible place.\n\n```shell\nmkdir posts\n```\n\nWrite some content in a markdown file and save it as '.mdx' inside the **posts** directory. I have created two posts and saved them as **batman-vs-superman.mdx** and **justice-league.mdx**.\n\n```markdown:posts/batman-vs-superman.mdx\n---\ntitle: \"Batman VS Superman\"\ndescription: \"An intense fight between two superheroes, me and Superman.\"\ndate: \"Mar 25, 2016\"\n---\n\n# Batman VS Superman\n\nI and Superman accidentally met (fight) and later realized there was a culprit (Lex Luthor) who we should fight.\n\nAs usual, it cost a whopping $250 million for this high-action story.\n```\n\n```markdown:posts/justice-league.mdx\n---\ntitle: \"Justice League\"\ndescription: \"Grand union with fellow superheroes which costs $300 million but received face slap from the audience.\"\ndate: \"Nov 17, 2017\"\n---\n\n# Justice League\n\nSuperheroes from the DC universe consisting of Superman, Wonder Woman, The Flash, Aquaman, and Cyborg and I met in 2017 to spoil the party plans of Steppenwolf who tried to steal Mother Boxes on Earth.\n\nIt's a very long story of how we met each other and all thanks to Avengers who had inspired me to search for other superheroes.\n```\n\nIn the above snippet content inside **---** is used as metadata to make routing for this **.mdx** file. We'll discuss this later.\n\n## Show blog posts on the Home page\n\n### Fetch posts data\n\nTo show our blog posts on the Home page, we have to fetch the **.mdx** files and parse content. We can also provide routing from the home page to any blog post. We write the logic to fetch the **.mdx** files to read the content inside and extract metadata useful to display posts on the Home page. These files should be separated from routing, so at the root level create a folder called **lib** where we store all program files to extract **.mdx** content. Inside **lib** create a file with name **getPostsData.js** which returns posts data like markdown content, title, path, etc.,\n\n```js:lib/getPostsData.js\nconst fs = require('fs');\nconst path = require('path');\nconst matter = require(\"gray-matter\");\n\n// current 'posts' directory\nconst postsDirectory = path.join(process.cwd(), 'posts');\nconst mdx_file_extention = '.mdx';\n\nfunction getAllFilesInDirectory() {\n  const fileNames = fs.readdirSync(postsDirectory);\n  return fileNames.map((fileName) =\u003e {\n    return path.parse(fileName)\n  })\n}\n\nfunction getMdxFiles() {\n  const allFiles = getAllFilesInDirectory();\n  return allFiles.filter(parsedFile =\u003e parsedFile.ext == mdx_file_extention);\n}\n\nexport function getAllPostsPath() {\n  const allMdxFiles = getMdxFiles();\n  return allMdxFiles.map((parsedFile) =\u003e {\n    return {\n      params: {\n        id: parsedFile.name\n      }\n    }\n  })\n}\n\nexport function getPostsMetaData() {\n  const allMdxFiles = getMdxFiles();\n\n  const postsMetaData = allMdxFiles.map((parsedFile) =\u003e {\n    const fullPath = path.join(postsDirectory, parsedFile.base);\n\n    // get MDX metadata and content\n    const fileContents = fs.readFileSync(fullPath, 'utf8');\n    // get metadata, content\n    const { data, content } = matter(fileContents);\n    let metadata = data;\n    metadata['id'] = parsedFile.name;\n    return metadata;\n  });\n  return postsMetaData;\n}\n\nexport function getPostData(id) {\n  const fullPath = path.join(postsDirectory, id + mdx_file_extention);\n\n  // get MDX metadata and content\n  const fileContents = fs.readFileSync(fullPath, 'utf8');\n  // get metadata, content\n  const { data, content } = matter(fileContents);\n\n  let metadata = data;\n  metadata['id'] = id;\n\n  return {'metadata': metadata, 'content': content};\n}\n```\n\n**getAllPostsPath** function returns all **.mdx** files path names to serve as URLs for dynamic routing of a page.\n\n**getPostsMetaData** function returns all **.mdx** files metadata (data inside **---**) which we use to gather information like title, description, etc., and function **getPostsData** returns both metadata and markdown content to render for a particular file we request through argument **id**. **gray-matter** parses the markdown file into metadata (data inside **---**) and markdown content to render.\n\nIf encountered error while accessing **fs** add the following to _next.config.js_\n\n```js:next.config.js\nconst withMDX = require('@next/mdx')({\n  extension: /\\.mdx?$/,\n})\n\nmodule.exports = withMDX({\n\twebpack: (config, { isServer }) =\u003e {\n\t\tif (!isServer) {\n\t  \tconfig.node = {\n\t    fs: 'empty'\n\t \t\t}\n\t\t}\n\treturn config\n\t},\n  pageExtensions: ['js', 'jsx', 'mdx'],\n  target: 'serverless',\n})\n```\n\n### Provide posts data to Home page\n\nWe have to call the **getallPostsData** function to get data. But how can we pass this data to the component in **pages/index.js**? Don't worry we can pass data as **props** to the component before rendering using the [getStaticProps](https://nextjs.org/docs/basic-features/data-fetching#getstaticprops-static-generation) function. **getStaticProps** allows us to fetch any dynamic data to provide before rendering the component. Change **pages/index.js** as\n\n```js:pages/index.js\nimport { getPostsMetaData } from '../lib/getPostsData.js';\n\nexport default function Home({ postsData }) {\n  return (\n    \u003cdiv className = 'info-container'\u003e\n      \u003cimg src = 'batman.png' alt = 'Batman Logo'/\u003e\n      \u003cp className = 'info-description'\u003eHi I'm Batman, the saviour of Gotham City and I like to roam in nights to bash the bad guys.\u003c/p\u003e\n      \u003cp className = 'info-description'\u003eBut please don't call me as a source for \u003cb\u003eCorona Virus\u003c/b\u003e and it could be the \u003cb\u003eJoker\u003c/b\u003e who\n      might have started this mess.\u003c/p\u003e\n      \u003chr/\u003e\n      {postsData.map((metadata) =\u003e {\n        return (\n          \u003cdiv key = {metadata.id}\u003e\n            \u003ch2 className = 'post-title'\u003e{metadata.title}\u003c/h2\u003e\n            \u003cp className = 'post-description'\u003e{metadata.description}\u003c/p\u003e\n          \u003c/div\u003e\n          )\n        })}\n\n      \u003cstyle jsx\u003e{`\n        .info-container {\n          margin: 0 5% 0 5%;\n        }\n\n        img {\n          width: 20%;\n          max-width: 20%;\n          height: auto;\n          margin-left: 40%;\n        }\n\n        .info-description {\n          font-size: 20px;\n        }\n\n        .post-title {\n          font-size: 24px;\n          color: black;\n        }\n\n        .post-description {\n          font-size: 16px;\n          color: #000000e6;\n        }\n      `}\u003c/style\u003e\n    \u003c/div\u003e\n  )\n}\n\nexport async function getStaticProps() {\n  const postsData = getPostsMetaData();\n  return {\n    props: {\n      postsData: postsData,\n    }\n  }\n}\n```\n\nWhich displays as\n\n![MDX blog posts display on Home page](blog-nextjs-mdx/blog-posts-display-on-home-page.jpg)\n\n## Rendering MDX and providing dynamic routing\n\nSo far so good until we want to display **.mdx** files as individual webpages. Right now it is not possible because Next.js only treats components exported inside the **pages** folder as webpages and provides routing, to provide routing for our posts we must export content from the **posts** directory to the **pages** folder. This is where we should spend some time to serve markdown files as webpages.\n\nThe beauty of Next.js is that we can dynamically serve pages by fetching these **.mdx** files and provide routing inside the **pages** directory with having dynamic pages.\n\nCreate **blog** folder inside the **pages** folder and inside this **blog** folder create a file with name **[id].js**. Dynamic routes in Next.js are identified by **[]** (square brackets) in the filename. We can provide any query parameter to this **[]** page component which will end up as _http://localhost:8000/blog/post-name_ for **post-name.js**. Now add following code to _pages/blog/**[id]**.js_\n\n```js:pages/blog/[id].js\nimport { serialize } from 'next-mdx-remote/serialize';\nimport { MDXRemote } from 'next-mdx-remote';\nimport { getAllPostsPath, getPostData } from '../../lib/getPostsData.js';\n\nconst components = {\n\th1: props =\u003e \u003ch1 style = {{\n\t\tfontSize: 'calc(1rem + 1.5vw)',\n\t\tcolor: 'black',\n\t\tmargin: '1vh 0 1vh 0', }}\n\t\t{...props} /\u003e,\n\n\tp: props =\u003e \u003cp style = {{\n\t\tfontSize: 'calc(1rem + 0.1vw)',\n\t\tcolor: '#000000e6',\n\t\tmargin: '0vh 0 1vh 0' }}\n\t\t{...props} /\u003e,\n}\n\nexport default function Blog({ postMetadata, postContent }) {\n\n\treturn (\n\t\t\u003cdiv\u003e\n\t\t\t\u003cdiv className = 'blog-content'\u003e\n\t\t\t\t\u003cMDXRemote {...postContent} components = {components} /\u003e\n\t\t\t\u003c/div\u003e\n\n\t\t\t\u003cstyle jsx\u003e{`\n\t\t\t\t.blog-content {\n\t\t\t\t\tdisplay: flex;\n\t\t\t\t\tflex: 100%;\n\t\t\t\t\tflex-direction: column;\n\t\t\t\t\tmargin: 1vw 25vw 1vw 25vw;\n\t\t\t\t\twidth: 50vw;\n\t\t\t\t\tmax-width: 50vw;\n\t\t\t\t}\n\t\t  `}\u003c/style\u003e\n\n\t\t\u003c/div\u003e\n\t)\n}\n\nexport async function getStaticPaths() {\n\tconst paths = getAllPostsPath();\n\treturn {\n\t\tpaths,\n\t\tfallback: false,\n\t}\n}\n\nexport async function getStaticProps({ params }) {\n\tconst postData = await getPostData(params.id);\n\tconst mdxSource = await serialize(postData.content);\n\treturn {\n\t\tprops: {\n\t\t\tpostMetadata: postData.metadata,\n\t\t\tpostContent: mdxSource,\n\t\t\tid: params.id,\n\t\t}\n\t}\n}\n```\n\nIf any page provides dynamic routing we must provide all dynamic paths we want to serve to this page through the [getStaticPaths](https://nextjs.org/docs/basic-features/data-fetching#getstaticpaths-static-generation) function.\n\nWith **serialize** and **MDXRemote**, we parse markdown content to HTML string and render it as plain HTML. To style the HTML tags in markdown, we pass custom tags as components to the MDX loader which maps tags automatically. In the above file two tags, **h1** and **p** are customized and combined as components.\n\nNow, in the browser, hit URL _http://localhost:3000/blog/batman-vs-superman_ or _http://localhost:3000/blog/justice-league_ to see the post. You might see output similar to below\n\n![MDX Post Display](blog-nextjs-mdx/mdx-post-display.jpg)\n\n## Navigation from the Home page\n\nWhat if we want to navigate from the Home page to blog posts by clicking on the title of the post? For this Next.js provides a [next/link](https://nextjs.org/docs/api-reference/next/link) component that takes care of dynamic routing from any page to another by pre-pending the necessary path before the page to navigate like navigation to **batman-vs-superman** results as _http://localhost:3000/blog/batman-vs-superman_. We must navigate like this only if we are not pre-pending the base URL manually inside the website. Now change **pages/index.js** to get dynamic navigation\n\n```js\nimport Link from 'next/link';\n{ ... }\n\n{postsData.map((metadata) =\u003e {\n  return (\n    \u003cdiv key = {metadata.id}\u003e\n      \u003cLink href={`/blog/${metadata.id}`} key = {metadata.title} \u003e\n        \u003ca className = 'post-title'\u003e{metadata.title}\u003c/a\u003e\n      \u003c/Link\u003e\n      \u003cp className = 'post-description'\u003e{metadata.description}\u003c/p\u003e\n    \u003c/div\u003e\n    )\n  })}\n\n { ... }\n```\n\n## Image Optimization\n\nImages take a lot of space in a webpage which reduces page loading time results in poor performance if the user has a poor internet connection. Images can be optimized many ways like converting all PNG/JPEG files to Webp/JPEG2000 format, [responsive images](https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images) for different screen dimensions by rescaling images, lazy loading, etc.,\n\nNext.js provides [next/image](https://nextjs.org/docs/api-reference/next/image) API for image optimization. But it needs the **next start** that runs on the node server which is not possible for static site generation. So we will use another plugin [next-optimized-images](https://github.com/cyrilwanner/next-optimized-images) which provides many options to optimize images.\n\nHere we will optimize images to serve in Webp format, to do so install **next-optimized-images**\n\n```shell\nnpm install next-optimized-images\n```\n\nBesides this install some additional plugins to convert PNG/JPEG to Webp format and loading Webp images.\n\n```shell\nnpm install imagemin-mozjpeg imagemin-optipng webp-loader\n```\n\nand change _next.config.js_ as\n\n```js:next.config.js\nconst withNextOptimizedImages = require('next-optimized-images');\n\nconst withMDX = require('@next/mdx')({\n  extension: /\\.mdx?$/,\n})\n\nmodule.exports = withNextOptimizedImages(\n\twithMDX({\n\t\twebpack: (config, { isServer }) =\u003e {\n\t\t\t\tif (!isServer) {\n\t\t  \t\tconfig.node = {\n\t\t    \tfs: 'empty'\n\t\t \t\t}\n\t\t\t}\n\t\t\treturn config\n\t\t},\n\t  pageExtensions: ['js', 'jsx', 'mdx'],\n\t  target: 'serverless',\n\t})\n)\n```\n\nIn build time **next-optimized-images** exports optimized images to **images** (custom name) folder inside _out/\\_next/static_. So create **images** directory at the root level and move images to this folder which needs optimization and provide relative paths now which were previously not required because of the **public** folder.\n\nChange the **img** tag in **pages/index.js** to\n\n```jsx\n\u003cpicture\u003e\n  \u003csource srcSet={require(\"../images/batman.png?webp\")} type=\"image/webp\" /\u003e\n  \u003csource srcSet={require(\"../images/batman.png\")} type=\"image/png\" /\u003e\n  \u003cimg src={require(\"../images/batman.png\")} alt=\"Batman Logo\" /\u003e\n\u003c/picture\u003e\n```\n\nThis will convert a PNG image to Webp format and loads Webp images. If the browser doesn't support Webp images **\u003cpicture\\\u003e** will automatically load the normal PNG image.\n\nYou can more than this by exploring more about this plugin.\n\n## SEO in Next.js\n\nFor Single Application Websites (SPA) SEO is a major problem which Next.js takes care of this by providing API [next/head](https://nextjs.org/docs/api-reference/next/head) which behaves exactly like **\u003chead\\\u003e** in HTML. We can wrap meta properties, title, description, Open Graph (OG) properties, Twitter cards, etc., inside the **Head** component. For our Home page we can set title and description as\n\n![Seo in Next.js:=:80](blog-nextjs-mdx/seo-in-nextjs.jpg)\n\nIf you don't want to set meta properties, title, description, and others there are so many plugins like [next-seo](https://www.npmjs.com/package/next-seo) available which handle all of these manual adding for you.\n\n---\n\n## Deploy to Github pages\n\n### Export static files to deploy\n\nNow our website is ready to move from development to production. To host our site we can use the static-site-generator of Next.js to generate all pre-render pages bundled inside the **out** directory. Build and generate **out** directory by typing the below command in the terminal\n\n```shell\nyarn deploy\n```\n\nYou can find a new directory **out** at the root level which contains all dynamic pages pre-rendered and ready to serve as HTML pages on the client-side. We will use this folder to host our website on Github pages.\n\n### Set up Github Pages\n\nGithub Pages is a very great place to host static sites. But we need to push and configure deployment changes every time we add content to the website. This is where we utilize Github Actions which automates deployment actions according to the configuration file we provide. But first, create a repository in Github to store our code files and push source code to this repository on the **main** branch. We use the **gh-pages** branch to which Github Actions deploy static files for hosting.\n\nTo do this we must provide access for Github Actions to this repository to access source files. To provide access, go to [Github Settings -\u003e tokens](https://github.com/settings/tokens) and create a new **personal access token** by checking **repo** scopes and others if you need and save as **GITHUB_TOKEN** (or any other name). Copy this **access code** and in the repository, move to the **secretes** tab in the **Settings** section and create a new secrete and copy this code. Remember the name of the secrete token you created in this repo for future purposes.\n\n![Github Repository Secretes Token:=:80](blog-nextjs-mdx/github-repo-secretes-token.jpg)\n\nIt's time to configure GitHub Actions. Create a directory called _.github/workflows_ at the root level locally. Create a file **integrate.yml** inside _.github/workflows_ and add the following configuration\n\n```yaml:.github/workflows/integrate.yml\nname: Build and Deploy\non:\n  push:\n    branches:\n      - master\njobs:\n  build-and-deploy:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2.3.1\n        with:\n          persist-credentials: false\n\n      - name: Cache\n        uses: actions/cache@v2\n        with:\n          path: ${{ github.workspace }}/.next/cache\n          key: ${{ runner.os }}-nextjs-${{ hashFiles('**/package-lock.json') }}\n\n      - name: Install and Build\n        uses: actions/setup-node@v1\n      - run: npm install\n      - run: npm run build\n      - run: npm run export\n        env:\n            CI: true\n      - run: touch out/.nojekyll\n\n      - name: Deploy\n        uses: JamesIves/github-pages-deploy-action@3.7.1\n        with:\n          ACCESS_TOKEN: ${{ secrets.ACCESS_TOKEN }}\n          BRANCH: gh-pages\n          FOLDER: out\n          CLEAN: true\n```\n\nYou may have to replace **ACCESS_TOKEN** with your custom name. This is what we configured\n\n1. Execute workflow action on every push to the **master** branch.\n2. Run commands _npm install \u0026\u0026 npm run build \u0026\u0026 npm run export_ to build and export static version of our website.\n3. Deploy contents inside **out** folder to the **gh-pages** branch.\n4. Added _touch out/.nojekyll_ to **gh-pages** because Github pages **Jekyll** to render static sites. **Jekyll** ignores files in the directory starting with **\\_** and it is an issue for us because all our static assets are created in **\\_next** folder. **.nojekyll** tells Github Pages not to run published files through **Jekyll**.\n\nPush all changes to Github repo\n\n```shell\ngit add .\ngit commit -m \"initial deployment of the blog\"\ngit push -u origin master\n```\n\nThis will push all your working source files to the Github repo and Github Actions starts a workflow to deploy static files in the **out** folder to **gh-pages**.\n\nYou can monitor the status of the Github Actions workflow after every push to the **master** branch in the **Actions** tab.\n\nEnable Github Pages in the **Settings** section of the repo and for source select the **gh-pages** branch.\n\nIf everything worked properly you can have your website hosted at _https://\u003cusername\\\u003e.github.io/\u003crepo\\\u003e_. Here **\u003crepo\\\u003e** name is **blog**.\n\n---\n\n## Manage CSS, assets, and page links to work properly\n\nIf you host the website at _https://\u003cusername\\\u003e.github.io/\u003crepo\\\u003e_ you can observe CSS or other static assets and routing not working properly. This is because Next.js assumes **out** directory hosted at root level as _https://\u003cusername\\\u003e.github.io/_ and directs all routing, replaces assets and everything to this basepath. But we have hosted the **out** folder in **blog/out**, so we must add **subpath** **blog** to the **basepath** to manage assets linking and routing. We can do this by changing the configuration in **next.config.js**\n\n```js:next.config.js\nconst ghPages = process.env.DEPLOY_TARGET === 'gh-pages';\n\nconst withNextOptimizedImages = require('next-optimized-images');\n\nconst withMDX = require('@next/mdx')({\n  extension: /\\.mdx?$/,\n})\n\nmodule.exports = withNextOptimizedImages(\n\twithMDX({\n\t\twebpack: (config, { isServer }) =\u003e {\n\t\t\tif (!isServer) {\n\t\t  \tconfig.node = {\n\t\t    fs: 'empty'\n\t\t \t\t}\n\t\t\t}\n\t\t\treturn config\n\t\t},\n\t  pageExtensions: ['js', 'jsx', 'mdx'],\n\t  target: 'serverless',\n\t\tbasePath: ghPages? '/blog/' : '',\n\t\tassetPrefix: ghPages ? '/blog/' : '',\n\t})\n)\n```\n\nWhile developing it works fine everything, so we check the environment phase we are processing with **process.env** and **process.env.DEPLOY_TARGET** tells the current hosted environment. In local development, we run on the **node** server hosted on our machine so we don't need to manage any **basepath** or **subpath**.\n\n**basePath** specifies the base path of the application to manage linking pages. If we are on the **gh-pages**, the base path **/blog/** resolves to **username.github.io/blog/** where **/** is the home path.\n\n**assetPrefix** specifies where to look for assets (CSS, Images, etc.,).\n\n---\n\nAnd here we are with our personal blog on the internet and we can take our blog to next level by adding fancy CSS, custom components, and other pages like **about**, **contact**... I hope you find this articl useful to build your own blog. To check the source code of this website, you can find it at [github.com/santhalakshminarayana/santhalakshminarayana.github.io](https://github.com/santhalakshminarayana/santhalakshminarayana.github.io).\n"}]},"__N_SSG":true},"page":"/","query":{},"buildId":"WcgTTsGPX2O1xgu7UXzKY","runtimeConfig":{},"isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>